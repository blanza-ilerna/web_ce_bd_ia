<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Algoritmos de Boosting: AdaBoost, Gradient Boosting, XGBoost. Aprende c√≥mo el aprendizaje secuencial mejora modelos d√©biles en Machine Learning.">
    <meta name="keywords"
        content="Boosting, AdaBoost, Gradient Boosting, XGBoost, Ensemble Learning, Machine Learning, Sequential Learning, Weak Learners">
    <meta name="author" content="Bjlanza">
    <meta name="organization" content="ILERNA">
    <meta property="og:title" content="Algoritmos de Boosting | iLERNA">
    <meta property="og:description"
        content="Aprende c√≥mo funcionan los algoritmos de Boosting: AdaBoost, Gradient Boosting y XGBoost. Aprendizaje secuencial y reducci√≥n de sesgo.">
    <meta property="og:type" content="article">
    <title>Algoritmos de Boosting | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>

<body>
    <div class="container">
        <header>
            <div class="header-container">
                <div class="logo-container">
                    <a href="../index.html">
                        <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                    </a>
                    <div class="logo-text">
                        iLERNA
                        <span>Curso de Especializaci√≥n en IA y Big Data</span>
                    </div>
                </div>
                <div class="breadcrumb">
                    <a href="../index.html">Inicio</a> ‚Ä∫
                    <a href="index.html">Sistemas de Aprendizaje Autom√°tico</a> ‚Ä∫
                    <span>Algoritmos de Boosting</span>
                </div>
            </div>
            <h1 class="text-center">Algoritmos de Boosting</h1>
            <p class="subtitle text-center">Aprender de los errores: La evoluci√≥n secuencial de los modelos</p>
        </header>

        <main>
            <!-- SECCI√ìN 1: INTRODUCCI√ìN -->
            <section class="section">
                <h2 class="section-title">¬øQu√© es Boosting?</h2>
                <p>
                    <strong>Boosting</strong> es una familia de algoritmos de <strong>Ensemble Learning</strong> que
                    convierte modelos d√©biles (weak learners) en un modelo fuerte. A diferencia del Bagging (donde los
                    modelos son independientes), en Boosting los modelos se entrenan de forma <strong>secuencial</strong>.
                </p>
                <p>
                    Cada nuevo modelo intenta <strong>corregir los errores</strong> cometidos por los modelos anteriores. Es
                    como un equipo de expertos donde cada uno se especializa en resolver los casos que los dem√°s encontraron
                    dif√≠ciles.
                </p>

                <div class="comparison-grid">
                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">Secuencial</h4>
                        <p>
                            El modelo N depende del modelo N-1. No se pueden entrenar en paralelo.
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #8A7AAF;">Foco en Errores</h4>
                        <p>
                            Se da m√°s peso a las observaciones mal clasificadas anteriormente.
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">Reducci√≥n de Sesgo</h4>
                        <p>
                            Principalmente reduce el <strong>bias</strong> (sesgo), haciendo modelos muy precisos.
                        </p>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 2: EL PROCESO DE BOOSTING -->
            <section class="section">
                <h2 class="section-title">¬øC√≥mo funciona? (Paso a paso)</h2>

                <div style="margin: 2rem 0;">
                    <svg viewBox="0 0 800 300" style="width: 100%; max-width: 100%; display: block; border: 2px solid #e5e5e5; border-radius: 1rem; background: #fafafa; padding: 1rem;">
                        <!-- Definiciones de gradientes -->
                        <defs>
                            <linearGradient id="modelGrad" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#49B9CE;stop-opacity:1" />
                                <stop offset="100%" style="stop-color:#6AD1E3;stop-opacity:1" />
                            </linearGradient>
                            <linearGradient id="errorGrad" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#8A7AAF;stop-opacity:1" />
                                <stop offset="100%" style="stop-color:#A594C9;stop-opacity:1" />
                            </linearGradient>
                            <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#333" />
                            </marker>
                        </defs>

                        <!-- Paso 1 -->
                        <g transform="translate(50, 50)">
                            <rect x="0" y="0" width="180" height="120" rx="10" fill="#f9f9f9" stroke="#cccccc"
                                stroke-width="2" />
                            <text x="90" y="30" text-anchor="middle" font-family="Montserrat" font-weight="700"
                                fill="#333">Datos Originales</text>
                            <!-- Puntos -->
                            <circle cx="40" cy="60" r="5" fill="#49B9CE" />
                            <circle cx="60" cy="80" r="5" fill="#49B9CE" />
                            <circle cx="140" cy="60" r="5" fill="#FF5252" /> <!-- Error -->
                            <circle cx="120" cy="90" r="5" fill="#49B9CE" />

                            <text x="90" y="110" text-anchor="middle" font-family="Montserrat" font-size="12" fill="#555">Modelo
                                D√©bil 1</text>
                        </g>

                        <!-- Flecha 1 -->
                        <path d="M240 110 L290 110" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)" />

                        <!-- Paso 2 -->
                        <g transform="translate(300, 50)">
                            <rect x="0" y="0" width="180" height="120" rx="10" fill="#f9f9f9" stroke="#cccccc"
                                stroke-width="2" />
                            <text x="90" y="30" text-anchor="middle" font-family="Montserrat" font-weight="700"
                                fill="#333">Ponderaci√≥n</text>
                            <!-- Puntos con peso -->
                            <circle cx="40" cy="60" r="3" fill="#49B9CE" opacity="0.5" />
                            <circle cx="60" cy="80" r="3" fill="#49B9CE" opacity="0.5" />
                            <circle cx="140" cy="60" r="12" fill="#FF5252" opacity="0.8" /> <!-- Error enfatizado -->
                            <circle cx="120" cy="90" r="3" fill="#49B9CE" opacity="0.5" />

                            <text x="90" y="110" text-anchor="middle" font-family="Montserrat" font-size="12" fill="#555">Modelo
                                D√©bil 2</text>
                        </g>

                        <!-- Flecha 2 -->
                        <path d="M490 110 L540 110" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)" />

                        <!-- Paso 3 -->
                        <g transform="translate(550, 50)">
                            <rect x="0" y="0" width="180" height="120" rx="10" fill="url(#modelGrad)" stroke="none" />
                            <text x="90" y="65" text-anchor="middle" font-family="Montserrat" font-weight="700" font-size="18"
                                fill="#fff">Modelo Fuerte</text>
                            <text x="90" y="90" text-anchor="middle" font-family="Montserrat" font-size="12" fill="#fff">Suma
                                Ponderada</text>
                        </g>
                    </svg>
                </div>

                <div class="highlight-box" style="background: #E8F7FA; border-left: 4px solid #49B9CE;">
                    <h3 style="color: #49B9CE; margin-top: 0;">Proceso de Boosting</h3>
                    <ol style="line-height: 1.8;">
                        <li>Se entrena un <strong>modelo base</strong> (generalmente un √°rbol poco profundo).</li>
                        <li>Se identifican los <strong>errores</strong> (instancias mal clasificadas).</li>
                        <li>Se asigna <strong>mayor peso</strong> a esos errores para el siguiente modelo.</li>
                        <li>El proceso se repite y finalmente se combinan todos los modelos en una votaci√≥n ponderada.</li>
                    </ol>
                </div>
            </section>

            <!-- SECCI√ìN 3: ALGORITMOS PRINCIPALES -->
            <section class="section">
                <h2 class="section-title">Algoritmos Estrella</h2>
                <p>
                    Los algoritmos de Boosting m√°s populares han revolucionado el Machine Learning competitivo y aplicado.
                    Cada uno introduce mejoras incrementales sobre el concepto original:
                </p>

                <div class="grid-features">
                    <div class="feature-card">
                        <h3 style="color: #8A7AAF;">AdaBoost</h3>
                        <h4 style="color: #8A7AAF; font-size: 1rem; margin-top: 0.5rem;">(Adaptive Boosting)</h4>
                        <p>
                            El pionero. Ajusta los pesos de las <strong>instancias</strong> de datos.
                            Si un dato es dif√≠cil de clasificar, su peso aumenta para que el siguiente √°rbol se centre en
                            √©l. Creado por Freund y Schapire en 1996, gan√≥ el Premio G√∂del en 2003.
                        </p>
                        <div class="highlight-box" style="background: #F0EDF5; margin-top: 1rem; border-left: 3px solid #8A7AAF;">
                            <p style="font-size: 0.95rem; margin: 0;">
                                <strong>Caso de uso:</strong> Clasificaci√≥n binaria, detecci√≥n de rostros (Viola-Jones),
                                problemas con clases desbalanceadas.
                            </p>
                        </div>
                    </div>

                    <div class="feature-card secondary">
                        <h3 style="color: #49B9CE;">Gradient Boosting</h3>
                        <h4 style="color: #49B9CE; font-size: 1rem; margin-top: 0.5rem;">(Gradient Boosting Machines - GBM)</h4>
                        <p>
                            Generalizaci√≥n matem√°tica. En lugar de pesos, entrena nuevos modelos para predecir los
                            <strong>residuos</strong> (la diferencia entre el valor real y el predicho) del modelo anterior.
                            Usa descenso de gradiente para minimizar la funci√≥n de p√©rdida.
                        </p>
                        <div class="highlight-box" style="background: #E8F7FA; margin-top: 1rem; border-left: 3px solid #49B9CE;">
                            <p style="font-size: 0.95rem; margin: 0;">
                                <strong>Caso de uso:</strong> Regresi√≥n y clasificaci√≥n multi-clase, ranking, predicci√≥n
                                de series temporales.
                            </p>
                        </div>
                    </div>

                    <div class="feature-card">
                        <h3 style="color: #8A7AAF;">XGBoost</h3>
                        <h4 style="color: #8A7AAF; font-size: 1rem; margin-top: 0.5rem;">(Extreme Gradient Boosting)</h4>
                        <p>
                            La versi√≥n optimizada y m√°s popular en competiciones (Kaggle).
                            Incluye regularizaci√≥n para evitar overfitting, manejo autom√°tico de valores nulos, ejecuci√≥n
                            paralela y distribuida, y m√∫ltiples optimizaciones algor√≠tmicas.
                        </p>
                        <div class="highlight-box" style="background: #F0EDF5; margin-top: 1rem; border-left: 3px solid #8A7AAF;">
                            <p style="font-size: 0.95rem; margin: 0;">
                                <strong>Caso de uso:</strong> Ganador de mayor√≠a de competiciones Kaggle, sistemas de
                                recomendaci√≥n, fraud detection, CTR prediction.
                            </p>
                        </div>
                    </div>

                    <div class="feature-card secondary">
                        <h3 style="color: #49B9CE;">LightGBM & CatBoost</h3>
                        <h4 style="color: #49B9CE; font-size: 1rem; margin-top: 0.5rem;">(Variantes Modernas)</h4>
                        <p>
                            <strong>LightGBM</strong> (Microsoft): Usa "leaf-wise" growth y t√©cnicas de histograma para
                            entrenamiento ultra-r√°pido. <strong>CatBoost</strong> (Yandex): Manejo nativo de variables
                            categ√≥ricas y menor overfitting por defecto.
                        </p>
                        <div class="highlight-box" style="background: #E8F7FA; margin-top: 1rem; border-left: 3px solid #49B9CE;">
                            <p style="font-size: 0.95rem; margin: 0;">
                                <strong>Caso de uso:</strong> Datasets enormes (LightGBM), datos con muchas categor√≠as (CatBoost).
                            </p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 4: EJEMPLO PR√ÅCTICO ADABOOST VS GRADIENT BOOSTING -->
            <section class="section">
                <h2 class="section-title">Ejemplo en Python: AdaBoost vs Gradient Boosting</h2>
                <p>
                    Implementaci√≥n sencilla usando <code>scikit-learn</code> para clasificar datos de c√°ncer de mama.
                    Comparamos ambos algoritmos para ver sus diferencias en rendimiento:
                </p>

                <div class="code-block">
                    <div class="code-header">Python - Comparaci√≥n AdaBoost vs Gradient Boosting</div>
                    <pre><code class="language-python">from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 1. Cargar datos
data = load_breast_cancer()
X, y = data.data, data.target

# 2. Dividir en train y test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. AdaBoost
# Usa √°rboles muy peque√±os (stumps) por defecto
ada = AdaBoostClassifier(n_estimators=100, random_state=42)
ada.fit(X_train, y_train)
ada_pred = ada.predict(X_test)

# 4. Gradient Boosting
# Usa √°rboles m√°s profundos y optimiza residuos
gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb.fit(X_train, y_train)
gb_pred = gb.predict(X_test)

# 5. Resultados
print(f"AdaBoost Accuracy: {accuracy_score(y_test, ada_pred):.4f}")
print(f"Gradient Boosting Accuracy: {accuracy_score(y_test, gb_pred):.4f}")

# Salida esperada (aprox):
# AdaBoost Accuracy: 0.9737
# Gradient Boosting Accuracy: 0.9649</code></pre>
                </div>

                <div class="highlight-box" style="background: #FFF8DC; border-left: 4px solid #FFA726; margin-top: 1.5rem;">
                    <h4 style="color: #E65100; margin-top: 0;">Interpretaci√≥n de Resultados</h4>
                    <p>
                        En este ejemplo, AdaBoost supera ligeramente a Gradient Boosting. Esto es t√≠pico en datasets
                        peque√±os y limpios. Sin embargo, Gradient Boosting suele ser superior en datasets complejos
                        con ruido, especialmente en problemas de regresi√≥n.
                    </p>
                </div>
            </section>

            <!-- SECCI√ìN 5: XGBOOST EJEMPLO COMPLETO -->
            <section class="section">
                <h2 class="section-title">XGBoost: El Rey del Boosting</h2>
                <p>
                    XGBoost es el algoritmo de Boosting m√°s potente y popular. Incluye optimizaciones avanzadas como
                    regularizaci√≥n L1/L2, pruning de √°rboles, manejo de valores faltantes y ejecuci√≥n paralela.
                    Veamos un ejemplo completo con optimizaci√≥n de hiperpar√°metros:
                </p>

                <div class="code-block">
                    <div class="code-header">Python - XGBoost con Grid Search</div>
                    <pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

# 1. Cargar datos
data = load_breast_cancer()
X, y = data.data, data.target

# 2. Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 3. Crear modelo base XGBoost
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    random_state=42
)

# 4. Grid de hiperpar√°metros
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.1, 1],  # Regularizaci√≥n L1
    'reg_lambda': [0, 1, 10]   # Regularizaci√≥n L2
}

# 5. Grid Search con validaci√≥n cruzada
grid_search = GridSearchCV(
    xgb_model,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)
grid_search.fit(X_train, y_train)

# 6. Mejor modelo
best_xgb = grid_search.best_estimator_
print(f"\nMejores par√°metros: {grid_search.best_params_}")
print(f"Mejor score CV: {grid_search.best_score_:.4f}")

# 7. Evaluaci√≥n en test
y_pred = best_xgb.predict(X_test)
print(f"\nAccuracy en Test: {accuracy_score(y_test, y_pred):.4f}")
print("\nReporte de clasificaci√≥n:")
print(classification_report(y_test, y_pred, target_names=data.target_names))

# 8. Importancia de caracter√≠sticas
importances = best_xgb.feature_importances_
feature_importance = pd.DataFrame({
    'feature': data.feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

print("\nTop 10 caracter√≠sticas m√°s importantes:")
print(feature_importance.head(10))

# 9. Curva de aprendizaje (opcional)
eval_set = [(X_train, y_train), (X_test, y_test)]
best_xgb.fit(X_train, y_train, eval_set=eval_set, verbose=False)

# Obtener historial de evaluaci√≥n
results = best_xgb.evals_result()
print(f"\nError final en Train: {results['validation_0']['logloss'][-1]:.4f}")
print(f"Error final en Test: {results['validation_1']['logloss'][-1]:.4f}")</code></pre>
                </div>

                <div class="highlight-box" style="background: linear-gradient(to right, #E8F7FA, #F0EDF5); margin-top: 1.5rem; border-left: 4px solid #49B9CE;">
                    <h4 style="color: #49B9CE; margin-top: 0;">Hiperpar√°metros Clave de XGBoost</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>n_estimators:</strong> N√∫mero de √°rboles. M√°s √°rboles = mejor rendimiento pero mayor tiempo.</li>
                        <li><strong>max_depth:</strong> Profundidad m√°xima de cada √°rbol. Controla complejidad (t√≠pico: 3-10).</li>
                        <li><strong>learning_rate:</strong> Tasa de aprendizaje. Valores bajos (0.01-0.1) requieren m√°s √°rboles pero generalizan mejor.</li>
                        <li><strong>subsample:</strong> Fracci√≥n de muestras usadas por √°rbol. Reduce overfitting (t√≠pico: 0.8-1.0).</li>
                        <li><strong>colsample_bytree:</strong> Fracci√≥n de caracter√≠sticas usadas por √°rbol (similar a Random Forest).</li>
                        <li><strong>gamma:</strong> Reducci√≥n m√≠nima de p√©rdida para hacer un split. Mayor = m√°s conservador.</li>
                        <li><strong>reg_alpha/reg_lambda:</strong> Regularizaci√≥n L1/L2 para prevenir overfitting.</li>
                    </ul>
                </div>
            </section>

            <!-- SECCI√ìN 6: BAGGING VS BOOSTING -->
            <section class="section">
                <h2 class="section-title">Bagging vs Boosting: El Duelo Final</h2>
                <p>
                    Tanto Bagging como Boosting son t√©cnicas de Ensemble Learning, pero tienen enfoques y objetivos
                    fundamentalmente diferentes. Entender cu√°ndo usar cada uno es crucial:
                </p>

                <div style="overflow-x: auto;">
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Caracter√≠stica</th>
                                <th>Bagging (Random Forest)</th>
                                <th>Boosting (XGBoost, AdaBoost)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Construcci√≥n</strong></td>
                                <td>Paralela (Independiente)</td>
                                <td>Secuencial (Dependiente)</td>
                            </tr>
                            <tr>
                                <td><strong>Objetivo Principal</strong></td>
                                <td>Reducir Varianza (Overfitting)</td>
                                <td>Reducir Sesgo (Underfitting)</td>
                            </tr>
                            <tr>
                                <td><strong>Modelos Base</strong></td>
                                <td>√Årboles profundos (complejos)</td>
                                <td>√Årboles poco profundos (d√©biles)</td>
                            </tr>
                            <tr>
                                <td><strong>Muestreo de Datos</strong></td>
                                <td>Bootstrap (con reemplazo)</td>
                                <td>Ponderaci√≥n adaptativa</td>
                            </tr>
                            <tr>
                                <td><strong>Combinaci√≥n</strong></td>
                                <td>Votaci√≥n/Promedio simple</td>
                                <td>Votaci√≥n ponderada</td>
                            </tr>
                            <tr>
                                <td><strong>Riesgo de Overfitting</strong></td>
                                <td>Bajo (robusto al ruido)</td>
                                <td>Alto (sensible al ruido/outliers)</td>
                            </tr>
                            <tr>
                                <td><strong>Velocidad de Entrenamiento</strong></td>
                                <td>R√°pida (paralelizable)</td>
                                <td>M√°s lenta (secuencial)</td>
                            </tr>
                            <tr>
                                <td><strong>Sensibilidad a Outliers</strong></td>
                                <td>Baja (robusto)</td>
                                <td>Alta (puede sobreajustar)</td>
                            </tr>
                            <tr>
                                <td><strong>Interpretabilidad</strong></td>
                                <td>Baja (muchos √°rboles)</td>
                                <td>Baja (secuencia compleja)</td>
                            </tr>
                            <tr>
                                <td><strong>Mejor para</strong></td>
                                <td>Datos ruidosos, alta dimensionalidad</td>
                                <td>M√°xima precisi√≥n, competiciones</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="warning-box" style="margin-top: 2rem;">
                    <h4 style="margin-top: 0;">¬øCu√°ndo usar cada t√©cnica?</h4>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem; margin-top: 1rem;">
                        <div>
                            <h5 style="color: #49B9CE; margin-bottom: 0.5rem;">Usa Bagging si:</h5>
                            <ul style="line-height: 1.8;">
                                <li>Tus datos tienen mucho ruido u outliers</li>
                                <li>Necesitas entrenamiento r√°pido (paralelizable)</li>
                                <li>Quieres un modelo robusto y estable</li>
                                <li>Tienes alta varianza (overfitting)</li>
                                <li>Necesitas importancia de caracter√≠sticas</li>
                            </ul>
                        </div>
                        <div>
                            <h5 style="color: #8A7AAF; margin-bottom: 0.5rem;">Usa Boosting si:</h5>
                            <ul style="line-height: 1.8;">
                                <li>Buscas la m√°xima precisi√≥n posible</li>
                                <li>Tus datos son limpios (pocos outliers)</li>
                                <li>Tienes alto sesgo (underfitting)</li>
                                <li>Est√°s en una competici√≥n (Kaggle)</li>
                                <li>Puedes dedicar m√°s tiempo al tuning</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 7: VENTAJAS Y DESVENTAJAS DEL BOOSTING -->
            <section class="section">
                <h2 class="section-title">Ventajas y Desventajas del Boosting</h2>

                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(320px, 1fr)); gap: 2rem;">
                    <div class="feature-card">
                        <h3 style="color: #4CAF50; margin-top: 0;">Ventajas</h3>
                        <ul style="line-height: 1.8; margin-left: 1.5rem;">
                            <li><strong>Alta precisi√≥n:</strong> Generalmente supera a otros algoritmos en precisi√≥n.</li>
                            <li><strong>Reduce bias:</strong> Convierte modelos d√©biles en modelos muy potentes.</li>
                            <li><strong>Versatilidad:</strong> Funciona bien en clasificaci√≥n, regresi√≥n y ranking.</li>
                            <li><strong>Manejo de datos:</strong> XGBoost maneja valores faltantes autom√°ticamente.</li>
                            <li><strong>Importancia de caracter√≠sticas:</strong> Identifica variables m√°s relevantes.</li>
                            <li><strong>Poco preprocesamiento:</strong> No requiere normalizaci√≥n ni escalado.</li>
                            <li><strong>Ganador de Kaggle:</strong> Algoritmo m√°s usado en competiciones de ML.</li>
                        </ul>
                    </div>

                    <div class="feature-card secondary">
                        <h3 style="color: #C62828; margin-top: 0;">Desventajas</h3>
                        <ul style="line-height: 1.8; margin-left: 1.5rem;">
                            <li><strong>Sensible a outliers:</strong> Puede sobreajustarse a datos an√≥malos.</li>
                            <li><strong>Entrenamiento secuencial:</strong> No se puede paralelizar completamente.</li>
                            <li><strong>Tiempo de entrenamiento:</strong> M√°s lento que Bagging o modelos simples.</li>
                            <li><strong>Riesgo de overfitting:</strong> Requiere ajuste cuidadoso de hiperpar√°metros.</li>
                            <li><strong>Complejidad:</strong> Muchos hiperpar√°metros para optimizar.</li>
                            <li><strong>Caja negra:</strong> Dif√≠cil de interpretar (cientos de √°rboles secuenciales).</li>
                            <li><strong>Curva de aprendizaje:</strong> Requiere experiencia para usar correctamente.</li>
                        </ul>
                    </div>
                </div>

                <div class="highlight-box" style="background: linear-gradient(to right, #E8F7FA, #F0EDF5); margin-top: 2rem; border-left: 4px solid #8A7AAF;">
                    <h4 style="color: #8A7AAF; margin-top: 0;">Consejos para Evitar Overfitting en Boosting</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>Reduce learning_rate:</strong> Valores bajos (0.01-0.05) generalizan mejor.</li>
                        <li><strong>Limita max_depth:</strong> √Årboles poco profundos (3-7) previenen complejidad excesiva.</li>
                        <li><strong>Usa regularizaci√≥n:</strong> reg_alpha y reg_lambda en XGBoost son muy efectivos.</li>
                        <li><strong>Early stopping:</strong> Det√©n el entrenamiento cuando el error de validaci√≥n deje de mejorar.</li>
                        <li><strong>Subsample:</strong> Usa solo una fracci√≥n de datos por √°rbol (0.7-0.9).</li>
                        <li><strong>Cross-validation:</strong> Siempre valida con CV para detectar overfitting temprano.</li>
                    </ul>
                </div>
            </section>

            <!-- SECCI√ìN 8: APLICACIONES REALES -->
            <section class="section">
                <h2 class="section-title">Aplicaciones Reales del Boosting</h2>
                <p>
                    Los algoritmos de Boosting dominan en aplicaciones industriales y competiciones de Machine Learning
                    gracias a su precisi√≥n superior. Estos son algunos casos de √©xito:
                </p>

                <div class="grid-features">
                    <div class="feature-card">
                        <h4 style="color: #49B9CE;">Kaggle Competitions</h4>
                        <p>
                            XGBoost y LightGBM han ganado m√°s del <strong>70% de las competiciones Kaggle</strong> en
                            los √∫ltimos a√±os. Son la primera opci√≥n para tabular data en ML competitivo.
                        </p>
                    </div>

                    <div class="feature-card secondary">
                        <h4 style="color: #8A7AAF;">Sistemas de Recomendaci√≥n</h4>
                        <p>
                            Netflix, Amazon y Spotify usan Gradient Boosting para predecir preferencias de usuarios.
                            XGBoost procesa billones de eventos diarios para recomendar contenido personalizado.
                        </p>
                    </div>

                    <div class="feature-card">
                        <h4 style="color: #49B9CE;">Detecci√≥n de Fraude</h4>
                        <p>
                            Bancos como PayPal, Visa y Mastercard usan XGBoost para detectar transacciones fraudulentas
                            en tiempo real, analizando patrones complejos en millones de transacciones.
                        </p>
                    </div>

                    <div class="feature-card secondary">
                        <h4 style="color: #8A7AAF;">Publicidad Digital</h4>
                        <p>
                            Google Ads, Facebook Ads y Baidu usan Gradient Boosting para predecir CTR (Click-Through Rate)
                            y optimizar pujas en subastas de publicidad program√°tica.
                        </p>
                    </div>

                    <div class="feature-card">
                        <h4 style="color: #49B9CE;">Medicina Predictiva</h4>
                        <p>
                            Hospitales usan XGBoost para predecir readmisiones, mortalidad, y respuesta a tratamientos.
                            Super√≥ a modelos tradicionales en predicci√≥n de diabetes y enfermedades card√≠acas.
                        </p>
                    </div>

                    <div class="feature-card secondary">
                        <h4 style="color: #8A7AAF;">B√∫squeda y Ranking</h4>
                        <p>
                            Bing de Microsoft usa LambdaMART (variante de Gradient Boosting) para ranking de resultados
                            de b√∫squeda. Yandex desarroll√≥ CatBoost espec√≠ficamente para su motor de b√∫squeda.
                        </p>
                    </div>
                </div>

                <div class="highlight-box" style="background: #FFF8DC; border-left: 4px solid #FFA726; margin-top: 2rem;">
                    <h4 style="color: #E65100; margin-top: 0;">Caso de √âxito: Airbnb</h4>
                    <p>
                        Airbnb usa XGBoost en su algoritmo de <strong>Smart Pricing</strong> para predecir el precio
                        √≥ptimo de cada propiedad. El modelo analiza m√°s de 70 caracter√≠sticas (ubicaci√≥n, amenidades,
                        estacionalidad, eventos locales) y procesa millones de anuncios diariamente. Resultado: aument√≥
                        las reservas un 9% y los ingresos de los hosts un 4% en promedio.
                    </p>
                    <p style="margin-top: 1rem; margin-bottom: 0;">
                        Seg√∫n su equipo de Data Science, probaron Random Forest, redes neuronales y regresi√≥n lineal,
                        pero XGBoost super√≥ a todos en precisi√≥n y velocidad de inferencia.
                    </p>
                </div>
            </section>

            <!-- SECCI√ìN 9: COMPARACI√ìN DE LIBRER√çAS -->
            <section class="section">
                <h2 class="section-title">Comparaci√≥n de Librer√≠as de Boosting</h2>
                <p>
                    Existen varias implementaciones de Gradient Boosting, cada una con sus ventajas. Aqu√≠ comparamos
                    las m√°s populares:
                </p>

                <div style="overflow-x: auto;">
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Caracter√≠stica</th>
                                <th>XGBoost</th>
                                <th>LightGBM</th>
                                <th>CatBoost</th>
                                <th>scikit-learn GB</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Organizaci√≥n</strong></td>
                                <td>DMLC (Open Source)</td>
                                <td>Microsoft</td>
                                <td>Yandex</td>
                                <td>scikit-learn</td>
                            </tr>
                            <tr>
                                <td><strong>Velocidad</strong></td>
                                <td>R√°pida</td>
                                <td>Muy r√°pida</td>
                                <td>Media-R√°pida</td>
                                <td>Lenta</td>
                            </tr>
                            <tr>
                                <td><strong>Precisi√≥n</strong></td>
                                <td>Excelente</td>
                                <td>Excelente</td>
                                <td>Excelente</td>
                                <td>Buena</td>
                            </tr>
                            <tr>
                                <td><strong>Manejo de Categ√≥ricas</strong></td>
                                <td>Requiere encoding</td>
                                <td>Requiere encoding</td>
                                <td>Nativo (built-in)</td>
                                <td>Requiere encoding</td>
                            </tr>
                            <tr>
                                <td><strong>Overfitting</strong></td>
                                <td>Medio (requiere tuning)</td>
                                <td>Medio-Alto</td>
                                <td>Bajo (mejor default)</td>
                                <td>Medio</td>
                            </tr>
                            <tr>
                                <td><strong>Memoria</strong></td>
                                <td>Media</td>
                                <td>Baja (eficiente)</td>
                                <td>Alta</td>
                                <td>Media</td>
                            </tr>
                            <tr>
                                <td><strong>GPU Support</strong></td>
                                <td>S√≠</td>
                                <td>S√≠</td>
                                <td>S√≠</td>
                                <td>No</td>
                            </tr>
                            <tr>
                                <td><strong>Distribuci√≥n</strong></td>
                                <td>S√≠ (Dask, Spark)</td>
                                <td>S√≠ (Dask, Spark)</td>
                                <td>Limitada</td>
                                <td>No</td>
                            </tr>
                            <tr>
                                <td><strong>Mejor para</strong></td>
                                <td>Uso general, Kaggle</td>
                                <td>Datasets grandes</td>
                                <td>Datos categ√≥ricos</td>
                                <td>Prototipado r√°pido</td>
                            </tr>
                            <tr>
                                <td><strong>Instalaci√≥n</strong></td>
                                <td>pip install xgboost</td>
                                <td>pip install lightgbm</td>
                                <td>pip install catboost</td>
                                <td>Incluido en sklearn</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="warning-box" style="margin-top: 2rem;">
                    <h4 style="margin-top: 0;">Recomendaciones de Uso</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>XGBoost:</strong> Tu primera opci√≥n. Equilibrio perfecto entre velocidad, precisi√≥n y comunidad.</li>
                        <li><strong>LightGBM:</strong> Si tienes millones de filas o necesitas entrenamiento ultra-r√°pido.</li>
                        <li><strong>CatBoost:</strong> Si tu dataset tiene muchas variables categ√≥ricas o quieres menos overfitting por defecto.</li>
                        <li><strong>scikit-learn GB:</strong> Solo para prototipado r√°pido o cuando no puedas instalar dependencias adicionales.</li>
                    </ul>
                </div>
            </section>

            <!-- SECCI√ìN 10: DATO CURIOSO -->
            <section class="section">
                <div class="warning-box">
                    <h3 style="margin-top: 0;">Dato Curioso: El Origen de AdaBoost</h3>
                    <p>
                        <strong>AdaBoost</strong> fue inventado por <strong>Yoav Freund</strong> y <strong>Robert Schapire</strong>
                        en 1996. El algoritmo fue tan revolucionario que les vali√≥ el <strong>Premio G√∂del 2003</strong>,
                        uno de los premios m√°s prestigiosos en Ciencias de la Computaci√≥n Te√≥rica.
                    </p>
                    <div class="highlight-box" style="margin-top: 1rem; background: linear-gradient(to right, #E8F7FA, #F0EDF5);">
                        <p style="margin-bottom: 0;">
                            <strong>Curiosidad adicional:</strong> El paper original de AdaBoost tiene m√°s de 15,000 citas
                            y revolucion√≥ el campo del Machine Learning. Fue el primer algoritmo pr√°ctico en demostrar que
                            combinar modelos "d√©biles" pod√≠a crear un clasificador arbitrariamente preciso. La teor√≠a detr√°s
                            de AdaBoost tambi√©n inspir√≥ el desarrollo de XGBoost 15 a√±os despu√©s, que hoy domina Kaggle.
                            ¬°La ciencia construye sobre ciencia!
                        </p>
                    </div>
                </div>
            </section>
        </main>

        <footer>
            <div class="footer-content">
                <img src="../img/logo-ilerna.svg" alt="ILERNA" style="height: 40px; margin-bottom: 1rem;">
                <h3>Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</h3>
                <p><a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a></p>
                <p style="font-size: 0.9rem; color: #777; margin-top: 1rem;">Centro oficial de FP online y presencial.
                    Ciclos formativos de Grado Medio y Grado Superior.</p>
                <p style="font-size: 0.9rem; color: #777;">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>
            </div>
            <div class="penguin">
                <span>üêß</span>
            </div>
        </footer>
    </div>

    <!-- Prism.js para syntax highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <!-- Script para copiar c√≥digo -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const codeBlocks = document.querySelectorAll('pre code');

            codeBlocks.forEach((block) => {
                const pre = block.parentElement;
                const wrapper = document.createElement('div');
                wrapper.style.position = 'relative';

                pre.parentNode.insertBefore(wrapper, pre);
                wrapper.appendChild(pre);

                const button = document.createElement('button');
                button.className = 'copy-code-btn';
                button.innerHTML = `
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                    </svg>
                    <span>Copiar c√≥digo</span>
                `;

                button.addEventListener('click', async () => {
                    const code = block.textContent;

                    try {
                        await navigator.clipboard.writeText(code);
                        button.innerHTML = `
                            <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
                            </svg>
                            <span>¬°Copiado!</span>
                        `;
                        button.style.background = '#43A047';

                        setTimeout(() => {
                            button.innerHTML = `
                                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                                </svg>
                                <span>Copiar c√≥digo</span>
                            `;
                            button.style.background = '#49B9CE';
                        }, 2000);
                    } catch (err) {
                        console.error('Error al copiar:', err);
                    }
                });

                wrapper.appendChild(button);
            });
        });
    </script>
</body>

</html>
