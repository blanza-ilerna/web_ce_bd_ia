<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Conjuntos de Datos: Entrenamiento, Validaci√≥n y Prueba. Aprende a dividir datos para Machine Learning.">
    <title>Conjuntos de Datos: Entrenamiento, Validaci√≥n y Prueba | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
</head>

<body>
    <div class="container">
        <header>
            <div class="header-container">
                <div class="logo-container">
                    <a href="../index.html">
                        <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                    </a>
                    <div class="logo-text">
                        iLERNA
                        <span>Curso de Especializaci√≥n en IA y Big Data</span>
                    </div>
                </div>
                <div class="breadcrumb">
                    <a href="../index.html">Inicio</a> ‚Ä∫
                    <a href="index.html">Sistemas de Aprendizaje Autom√°tico</a> ‚Ä∫
                    <span>Conjuntos de Datos</span>
                </div>
            </div>
            <h1 class="text-center">Conjuntos de Datos: Entrenamiento, Validaci√≥n y Prueba</h1>
        </header>

        <main>

            <!-- Hero Section -->
            <section class="hero-section">
                <div class="hero-content">
                    <h2>üìä Divisi√≥n Estrat√©gica de Datos para Machine Learning</h2>
                    <p class="hero-description">
                        Entrenar y evaluar un modelo con los mismos datos genera una falsa sensaci√≥n de √©xito:
                        el modelo puede memorizar (overfitting) sin aprender patrones generalizables. La soluci√≥n
                        es dividir el dataset en tres subconjuntos independientes: Training Set (60-80%) para
                        aprender patrones, Validation Set (10-20%) para ajustar hiperpar√°metros, y Test Set
                        (10-20%) para evaluaci√≥n final imparcial. Esta lecci√≥n cubre proporciones √≥ptimas,
                        validaci√≥n cruzada K-Fold, y mejores pr√°cticas con c√≥digo Python.
                    </p>
                </div>
            </section>

            <!-- Table of Contents -->
            <nav class="toc-container">
                <h3>üìë Contenido de la Lecci√≥n</h3>
                <ul class="toc-list">
                    <li><a href="#introduccion">1. Introducci√≥n: ¬øPor Qu√© Dividir los Datos?</a></li>
                    <li><a href="#training-set">2. Training Set (Conjunto de Entrenamiento)</a></li>
                    <li><a href="#validation-set">3. Validation Set (Conjunto de Validaci√≥n)</a></li>
                    <li><a href="#test-set">4. Test Set (Conjunto de Prueba)</a></li>
                    <li><a href="#proporciones">5. Proporciones T√≠picas de Divisi√≥n</a></li>
                    <li><a href="#cross-validation">6. Validaci√≥n Cruzada (K-Fold)</a></li>
                    <li><a href="#implementacion">7. Implementaci√≥n Pr√°ctica en Python</a></li>
                    <li><a href="#mejores-practicas">8. Mejores Pr√°cticas y Errores Comunes</a></li>
                </ul>
            </nav>

            <!-- Secci√≥n 1: Introducci√≥n -->
            <section id="introduccion" class="content-section">
                <h2>1. Introducci√≥n: ¬øPor Qu√© Dividir los Datos?</h2>

                <div class="highlight-box warning">
                    <h4>‚ö†Ô∏è El Problema: Overfitting y Evaluaci√≥n Sesgada</h4>
                    <p>
                        Uno de los errores m√°s comunes en Machine Learning es entrenar un modelo y luego
                        evaluarlo con los mismos datos. Esto genera una <strong>falsa sensaci√≥n de √©xito</strong>:
                        el modelo puede obtener 99% de accuracy simplemente porque ha <strong>memorizado</strong>
                        los datos de entrenamiento, sin aprender patrones generalizables.
                    </p>
                    <p>
                        Este fen√≥meno se llama <strong>overfitting</strong> (sobreajuste): el modelo se ajusta
                        perfectamente a los datos de entrenamiento pero falla con datos nuevos.
                    </p>
                </div>

                <h3>La Soluci√≥n: Divisi√≥n en Tres Subconjuntos</h3>
                <p>
                    Para evaluar correctamente el rendimiento de un modelo, dividimos el dataset original
                    en <strong>tres subconjuntos independientes</strong>, cada uno con un prop√≥sito espec√≠fico:
                </p>

                <div style="display: flex; justify-content: center; margin: 3rem 0;">
                    <div class="mermaid" style="width: 100%; max-width: 900px;">
                        graph LR
                        A[Dataset Original<br />100% de datos] --> B[Training Set<br />60-80%]
                        A --> C[Validation Set<br />10-20%]
                        A --> D[Test Set<br />10-20%]

                        B --> E[Modelo aprende<br />ajusta par√°metros]
                        C --> F[Ajustar hiperpar√°metros<br />comparar modelos]
                        D --> G[Evaluaci√≥n final<br />rendimiento real]

                        style A fill:#49B9CE,color:#fff
                        style B fill:#49B9CE,color:#fff
                        style C fill:#8A7AAF,color:#fff
                        style D fill:#43A047,color:#fff
                    </div>
                </div>

                <h3>Esquema Visual: Divisi√≥n de Datos</h3>
                <p>
                    La proporci√≥n m√°s com√∫n es <strong>70% entrenamiento, 15% validaci√≥n y 15% prueba</strong>,
                    distribuidos estrat√©gicamente para asegurar una evaluaci√≥n imparcial:
                </p>

                <div style="display: flex; justify-content: center; margin: 3rem 0;">
                    <svg viewBox="0 0 1000 500" style="width: 100%; max-width: 1000px; display: block;">
                        <defs>
                            <linearGradient id="trainGrad" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#49B9CE;stop-opacity:1" />
                                <stop offset="100%" style="stop-color:#A3E0EA;stop-opacity:1" />
                            </linearGradient>
                            <linearGradient id="valGrad" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#8A7AAF;stop-opacity:1" />
                                <stop offset="100%" style="stop-color:#C5B9D8;stop-opacity:1" />
                            </linearGradient>
                            <linearGradient id="testGrad" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#43A047;stop-opacity:1" />
                                <stop offset="100%" style="stop-color:#A5D6A7;stop-opacity:1" />
                            </linearGradient>
                            <marker id="arrowDiv" markerWidth="10" markerHeight="10" refX="6" refY="3" orient="auto"
                                markerUnits="strokeWidth">
                                <path d="M0,0 L0,6 L6,3 z" fill="#333333" />
                            </marker>
                            <marker id="arrowProc" markerWidth="10" markerHeight="10" refX="6" refY="3" orient="auto"
                                markerUnits="strokeWidth">
                                <path d="M0,0 L0,6 L6,3 z" fill="#49B9CE" />
                            </marker>
                            <marker id="arrowVal" markerWidth="10" markerHeight="10" refX="6" refY="3" orient="auto"
                                markerUnits="strokeWidth">
                                <path d="M0,0 L0,6 L6,3 z" fill="#8A7AAF" />
                            </marker>
                        </defs>

                        <!-- FONDO -->
                        <rect x="0" y="0" width="1000" height="500" rx="20" fill="#f9f9f9" />

                        <!-- DATASET COMPLETO -->
                        <text x="500" y="50" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="20"
                            font-weight="700" fill="#333333">
                            Dataset Completo (100%)
                        </text>
                        <rect x="90" y="70" width="820" height="60" rx="10" fill="#e5e5e5" stroke="#333333"
                            stroke-width="2" />
                        <text x="500" y="108" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="16"
                            fill="#333333">
                            10,000 muestras de datos originales
                        </text>

                        <!-- FLECHA DIVISI√ìN -->
                        <line x1="500" y1="140" x2="500" y2="180" stroke="#333333" stroke-width="3"
                            marker-end="url(#arrowDiv)" />
                        <text x="520" y="165" font-family="Montserrat, sans-serif" font-size="14" fill="#333333"
                            font-weight="700">Divisi√≥n Estrat√©gica</text>

                        <!-- TRAINING SET (70%) -->
                        <rect x="80" y="200" width="580" height="80" rx="10" fill="url(#trainGrad)" stroke="#49B9CE"
                            stroke-width="3" />
                        <text x="370" y="235" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="18"
                            font-weight="700" fill="#ffffff">
                            Training Set (70%)
                        </text>
                        <text x="370" y="260" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="14"
                            fill="#ffffff">
                            7,000 muestras ‚Ä¢ Ajuste de par√°metros
                        </text>

                        <!-- VALIDATION SET (15%) -->
                        <rect x="680" y="200" width="240" height="80" rx="10" fill="url(#valGrad)" stroke="#8A7AAF"
                            stroke-width="3" />
                        <text x="800" y="230" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="16"
                            font-weight="700" fill="#ffffff">
                            Validation Set
                        </text>
                        <text x="800" y="250" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="14"
                            fill="#ffffff">
                            (15%)
                        </text>
                        <text x="800" y="270" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="12"
                            fill="#ffffff">
                            1,500 muestras
                        </text>

                        <!-- TEST SET (15%) -->
                        <rect x="330" y="310" width="340" height="80" rx="10" fill="url(#testGrad)" stroke="#43A047"
                            stroke-width="3" />
                        <text x="500" y="345" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="18"
                            font-weight="700" fill="#ffffff">
                            Test Set (15%)
                        </text>
                        <text x="500" y="370" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="14"
                            fill="#ffffff">
                            1,500 muestras ‚Ä¢ Evaluaci√≥n final
                        </text>

                        <!-- FLECHAS DE PROCESO -->
                        <line x1="370" y1="290" x2="370" y2="310" stroke="#49B9CE" stroke-width="2"
                            marker-end="url(#arrowProc)" />
                        <line x1="800" y1="290" x2="630" y2="310" stroke="#8A7AAF" stroke-width="2"
                            marker-end="url(#arrowVal)" />

                        <!-- RESULTADO FINAL -->
                        <rect x="290" y="420" width="420" height="60" rx="10" fill="#ffffff" stroke="#333333"
                            stroke-width="2" />
                        <text x="500" y="445" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="16"
                            font-weight="700" fill="#333333">
                            ‚úÖ Modelo Evaluado e Imparcial
                        </text>
                        <text x="500" y="465" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="13"
                            fill="#555555">
                            Listo para Despliegue en Producci√≥n
                        </text>

                        <line x1="500" y1="400" x2="500" y2="420" stroke="#333333" stroke-width="2"
                            marker-end="url(#arrowProc)" />
                    </svg>
                </div>

                <div class="highlight-box warning" style="margin-top: 1.5rem;">
                    <h4>‚ö†Ô∏è Regla de Oro: Aislamiento del Test Set</h4>
                    <p>
                        <strong>Nunca uses el test set durante el desarrollo.</strong> Solo se utiliza al final, una
                        √∫nica vez, para obtener la m√©trica definitiva de rendimiento real.
                    </p>
                </div>

            </section>

            <!-- Secci√≥n 2: Training Set -->
            <section id="training-set" class="content-section">
                <h2>2. Training Set (Conjunto de Entrenamiento)</h2>

                <div class="highlight-box" style="border-left: 4px solid #06A77D;">
                    <h3>üìê Definici√≥n</h3>
                    <p>
                        El <strong>Training Set</strong> es el subconjunto de datos que el modelo utiliza para
                        <strong>aprender</strong>. Durante el entrenamiento, el modelo ajusta sus par√°metros
                        internos (pesos en redes neuronales, coeficientes en regresi√≥n lineal, reglas en
                        √°rboles de decisi√≥n) para minimizar el error en estos datos.
                    </p>
                </div>

                <h3>Caracter√≠sticas del Training Set</h3>
                <div class="grid-features">
                    <div class="feature-card">
                        <h4>Tama√±o T√≠pico</h4>
                        <p><strong>60-80%</strong> del dataset total</p>
                        <p>Es el conjunto m√°s grande porque el modelo necesita suficientes ejemplos para aprender
                            patrones robustos.</p>
                    </div>

                    <div class="feature-card">
                        <h4>Qu√© Aprende el Modelo</h4>
                        <ul>
                            <li>Relaciones entre caracter√≠sticas (X) y etiquetas (y)</li>
                            <li>Patrones y regularidades en los datos</li>
                            <li>Fronteras de decisi√≥n para clasificaci√≥n</li>
                            <li>Funciones de mapeo para regresi√≥n</li>
                        </ul>
                    </div>

                    <div class="feature-card">
                        <h4>Riesgo Principal: Overfitting</h4>
                        <p>
                            Si el modelo es muy complejo o entrena demasiado tiempo, puede <strong>memorizar</strong>
                            el training set en lugar de aprender patrones generalizables.
                        </p>
                        <p><strong>Se√±al de alerta:</strong> Accuracy muy alto en training (99%) pero bajo en validation
                            (70%)</p>
                    </div>
                </div>

                <h3>Ejemplo: Entrenamiento de un Clasificador</h3>
                <pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

# Supongamos X_train, y_train ya separados (70% del total)
# El modelo ajusta sus par√°metros internos
modelo = RandomForestClassifier(n_estimators=100, random_state=42)
modelo.fit(X_train, y_train)  # ‚Üê El modelo APRENDE de estos datos

# Despu√©s del fit(), el modelo ha:
# - Construido 100 √°rboles de decisi√≥n
# - Aprendido qu√© combinaciones de caracter√≠sticas predicen cada clase
# - Ajustado las reglas de divisi√≥n en cada nodo

print(f"N√∫mero de caracter√≠sticas usadas: {modelo.n_features_in_}")
print(f"Clases aprendidas: {modelo.classes_}")
# N√∫mero de caracter√≠sticas usadas: 4
# Clases aprendidas: [0 1 2]</code></pre>

            </section>

            <!-- Secci√≥n 3: Validation Set -->
            <section id="validation-set" class="content-section">
                <h2>3. Validation Set (Conjunto de Validaci√≥n)</h2>

                <div class="highlight-box" style="border-left: 4px solid #F18F01;">
                    <h3>üìê Definici√≥n</h3>
                    <p>
                        El <strong>Validation Set</strong> es el subconjunto utilizado para <strong>ajustar
                            hiperpar√°metros</strong> y <strong>comparar diferentes modelos</strong> durante el
                        desarrollo. A diferencia del test set, el validation set <strong>s√≠ influye</strong>
                        en las decisiones del desarrollador.
                    </p>
                </div>

                <h3>Usos Principales del Validation Set</h3>
                <div class="grid-features">
                    <div class="feature-card">
                        <h4>1. Selecci√≥n de Hiperpar√°metros</h4>
                        <p>Encontrar los mejores valores para configuraciones no aprendidas autom√°ticamente:</p>
                        <ul>
                            <li><strong>Learning rate:</strong> Velocidad de aprendizaje</li>
                            <li><strong>N√∫mero de capas/neuronas:</strong> Arquitectura de red</li>
                            <li><strong>Regularizaci√≥n (L1, L2):</strong> Control de overfitting</li>
                            <li><strong>n_estimators, max_depth:</strong> Par√°metros de Random Forest</li>
                        </ul>
                    </div>

                    <div class="feature-card">
                        <h4>2. Comparaci√≥n de Modelos</h4>
                        <p>Evaluar qu√© algoritmo funciona mejor para el problema:</p>
                        <ul>
                            <li>Random Forest vs XGBoost vs Red Neuronal</li>
                            <li>Regresi√≥n Lineal vs Regresi√≥n Polin√≥mica</li>
                            <li>SVM con kernel RBF vs kernel lineal</li>
                        </ul>
                    </div>

                    <div class="feature-card">
                        <h4>3. Detecci√≥n de Overfitting</h4>
                        <p>Monitorizar si el modelo generaliza o solo memoriza:</p>
                        <ul>
                            <li>Si training accuracy sube pero validation baja ‚Üí Overfitting</li>
                            <li><strong>Early Stopping:</strong> Detener entrenamiento cuando validation deja de mejorar
                            </li>
                        </ul>
                    </div>
                </div>

                <h3>Ejemplo: B√∫squeda de Hiperpar√°metros con Validation Set</h3>
                <pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Diferentes configuraciones a probar
hiperparametros = [
    {'n_estimators': 50, 'max_depth': 5},
    {'n_estimators': 100, 'max_depth': 10},
    {'n_estimators': 200, 'max_depth': 15},
    {'n_estimators': 100, 'max_depth': None},  # Sin l√≠mite de profundidad
]

mejores_params = None
mejor_accuracy = 0

# Probar cada configuraci√≥n
for params in hiperparametros:
    modelo = RandomForestClassifier(**params, random_state=42)
    modelo.fit(X_train, y_train)

    # Evaluar en VALIDATION set (no en test)
    y_val_pred = modelo.predict(X_val)
    accuracy = accuracy_score(y_val, y_val_pred)

    print(f"Params: {params}")
    print(f"  ‚Üí Accuracy en VALIDATION: {accuracy:.3f}")

    if accuracy > mejor_accuracy:
        mejor_accuracy = accuracy
        mejores_params = params

print(f"\n‚úÖ Mejores hiperpar√°metros: {mejores_params}")
print(f"‚úÖ Mejor accuracy en validation: {mejor_accuracy:.3f}")

# Salida t√≠pica:
# Params: {'n_estimators': 50, 'max_depth': 5}
#   ‚Üí Accuracy en VALIDATION: 0.909
# Params: {'n_estimators': 100, 'max_depth': 10}
#   ‚Üí Accuracy en VALIDATION: 0.955
# Params: {'n_estimators': 200, 'max_depth': 15}
#   ‚Üí Accuracy en VALIDATION: 0.955
# Params: {'n_estimators': 100, 'max_depth': None}
#   ‚Üí Accuracy en VALIDATION: 0.932
#
# ‚úÖ Mejores hiperpar√°metros: {'n_estimators': 100, 'max_depth': 10}
# ‚úÖ Mejor accuracy en validation: 0.955</code></pre>

                <div class="highlight-box info">
                    <h4>üí° Diferencia Clave: Validation vs Test</h4>
                    <p>
                        El <strong>validation set</strong> se usa m√∫ltiples veces durante el desarrollo para
                        tomar decisiones (qu√© hiperpar√°metros usar, qu√© modelo elegir). Por tanto, existe
                        cierto <strong>sesgo indirecto</strong>: optimizamos para este conjunto.
                    </p>
                    <p>
                        El <strong>test set</strong> se usa <strong>una √∫nica vez</strong> al final, cuando
                        todas las decisiones ya est√°n tomadas. Proporciona una estimaci√≥n <strong>imparcial</strong>
                        del rendimiento real.
                    </p>
                </div>

            </section>

            <!-- Secci√≥n 4: Test Set -->
            <section id="test-set" class="content-section">
                <h2>4. Test Set (Conjunto de Prueba)</h2>

                <div class="highlight-box" style="border-left: 4px solid #C73E1D;">
                    <h3>üìê Definici√≥n</h3>
                    <p>
                        El <strong>Test Set</strong> es el subconjunto reservado para la <strong>evaluaci√≥n
                            final</strong> del modelo. Se utiliza <strong>una √∫nica vez</strong>, cuando el modelo
                        est√° completamente entrenado y sus hiperpar√°metros ya han sido ajustados.
                    </p>
                    <p>
                        <strong>Regla de oro:</strong> El test set debe permanecer <strong>intocable</strong>
                        hasta el momento de la evaluaci√≥n final.
                    </p>
                </div>

                <h3>Caracter√≠sticas del Test Set</h3>
                <div class="grid-features">
                    <div class="feature-card" style="border-left: 3px solid #C73E1D;">
                        <h4>Prop√≥sito √önico</h4>
                        <p>
                            Obtener una <strong>estimaci√≥n imparcial</strong> del rendimiento del modelo
                            en datos completamente nuevos, simulando su comportamiento en producci√≥n.
                        </p>
                    </div>

                    <div class="feature-card" style="border-left: 3px solid #C73E1D;">
                        <h4>Tama√±o T√≠pico</h4>
                        <p><strong>10-20%</strong> del dataset total</p>
                        <p>Debe ser suficientemente grande para proporcionar m√©tricas estad√≠sticamente significativas.
                        </p>
                    </div>

                    <div class="feature-card" style="border-left: 3px solid #C73E1D;">
                        <h4>Uso √önico</h4>
                        <p>
                            Se eval√∫a <strong>una sola vez</strong>. Si ajustas el modelo despu√©s de ver
                            resultados del test set, introduces <strong>data leakage</strong> y la evaluaci√≥n
                            deja de ser imparcial.
                        </p>
                    </div>
                </div>

                <h3>Ejemplo: Evaluaci√≥n Final en Test Set</h3>
                <pre><code class="language-python">from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Entrenar modelo final con los mejores hiperpar√°metros encontrados
modelo_final = RandomForestClassifier(**mejores_params, random_state=42)
modelo_final.fit(X_train, y_train)

# ============================================
# EVALUACI√ìN FINAL EN TEST SET (√öNICA VEZ)
# ============================================
y_test_pred = modelo_final.predict(X_test)

# M√©tricas de rendimiento
accuracy_test = accuracy_score(y_test, y_test_pred)
print(f"üéØ ACCURACY EN TEST SET: {accuracy_test:.3f}")

# Reporte detallado por clase
print("\nüìä Classification Report:")
print(classification_report(y_test, y_test_pred, target_names=['setosa', 'versicolor', 'virginica']))

# Matriz de confusi√≥n
print("\nüî¢ Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))

# Salida t√≠pica:
# üéØ ACCURACY EN TEST SET: 0.957
#
# üìä Classification Report:
#               precision    recall  f1-score   support
#       setosa       1.00      1.00      1.00         8
#   versicolor       0.88      1.00      0.93         7
#    virginica       1.00      0.88      0.93         8
#     accuracy                           0.96        23
#    macro avg       0.96      0.96      0.96        23
# weighted avg       0.96      0.96      0.96        23
#
# üî¢ Confusion Matrix:
# [[8 0 0]
#  [0 7 0]
#  [0 1 7]]</code></pre>

                <div class="highlight-box warning">
                    <h4>‚ö†Ô∏è Error Grave: Usar el Test Set M√∫ltiples Veces</h4>
                    <p>
                        Si eval√∫as en el test set, ajustas el modelo bas√°ndote en esos resultados, y vuelves
                        a evaluar, est√°s cometiendo <strong>data leakage</strong>. El test set ha "contaminado"
                        tus decisiones y la evaluaci√≥n ya no es imparcial.
                    </p>
                    <p><strong>Consecuencia:</strong> El modelo puede parecer mejor de lo que realmente es en
                        producci√≥n.</p>
                </div>

            </section>

            <!-- Secci√≥n 5: Proporciones -->
            <section id="proporciones" class="content-section">
                <h2>5. Proporciones T√≠picas de Divisi√≥n</h2>

                <div class="feature-card">
                    <h3>Proporciones Est√°ndar</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Conjunto</th>
                                <th>Proporci√≥n T√≠pica</th>
                                <th>Ejemplo (10,000 muestras)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Training Set</strong></td>
                                <td>60-80%</td>
                                <td>7,000 muestras</td>
                            </tr>
                            <tr>
                                <td><strong>Validation Set</strong></td>
                                <td>10-20%</td>
                                <td>1,500 muestras</td>
                            </tr>
                            <tr>
                                <td><strong>Test Set</strong></td>
                                <td>10-20%</td>
                                <td>1,500 muestras</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3>Divisi√≥n M√°s Com√∫n: 70/15/15</h3>
                <div style="display: flex; justify-content: center; margin: 3rem 0;">
                    <div class="mermaid" style="width: 100%; max-width: 600px;">
                        pie title Divisi√≥n de Dataset (70/15/15)
                        "Training Set (70%)" : 70
                        "Validation Set (15%)" : 15
                        "Test Set (15%)" : 15
                    </div>
                </div>

                <h3>Factores que Influyen en las Proporciones</h3>
                <div class="grid-features">
                    <div class="feature-card">
                        <h4>Dataset Grande (&gt;100,000 muestras)</h4>
                        <p>Se puede usar proporci√≥n m√°s peque√±a para validation/test:</p>
                        <ul>
                            <li><strong>80/10/10</strong> o incluso <strong>90/5/5</strong></li>
                            <li>10,000 muestras de test siguen siendo estad√≠sticamente significativas</li>
                        </ul>
                    </div>

                    <div class="feature-card">
                        <h4>Dataset Peque√±o (&lt;1,000 muestras)</h4>
                        <p>Necesitas maximizar datos de entrenamiento:</p>
                        <ul>
                            <li><strong>60/20/20</strong> para mantener suficientes datos en cada conjunto</li>
                            <li>Considerar <strong>validaci√≥n cruzada</strong> en lugar de validation set fijo</li>
                        </ul>
                    </div>

                    <div class="feature-card">
                        <h4>Clases Desbalanceadas</h4>
                        <p>Asegurar que cada clase est√© representada en todos los conjuntos:</p>
                        <ul>
                            <li>Usar <strong>stratified split</strong> (stratify=y)</li>
                            <li>Verificar que clases minoritarias tengan suficientes muestras en test</li>
                        </ul>
                    </div>
                </div>

                <h3>Ejemplo: Divisi√≥n con Estratificaci√≥n</h3>
                <pre><code class="language-python">from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
import numpy as np

# Cargar dataset
iris = load_iris()
X, y = iris.data, iris.target

print(f"Dataset total: {len(y)} muestras")
# Distribuci√≥n de clases corregida para sintaxis
unique, counts = np.unique(y, return_counts=True)
print(f"Distribuci√≥n de clases: {dict(zip(unique, counts))}")
# Dataset total: 150 muestras
# Distribuci√≥n de clases: {0: 50, 1: 50, 2: 50}

# PASO 1: Separar 70% training, 30% temporal (para val + test)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.30,      # 30% para validation + test
    random_state=42,     # Reproducibilidad
    stratify=y           # Mantener proporci√≥n de clases
)

# PASO 2: Dividir el 30% temporal en 50/50 (15% val, 15% test del total)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.50,      # 50% del temporal = 15% del total
    random_state=42,
    stratify=y_temp
)

# Verificar tama√±os
print(f"\nüìä Divisi√≥n final:")
print(f"  Training:   {len(X_train)} muestras ({len(X_train)/len(X)*100:.0f}%)")
print(f"  Validation: {len(X_val)} muestras ({len(X_val)/len(X)*100:.0f}%)")
print(f"  Test:       {len(X_test)} muestras ({len(X_test)/len(X)*100:.0f}%)")

# Verificar estratificaci√≥n (proporci√≥n de clases)
print(f"\nüéØ Verificaci√≥n de estratificaci√≥n:")
print(f"  Training - Clase 0: {sum(y_train==0)}, Clase 1: {sum(y_train==1)}, Clase 2: {sum(y_train==2)}")
print(f"  Validation - Clase 0: {sum(y_val==0)}, Clase 1: {sum(y_val==1)}, Clase 2: {sum(y_val==2)}")
print(f"  Test - Clase 0: {sum(y_test==0)}, Clase 1: {sum(y_test==1)}, Clase 2: {sum(y_test==2)}")

# Salida:
# üìä Divisi√≥n final:
#   Training:   105 muestras (70%)
#   Validation: 22 muestras (15%)
#   Test:       23 muestras (15%)
#
# üéØ Verificaci√≥n de estratificaci√≥n:
#   Training - Clase 0: 35, Clase 1: 35, Clase 2: 35
#   Validation - Clase 0: 7, Clase 1: 8, Clase 2: 7
#   Test - Clase 0: 8, Clase 1: 7, Clase 2: 8</code></pre>

            </section>

            <!-- Secci√≥n 6: Cross-Validation (Readaptada de Old) -->
            <section id="cross-validation" class="content-section">
                <h2>6. Validaci√≥n Cruzada (K-Fold Cross-Validation)</h2>
                <h3>Maximizando el uso de los datos</h3>

                <p>
                    Cuando el dataset es <strong>peque√±o</strong>, dividir en train/val/test puede dejar muy pocas
                    muestras
                    para validaci√≥n. La soluci√≥n es usar <strong>validaci√≥n cruzada (k-fold cross-validation)</strong>:
                </p>

                <div class="feature-card" style="margin-bottom: 2rem;">
                    <ul>
                        <li>Se divide el dataset en <strong>k particiones</strong> (folds), por ejemplo k=5.</li>
                        <li>Se entrena el modelo <strong>k veces</strong>, usando cada vez una partici√≥n diferente como
                            validaci√≥n y el resto como entrenamiento.</li>
                        <li>Se promedian las m√©tricas de las k iteraciones para obtener una <strong>estimaci√≥n m√°s
                                robusta</strong>.</li>
                        <li>El test set se mantiene <strong>completamente separado</strong> y solo se usa al final.</li>
                    </ul>
                </div>

                <!-- Diagrama conceptual de k-fold (SVG Eficiente) -->
                <div style="display: flex; justify-content: center; margin: 3rem 0;">
                    <svg viewBox="0 0 900 320" style="width: 100%; max-width: 900px; display: block;">
                        <defs>
                            <linearGradient id="foldTrain" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#49B9CE;stop-opacity:0.8" />
                                <stop offset="100%" style="stop-color:#A3E0EA;stop-opacity:0.8" />
                            </linearGradient>
                            <linearGradient id="foldVal" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#8A7AAF;stop-opacity:0.9" />
                                <stop offset="100%" style="stop-color:#C5B9D8;stop-opacity:0.9" />
                            </linearGradient>
                        </defs>

                        <rect x="0" y="0" width="900" height="320" rx="15" fill="#f5f5f5" />

                        <text x="450" y="35" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="18"
                            font-weight="700" fill="#333333">
                            5-Fold Cross-Validation
                        </text>

                        <!-- Fold 1 -->
                        <text x="30" y="75" font-family="Montserrat, sans-serif" font-size="13" font-weight="700"
                            fill="#333333">Fold
                            1:</text>
                        <rect x="100" y="55" width="140" height="30" rx="5" fill="url(#foldVal)" stroke="#8A7AAF"
                            stroke-width="2" />
                        <text x="170" y="75" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="11"
                            fill="#ffffff">Validaci√≥n</text>
                        <rect x="245" y="55" width="560" height="30" rx="5" fill="url(#foldTrain)" stroke="#49B9CE"
                            stroke-width="2" />
                        <text x="525" y="75" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="11"
                            fill="#ffffff">Entrenamiento</text>

                        <!-- Fold 2 -->
                        <text x="30" y="120" font-family="Montserrat, sans-serif" font-size="13" font-weight="700"
                            fill="#333333">Fold
                            2:</text>
                        <rect x="100" y="100" width="140" height="30" rx="5" fill="url(#foldTrain)" stroke="#49B9CE"
                            stroke-width="2" />
                        <rect x="245" y="100" width="140" height="30" rx="5" fill="url(#foldVal)" stroke="#8A7AAF"
                            stroke-width="2" />
                        <text x="315" y="120" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="11"
                            fill="#ffffff">Validaci√≥n</text>
                        <rect x="390" y="100" width="415" height="30" rx="5" fill="url(#foldTrain)" stroke="#49B9CE"
                            stroke-width="2" />

                        <!-- Fold 3 -->
                        <text x="30" y="165" font-family="Montserrat, sans-serif" font-size="13" font-weight="700"
                            fill="#333333">Fold
                            3:</text>
                        <rect x="100" y="145" width="280" height="30" rx="5" fill="url(#foldTrain)" stroke="#49B9CE"
                            stroke-width="2" />
                        <rect x="385" y="145" width="140" height="30" rx="5" fill="url(#foldVal)" stroke="#8A7AAF"
                            stroke-width="2" />
                        <text x="455" y="165" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="11"
                            fill="#ffffff">Validaci√≥n</text>
                        <rect x="530" y="145" width="275" height="30" rx="5" fill="url(#foldTrain)" stroke="#49B9CE"
                            stroke-width="2" />

                        <!-- Fold 4 -->
                        <text x="30" y="210" font-family="Montserrat, sans-serif" font-size="13" font-weight="700"
                            fill="#333333">Fold
                            4:</text>
                        <rect x="100" y="190" width="420" height="30" rx="5" fill="url(#foldTrain)" stroke="#49B9CE"
                            stroke-width="2" />
                        <rect x="525" y="190" width="140" height="30" rx="5" fill="url(#foldVal)" stroke="#8A7AAF"
                            stroke-width="2" />
                        <text x="595" y="210" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="11"
                            fill="#ffffff">Validaci√≥n</text>
                        <rect x="670" y="190" width="135" height="30" rx="5" fill="url(#foldTrain)" stroke="#49B9CE"
                            stroke-width="2" />

                        <!-- Fold 5 -->
                        <text x="30" y="255" font-family="Montserrat, sans-serif" font-size="13" font-weight="700"
                            fill="#333333">Fold
                            5:</text>
                        <rect x="100" y="235" width="560" height="30" rx="5" fill="url(#foldTrain)" stroke="#49B9CE"
                            stroke-width="2" />
                        <rect x="665" y="235" width="140" height="30" rx="5" fill="url(#foldVal)" stroke="#8A7AAF"
                            stroke-width="2" />
                        <text x="735" y="255" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="11"
                            fill="#ffffff">Validaci√≥n</text>

                        <!-- Resultado -->
                        <text x="450" y="295" text-anchor="middle" font-family="Montserrat, sans-serif" font-size="14"
                            font-weight="700" fill="#333333">
                            Promedio de 5 m√©tricas ‚Üí Estimaci√≥n robusta del rendimiento
                        </text>
                    </svg>
                </div>

                <h3>Ejemplo en Python ‚Äì Validaci√≥n cruzada con scikit-learn</h3>
                <pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

# Crear modelo
modelo = RandomForestClassifier(n_estimators=100, random_state=42)

# Validaci√≥n cruzada con 5 folds
# Nota: aqu√≠ usamos X_train e y_train (sin tocar el test set)
scores = cross_val_score(modelo, X_train, y_train, cv=5, scoring='accuracy')

print("üìä Accuracy en cada fold:", scores)
print(f"üìà Accuracy promedio: {scores.mean():.3f} (+/- {scores.std():.3f})")

# Ejemplo de salida:
# Accuracy en cada fold: [0.952 0.905 0.952 0.905 0.905]
# Accuracy promedio: 0.924 (+/- 0.022)

# Ahora entrenar el modelo final con TODOS los datos de entrenamiento
# y evaluar en el test set
modelo.fit(X_train, y_train)
accuracy_final = modelo.score(X_test, y_test)
print(f"\n‚úÖ Accuracy final en test set: {accuracy_final:.3f}")</code></pre>

                <div class="highlight-box success">
                    <h4>‚úÖ Ventajas de K-Fold Cross-Validation</h4>
                    <ul>
                        <li><strong>Maximiza uso de datos:</strong> Cada muestra se usa tanto para entrenar como para
                            validar.</li>
                        <li><strong>Estimaci√≥n m√°s robusta:</strong> El promedio de K evaluaciones es m√°s fiable que una
                            sola.</li>
                        <li><strong>Detecta varianza:</strong> La desviaci√≥n est√°ndar indica qu√© tan estable es el
                            modelo.</li>
                        <li><strong>Ideal para datasets peque√±os:</strong> No "desperdicias" datos en un validation set
                            fijo.</li>
                    </ul>
                </div>
            </section>

            <!-- Secci√≥n 7: Implementaci√≥n -->
            <section id="implementacion" class="content-section">
                <h2>7. Implementaci√≥n Pr√°ctica Completa en Python</h2>

                <h3>Pipeline Completo: De Divisi√≥n a Evaluaci√≥n Final</h3>
                <pre><code class="language-python">"""
Pipeline completo de divisi√≥n de datos y evaluaci√≥n en Machine Learning
Dataset: Iris (clasificaci√≥n multiclase)
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ============================================
# 1. CARGAR Y EXPLORAR DATOS
# ============================================
print("=" * 60)
print("1. CARGA DE DATOS")
print("=" * 60)

iris = load_iris()
X, y = iris.data, iris.target

print(f"Total de muestras: {len(y)}")
print(f"Caracter√≠sticas: {iris.feature_names}")
print(f"Clases: {iris.target_names}")
# Correcci√≥n sintaxis para print
unique, counts = np.unique(y, return_counts=True)
print(f"Distribuci√≥n: {dict(zip(iris.target_names, counts))}")

# ============================================
# 2. DIVISI√ìN EN TRAIN / VALIDATION / TEST
# ============================================
print("\n" + "=" * 60)
print("2. DIVISI√ìN DE DATOS (70% / 15% / 15%)")
print("=" * 60)

# Primera divisi√≥n: 70% train, 30% temporal
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)

# Segunda divisi√≥n: 50% validation, 50% test (del temporal)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
)

print(f"Training set:   {len(X_train)} muestras ({len(X_train)/len(X)*100:.0f}%)")
print(f"Validation set: {len(X_val)} muestras ({len(X_val)/len(X)*100:.0f}%)")
print(f"Test set:       {len(X_test)} muestras ({len(X_test)/len(X)*100:.0f}%)")

# ============================================
# 3. B√öSQUEDA DE HIPERPAR√ÅMETROS (usando Validation Set)
# ============================================
print("\n" + "=" * 60)
print("3. B√öSQUEDA DE HIPERPAR√ÅMETROS")
print("=" * 60)

hiperparametros = [
    {'n_estimators': 50, 'max_depth': 5},
    {'n_estimators': 100, 'max_depth': 10},
    {'n_estimators': 200, 'max_depth': 15},
    {'n_estimators': 100, 'max_depth': None},
]

mejores_params = None
mejor_accuracy_val = 0

for params in hiperparametros:
    modelo = RandomForestClassifier(**params, random_state=42)
    modelo.fit(X_train, y_train)

    # Evaluar en VALIDATION (no en test)
    accuracy_val = accuracy_score(y_val, modelo.predict(X_val))
    accuracy_train = accuracy_score(y_train, modelo.predict(X_train))

    # Detectar overfitting: gran diferencia entre train y validation
    gap = accuracy_train - accuracy_val
    status = "‚ö†Ô∏è Posible overfitting" if gap > 0.1 else "‚úÖ OK"

    print(f"Params: {params}")
    print(f"  Train: {accuracy_train:.3f} | Val: {accuracy_val:.3f} | Gap: {gap:.3f} {status}")

    if accuracy_val > mejor_accuracy_val:
        mejor_accuracy_val = accuracy_val
        mejores_params = params

print(f"\nüèÜ Mejores hiperpar√°metros: {mejores_params}")
print(f"üèÜ Mejor accuracy en validation: {mejor_accuracy_val:.3f}")

# ============================================
# 4. VALIDACI√ìN CRUZADA (opcional, para estimaci√≥n m√°s robusta)
# ============================================
print("\n" + "=" * 60)
print("4. VALIDACI√ìN CRUZADA (5-Fold)")
print("=" * 60)

modelo_cv = RandomForestClassifier(**mejores_params, random_state=42)
cv_scores = cross_val_score(modelo_cv, X_train, y_train, cv=5, scoring='accuracy')

print(f"Scores por fold: {cv_scores}")
print(f"Promedio: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")

# ============================================
# 5. ENTRENAMIENTO DEL MODELO FINAL
# ============================================
print("\n" + "=" * 60)
print("5. ENTRENAMIENTO DEL MODELO FINAL")
print("=" * 60)

modelo_final = RandomForestClassifier(**mejores_params, random_state=42)
modelo_final.fit(X_train, y_train)

print(f"Modelo entrenado con {len(X_train)} muestras")
print(f"Hiperpar√°metros: {mejores_params}")

# ============================================
# 6. EVALUACI√ìN FINAL EN TEST SET (UNA SOLA VEZ)
# ============================================
print("\n" + "=" * 60)
print("6. EVALUACI√ìN FINAL EN TEST SET")
print("=" * 60)

y_test_pred = modelo_final.predict(X_test)
accuracy_test = accuracy_score(y_test, y_test_pred)

print(f"\nüéØ ACCURACY FINAL EN TEST SET: {accuracy_test:.3f}")

print("\nüìä Classification Report:")
print(classification_report(y_test, y_test_pred, target_names=iris.target_names))

print("üî¢ Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))

# ============================================
# 7. RESUMEN FINAL
# ============================================
print("\n" + "=" * 60)
print("7. RESUMEN")
print("=" * 60)
print(f"Dataset total: {len(X)} muestras")
print(f"Divisi√≥n: 70% train / 15% val / 15% test")
print(f"Modelo seleccionado: RandomForestClassifier")
print(f"Mejores hiperpar√°metros: {mejores_params}")
print(f"Cross-validation (5-fold): {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")
print(f"Accuracy final en TEST: {accuracy_test:.3f}")</code></pre>

            </section>

            <!-- Secci√≥n 8: Mejores Pr√°cticas -->
            <section id="mejores-practicas" class="content-section">
                <h2>8. Mejores Pr√°cticas y Errores Comunes</h2>

                <h3>Tabla Comparativa de Conjuntos</h3>
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Conjunto</th>
                                <th>Prop√≥sito Principal</th>
                                <th>Cu√°ndo se Usa</th>
                                <th>Tama√±o T√≠pico</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Training Set</strong></td>
                                <td>Ajustar par√°metros internos (pesos)</td>
                                <td>Durante el entrenamiento</td>
                                <td>60-80%</td>
                            </tr>
                            <tr>
                                <td><strong>Validation Set</strong></td>
                                <td>Ajustar hiperpar√°metros y detectar overfitting</td>
                                <td>Durante el desarrollo</td>
                                <td>10-20%</td>
                            </tr>
                            <tr>
                                <td><strong>Test Set</strong></td>
                                <td>Evaluaci√≥n final imparcial</td>
                                <td>Solo al final (una vez)</td>
                                <td>10-20%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3>Mejores Pr√°cticas</h3>
                <div class="vertical-features"
                    style="display: flex; flex-direction: column; gap: 1.5rem; margin-bottom: 2rem;">
                    <div class="feature-card" style="border-left: 5px solid #06A77D;">
                        <h4>1. Usar Estratificaci√≥n</h4>
                        <p>
                            Siempre usar <code>stratify=y</code> en problemas de clasificaci√≥n para mantener la
                            proporci√≥n
                            natural de las clases en todos los subconjuntos (especialmente cr√≠tico con clases
                            minoritarias).
                        </p>
                        <pre><code class="language-python"># Mantener balance de clases en train/test
train_test_split(X, y, test_size=0.2, stratify=y)</code></pre>
                    </div>

                    <div class="feature-card" style="border-left: 5px solid #49B9CE;">
                        <h4>2. Fijar la Semilla de Aleatoriedad</h4>
                        <p>
                            Utilizar siempre <code>random_state</code>. Esto garantiza que tus experimentos sean
                            <strong>reproducibles</strong> por otros investigadores o por ti mismo en el futuro.
                        </p>
                        <pre><code class="language-python"># Garantizar reproducibilidad
random_state=42</code></pre>
                    </div>

                    <div class="feature-card" style="border-left: 5px solid #8A7AAF;">
                        <h4>3. Aislamiento Estricto: Ajustar solo con Entrenamiento</h4>
                        <p>
                            Cualquier preprocesamiento (escalado, codificaci√≥n) debe aprender sus par√°metros
                            <strong>solo</strong>
                            del Training Set para evitar que informaci√≥n del futuro "se filtre" al modelo.
                        </p>
                        <pre><code class="language-python"># ‚úÖ CORRECTO: fit solo en train
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)</code></pre>
                    </div>
                </div>

                <h3>Errores Comunes a Evitar</h3>
                <div class="vertical-features"
                    style="display: flex; flex-direction: column; gap: 1.5rem; margin-bottom: 2rem;">
                    <div class="highlight-box error"
                        style="background-color: #FFF5F5; border-left: 5px solid #D32F2F; margin-bottom: 0;">
                        <h4 style="color: #C62828;">‚ùå Error 1: Evaluaci√≥n con Datos de Entrenamiento</h4>
                        <p>
                            Reportar la precisi√≥n del modelo usando los mismos datos con los que aprendi√≥. El resultado
                            ser√° enga√±osamente alto debido a la memorizaci√≥n (overfitting).
                        </p>
                    </div>

                    <div class="highlight-box error"
                        style="background-color: #FFF5F5; border-left: 5px solid #D32F2F; margin-bottom: 0;">
                        <h4 style="color: #C62828;">‚ùå Error 2: Reutilizar el Test Set</h4>
                        <p>
                            Si modificas tu modelo bas√°ndote en los resultados del Test Set, est√°s optimizando para
                            ese conjunto espec√≠fico. El Test Set se convierte en un "Validation Set" viciado.
                        </p>
                    </div>

                    <div class="highlight-box error"
                        style="background-color: #FFF5F5; border-left: 5px solid #D32F2F; margin-bottom: 0;">
                        <h4 style="color: #C62828;">‚ùå Error 3: Filtraci√≥n de Datos (Data Leakage)</h4>
                        <p>
                            Incluir por error variables que contienen la respuesta o realizar ingenier√≠a de
                            caracter√≠sticas
                            usando estad√≠sticas calculadas sobre todo el dataset (incluyendo el test).
                        </p>
                    </div>
                </div>

                <div
                    style="margin-top: 3rem; padding: 2.5rem; position: relative; background: linear-gradient(to right, var(--bg-primary-light), var(--bg-secondary-light)); border-radius: 1rem; border-left: 5px solid var(--color-primary); box-shadow: 0 4px 12px rgba(73, 185, 206, 0.15);">
                    <div
                        style="position: absolute; top: -25px; left: 20px; font-size: 2.5rem; background: white; padding: 0.5rem; border-radius: 50%; box-shadow: 0 4px 10px rgba(0,0,0,0.1);">
                        üéß</div>
                    <h3 style="color: var(--color-primary); margin-top: 0.5rem; font-weight: 700;">Caso de Estudio:
                        Spotify</h3>
                    <p style="font-size: 1.1rem; line-height: 1.8; color: var(--text-medium); font-style: italic;">
                        "El sistema de recomendaci√≥n de Spotify utiliza una divisi√≥n rigurosa de datos para asegurar que
                        las nuevas canciones sugeridas realmente gusten a los usuarios. Utilizan millones de
                        interacciones
                        hist√≥ricas para el <strong>Training Set</strong>, ajustan sus algoritmos de recomendaci√≥n con el
                        <strong>Validation Set</strong> y finalmente miden el compromiso (engagement) real con un
                        <strong>Test Set</strong> de usuarios aislados antes de desplegar cualquier mejora a nivel
                        global."
                    </p>
                    <span class="quote-author">‚Äî Ingenier√≠a de Machine Learning de Spotify</span>
                </div>

            </section>

        </main>

        <footer>
            <div class="footer-content">
                <img src="../img/logo-ilerna.svg" alt="ILERNA" style="height: 40px; margin-bottom: 1rem;">
                <h3>Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</h3>
                <p><a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a></p>
                <p style="font-size: 0.9rem; color: #777; margin-top: 1rem;">Centro oficial de FP online y presencial.
                    Ciclos formativos de Grado Medio y Grado Superior.</p>
                <p style="font-size: 0.9rem; color: #777;">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>
            </div>
            <div class="penguin">
                <span>üêß</span>
            </div>
        </footer>
    </div>

    <!-- Prism.js para syntax highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <!-- Script para copiar c√≥digo -->
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const codeBlocks = document.querySelectorAll('pre code');

            codeBlocks.forEach((block) => {
                const pre = block.parentElement;
                const wrapper = document.createElement('div');
                wrapper.style.position = 'relative';

                pre.parentNode.insertBefore(wrapper, pre);
                wrapper.appendChild(pre);

                const button = document.createElement('button');
                button.className = 'copy-code-btn';
                button.innerHTML = `
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                    </svg>
                    <span>Copiar c√≥digo</span>
                `;

                button.addEventListener('click', async () => {
                    const code = block.textContent;

                    try {
                        await navigator.clipboard.writeText(code);
                        button.innerHTML = `
                            <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
                            </svg>
                            <span>¬°Copiado!</span>
                        `;
                        button.style.background = '#43A047';

                        setTimeout(() => {
                            button.innerHTML = `
                                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                                </svg>
                                <span>Copiar c√≥digo</span>
                            `;
                            button.style.background = '#49B9CE';
                        }, 2000);
                    } catch (err) {
                        console.error('Error al copiar:', err);
                        button.innerHTML = '<span>Error</span>';
                        button.style.background = '#F44336';
                    }
                });

                wrapper.appendChild(button);
            });
        });

        // Smooth scroll para TOC
        document.querySelectorAll('.toc-list a').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                }
            });
        });

        // Inicializar Mermaid
        mermaid.initialize({
            startOnLoad: true,
            theme: 'base',
            themeVariables: {
                fontFamily: 'Montserrat, sans-serif',
                fontSize: '20px', // Aumentado para mayor legibilidad
                primaryColor: '#49B9CE',
                secondaryColor: '#8A7AAF',
                tertiaryColor: '#E65100',
                pie1: '#49B9CE',
                pie2: '#8A7AAF',
                pie3: '#E65100',
                edgeLabelBackground: '#ffffff',
            },
            flowchart: {
                useMaxWidth: true,
                htmlLabels: true,
                curve: 'basis'
            },
            pie: {
                useMaxWidth: true
            }
        });
    </script>

</body>

</html>