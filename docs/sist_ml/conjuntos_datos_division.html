<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conjuntos de Datos: Entrenamiento, Validaci√≥n y Prueba - iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
</head>
<body>
    <!-- Header iLERNA -->
    <header class="header-ilerna">
        <div class="logo-container">
            <img src="../images/logo-ilerna.png" alt="iLERNA" class="logo">
        </div>
        <h1 class="main-title">Conjuntos de Datos: Entrenamiento, Validaci√≥n y Prueba</h1>
        <p class="subtitle">Sistemas de Aprendizaje Autom√°tico - M√≥dulo 2.3.1</p>
    </header>

    <!-- Hero Section -->
    <section class="hero-section">
        <div class="hero-content">
            <h2>üìä Divisi√≥n Estrat√©gica de Datos para Machine Learning</h2>
            <p class="hero-description">
                Entrenar y evaluar un modelo con los mismos datos genera una falsa sensaci√≥n de √©xito:
                el modelo puede memorizar (overfitting) sin aprender patrones generalizables. La soluci√≥n
                es dividir el dataset en tres subconjuntos independientes: Training Set (60-80%) para
                aprender patrones, Validation Set (10-20%) para ajustar hiperpar√°metros, y Test Set
                (10-20%) para evaluaci√≥n final imparcial. Esta lecci√≥n cubre proporciones √≥ptimas,
                validaci√≥n cruzada K-Fold, y mejores pr√°cticas con c√≥digo Python.
            </p>
        </div>
    </section>

    <!-- Table of Contents -->
    <nav class="toc-container">
        <h3>üìë Contenido de la Lecci√≥n</h3>
        <ul class="toc-list">
            <li><a href="#introduccion">1. Introducci√≥n: ¬øPor Qu√© Dividir los Datos?</a></li>
            <li><a href="#training-set">2. Training Set (Conjunto de Entrenamiento)</a></li>
            <li><a href="#validation-set">3. Validation Set (Conjunto de Validaci√≥n)</a></li>
            <li><a href="#test-set">4. Test Set (Conjunto de Prueba)</a></li>
            <li><a href="#proporciones">5. Proporciones T√≠picas de Divisi√≥n</a></li>
            <li><a href="#cross-validation">6. Validaci√≥n Cruzada (K-Fold)</a></li>
            <li><a href="#implementacion">7. Implementaci√≥n Pr√°ctica en Python</a></li>
            <li><a href="#mejores-practicas">8. Mejores Pr√°cticas y Errores Comunes</a></li>
        </ul>
    </nav>

    <!-- Main Content -->
    <main class="main-content">

        <!-- Secci√≥n 1: Introducci√≥n -->
        <section id="introduccion" class="content-section">
            <h2>1. Introducci√≥n: ¬øPor Qu√© Dividir los Datos?</h2>

            <div class="highlight-box warning">
                <h4>‚ö†Ô∏è El Problema: Overfitting y Evaluaci√≥n Sesgada</h4>
                <p>
                    Uno de los errores m√°s comunes en Machine Learning es entrenar un modelo y luego
                    evaluarlo con los mismos datos. Esto genera una <strong>falsa sensaci√≥n de √©xito</strong>:
                    el modelo puede obtener 99% de accuracy simplemente porque ha <strong>memorizado</strong>
                    los datos de entrenamiento, sin aprender patrones generalizables.
                </p>
                <p>
                    Este fen√≥meno se llama <strong>overfitting</strong> (sobreajuste): el modelo se ajusta
                    perfectamente a los datos de entrenamiento pero falla con datos nuevos.
                </p>
            </div>

            <h3>La Soluci√≥n: Divisi√≥n en Tres Subconjuntos</h3>
            <p>
                Para evaluar correctamente el rendimiento de un modelo, dividimos el dataset original
                en <strong>tres subconjuntos independientes</strong>, cada uno con un prop√≥sito espec√≠fico:
            </p>

            <div class="mermaid">
graph LR
    A[Dataset Original<br/>100% de datos] --> B[Training Set<br/>60-80%]
    A --> C[Validation Set<br/>10-20%]
    A --> D[Test Set<br/>10-20%]

    B --> E[Modelo aprende<br/>ajusta par√°metros]
    C --> F[Ajustar hiperpar√°metros<br/>comparar modelos]
    D --> G[Evaluaci√≥n final<br/>rendimiento real]

    style A fill:#2E86AB,color:#fff
    style B fill:#06A77D,color:#fff
    style C fill:#F18F01,color:#fff
    style D fill:#C73E1D,color:#fff
            </div>

            <div class="feature-card">
                <h3>Analog√≠a: Preparaci√≥n para un Examen</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Conjunto</th>
                            <th>Analog√≠a Acad√©mica</th>
                            <th>Prop√≥sito en ML</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Training Set</strong></td>
                            <td>Material de estudio y ejercicios resueltos</td>
                            <td>El modelo aprende patrones</td>
                        </tr>
                        <tr>
                            <td><strong>Validation Set</strong></td>
                            <td>Ex√°menes de pr√°ctica para autoevaluaci√≥n</td>
                            <td>Ajustar hiperpar√°metros y detectar overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>Test Set</strong></td>
                            <td>Examen final oficial (solo una vez)</td>
                            <td>Evaluaci√≥n imparcial del rendimiento real</td>
                        </tr>
                    </tbody>
                </table>
            </div>

        </section>

        <!-- Secci√≥n 2: Training Set -->
        <section id="training-set" class="content-section">
            <h2>2. Training Set (Conjunto de Entrenamiento)</h2>

            <div class="highlight-box" style="border-left: 4px solid #06A77D;">
                <h3>üìê Definici√≥n</h3>
                <p>
                    El <strong>Training Set</strong> es el subconjunto de datos que el modelo utiliza para
                    <strong>aprender</strong>. Durante el entrenamiento, el modelo ajusta sus par√°metros
                    internos (pesos en redes neuronales, coeficientes en regresi√≥n lineal, reglas en
                    √°rboles de decisi√≥n) para minimizar el error en estos datos.
                </p>
            </div>

            <h3>Caracter√≠sticas del Training Set</h3>
            <div class="grid-features">
                <div class="feature-card">
                    <h4>Tama√±o T√≠pico</h4>
                    <p><strong>60-80%</strong> del dataset total</p>
                    <p>Es el conjunto m√°s grande porque el modelo necesita suficientes ejemplos para aprender patrones robustos.</p>
                </div>

                <div class="feature-card">
                    <h4>Qu√© Aprende el Modelo</h4>
                    <ul>
                        <li>Relaciones entre caracter√≠sticas (X) y etiquetas (y)</li>
                        <li>Patrones y regularidades en los datos</li>
                        <li>Fronteras de decisi√≥n para clasificaci√≥n</li>
                        <li>Funciones de mapeo para regresi√≥n</li>
                    </ul>
                </div>

                <div class="feature-card">
                    <h4>Riesgo Principal: Overfitting</h4>
                    <p>
                        Si el modelo es muy complejo o entrena demasiado tiempo, puede <strong>memorizar</strong>
                        el training set en lugar de aprender patrones generalizables.
                    </p>
                    <p><strong>Se√±al de alerta:</strong> Accuracy muy alto en training (99%) pero bajo en validation (70%)</p>
                </div>
            </div>

            <h3>Ejemplo: Entrenamiento de un Clasificador</h3>
            <pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

# Supongamos X_train, y_train ya separados (70% del total)
# El modelo ajusta sus par√°metros internos
modelo = RandomForestClassifier(n_estimators=100, random_state=42)
modelo.fit(X_train, y_train)  # ‚Üê El modelo APRENDE de estos datos

# Despu√©s del fit(), el modelo ha:
# - Construido 100 √°rboles de decisi√≥n
# - Aprendido qu√© combinaciones de caracter√≠sticas predicen cada clase
# - Ajustado las reglas de divisi√≥n en cada nodo

print(f"N√∫mero de caracter√≠sticas usadas: {modelo.n_features_in_}")
print(f"Clases aprendidas: {modelo.classes_}")
# N√∫mero de caracter√≠sticas usadas: 4
# Clases aprendidas: [0 1 2]</code></pre>

        </section>

        <!-- Secci√≥n 3: Validation Set -->
        <section id="validation-set" class="content-section">
            <h2>3. Validation Set (Conjunto de Validaci√≥n)</h2>

            <div class="highlight-box" style="border-left: 4px solid #F18F01;">
                <h3>üìê Definici√≥n</h3>
                <p>
                    El <strong>Validation Set</strong> es el subconjunto utilizado para <strong>ajustar
                    hiperpar√°metros</strong> y <strong>comparar diferentes modelos</strong> durante el
                    desarrollo. A diferencia del test set, el validation set <strong>s√≠ influye</strong>
                    en las decisiones del desarrollador.
                </p>
            </div>

            <h3>Usos Principales del Validation Set</h3>
            <div class="grid-features">
                <div class="feature-card">
                    <h4>1. Selecci√≥n de Hiperpar√°metros</h4>
                    <p>Encontrar los mejores valores para configuraciones no aprendidas autom√°ticamente:</p>
                    <ul>
                        <li><strong>Learning rate:</strong> Velocidad de aprendizaje</li>
                        <li><strong>N√∫mero de capas/neuronas:</strong> Arquitectura de red</li>
                        <li><strong>Regularizaci√≥n (L1, L2):</strong> Control de overfitting</li>
                        <li><strong>n_estimators, max_depth:</strong> Par√°metros de Random Forest</li>
                    </ul>
                </div>

                <div class="feature-card">
                    <h4>2. Comparaci√≥n de Modelos</h4>
                    <p>Evaluar qu√© algoritmo funciona mejor para el problema:</p>
                    <ul>
                        <li>Random Forest vs XGBoost vs Red Neuronal</li>
                        <li>Regresi√≥n Lineal vs Regresi√≥n Polin√≥mica</li>
                        <li>SVM con kernel RBF vs kernel lineal</li>
                    </ul>
                </div>

                <div class="feature-card">
                    <h4>3. Detecci√≥n de Overfitting</h4>
                    <p>Monitorizar si el modelo generaliza o solo memoriza:</p>
                    <ul>
                        <li>Si training accuracy sube pero validation baja ‚Üí Overfitting</li>
                        <li><strong>Early Stopping:</strong> Detener entrenamiento cuando validation deja de mejorar</li>
                    </ul>
                </div>
            </div>

            <h3>Ejemplo: B√∫squeda de Hiperpar√°metros con Validation Set</h3>
            <pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Diferentes configuraciones a probar
hiperparametros = [
    {'n_estimators': 50, 'max_depth': 5},
    {'n_estimators': 100, 'max_depth': 10},
    {'n_estimators': 200, 'max_depth': 15},
    {'n_estimators': 100, 'max_depth': None},  # Sin l√≠mite de profundidad
]

mejores_params = None
mejor_accuracy = 0

# Probar cada configuraci√≥n
for params in hiperparametros:
    modelo = RandomForestClassifier(**params, random_state=42)
    modelo.fit(X_train, y_train)

    # Evaluar en VALIDATION set (no en test)
    y_val_pred = modelo.predict(X_val)
    accuracy = accuracy_score(y_val, y_val_pred)

    print(f"Params: {params}")
    print(f"  ‚Üí Accuracy en VALIDATION: {accuracy:.3f}")

    if accuracy > mejor_accuracy:
        mejor_accuracy = accuracy
        mejores_params = params

print(f"\n‚úÖ Mejores hiperpar√°metros: {mejores_params}")
print(f"‚úÖ Mejor accuracy en validation: {mejor_accuracy:.3f}")

# Salida t√≠pica:
# Params: {'n_estimators': 50, 'max_depth': 5}
#   ‚Üí Accuracy en VALIDATION: 0.909
# Params: {'n_estimators': 100, 'max_depth': 10}
#   ‚Üí Accuracy en VALIDATION: 0.955
# Params: {'n_estimators': 200, 'max_depth': 15}
#   ‚Üí Accuracy en VALIDATION: 0.955
# Params: {'n_estimators': 100, 'max_depth': None}
#   ‚Üí Accuracy en VALIDATION: 0.932
#
# ‚úÖ Mejores hiperpar√°metros: {'n_estimators': 100, 'max_depth': 10}
# ‚úÖ Mejor accuracy en validation: 0.955</code></pre>

            <div class="highlight-box info">
                <h4>üí° Diferencia Clave: Validation vs Test</h4>
                <p>
                    El <strong>validation set</strong> se usa m√∫ltiples veces durante el desarrollo para
                    tomar decisiones (qu√© hiperpar√°metros usar, qu√© modelo elegir). Por tanto, existe
                    cierto <strong>sesgo indirecto</strong>: optimizamos para este conjunto.
                </p>
                <p>
                    El <strong>test set</strong> se usa <strong>una √∫nica vez</strong> al final, cuando
                    todas las decisiones ya est√°n tomadas. Proporciona una estimaci√≥n <strong>imparcial</strong>
                    del rendimiento real.
                </p>
            </div>

        </section>

        <!-- Secci√≥n 4: Test Set -->
        <section id="test-set" class="content-section">
            <h2>4. Test Set (Conjunto de Prueba)</h2>

            <div class="highlight-box" style="border-left: 4px solid #C73E1D;">
                <h3>üìê Definici√≥n</h3>
                <p>
                    El <strong>Test Set</strong> es el subconjunto reservado para la <strong>evaluaci√≥n
                    final</strong> del modelo. Se utiliza <strong>una √∫nica vez</strong>, cuando el modelo
                    est√° completamente entrenado y sus hiperpar√°metros ya han sido ajustados.
                </p>
                <p>
                    <strong>Regla de oro:</strong> El test set debe permanecer <strong>intocable</strong>
                    hasta el momento de la evaluaci√≥n final.
                </p>
            </div>

            <h3>Caracter√≠sticas del Test Set</h3>
            <div class="grid-features">
                <div class="feature-card" style="border-left: 3px solid #C73E1D;">
                    <h4>Prop√≥sito √önico</h4>
                    <p>
                        Obtener una <strong>estimaci√≥n imparcial</strong> del rendimiento del modelo
                        en datos completamente nuevos, simulando su comportamiento en producci√≥n.
                    </p>
                </div>

                <div class="feature-card" style="border-left: 3px solid #C73E1D;">
                    <h4>Tama√±o T√≠pico</h4>
                    <p><strong>10-20%</strong> del dataset total</p>
                    <p>Debe ser suficientemente grande para proporcionar m√©tricas estad√≠sticamente significativas.</p>
                </div>

                <div class="feature-card" style="border-left: 3px solid #C73E1D;">
                    <h4>Uso √önico</h4>
                    <p>
                        Se eval√∫a <strong>una sola vez</strong>. Si ajustas el modelo despu√©s de ver
                        resultados del test set, introduces <strong>data leakage</strong> y la evaluaci√≥n
                        deja de ser imparcial.
                    </p>
                </div>
            </div>

            <h3>Ejemplo: Evaluaci√≥n Final en Test Set</h3>
            <pre><code>from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Entrenar modelo final con los mejores hiperpar√°metros encontrados
modelo_final = RandomForestClassifier(**mejores_params, random_state=42)
modelo_final.fit(X_train, y_train)

# ============================================
# EVALUACI√ìN FINAL EN TEST SET (√öNICA VEZ)
# ============================================
y_test_pred = modelo_final.predict(X_test)

# M√©tricas de rendimiento
accuracy_test = accuracy_score(y_test, y_test_pred)
print(f"üéØ ACCURACY EN TEST SET: {accuracy_test:.3f}")

# Reporte detallado por clase
print("\nüìä Classification Report:")
print(classification_report(y_test, y_test_pred, target_names=['setosa', 'versicolor', 'virginica']))

# Matriz de confusi√≥n
print("\nüî¢ Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))

# Salida t√≠pica:
# üéØ ACCURACY EN TEST SET: 0.957
#
# üìä Classification Report:
#               precision    recall  f1-score   support
#       setosa       1.00      1.00      1.00         8
#   versicolor       0.88      1.00      0.93         7
#    virginica       1.00      0.88      0.93         8
#     accuracy                           0.96        23
#    macro avg       0.96      0.96      0.96        23
# weighted avg       0.96      0.96      0.96        23
#
# üî¢ Confusion Matrix:
# [[8 0 0]
#  [0 7 0]
#  [0 1 7]]</code></pre>

            <div class="highlight-box warning">
                <h4>‚ö†Ô∏è Error Grave: Usar el Test Set M√∫ltiples Veces</h4>
                <p>
                    Si eval√∫as en el test set, ajustas el modelo bas√°ndote en esos resultados, y vuelves
                    a evaluar, est√°s cometiendo <strong>data leakage</strong>. El test set ha "contaminado"
                    tus decisiones y la evaluaci√≥n ya no es imparcial.
                </p>
                <p><strong>Consecuencia:</strong> El modelo puede parecer mejor de lo que realmente es en producci√≥n.</p>
            </div>

        </section>

        <!-- Secci√≥n 5: Proporciones -->
        <section id="proporciones" class="content-section">
            <h2>5. Proporciones T√≠picas de Divisi√≥n</h2>

            <div class="feature-card">
                <h3>Proporciones Est√°ndar</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Conjunto</th>
                            <th>Proporci√≥n T√≠pica</th>
                            <th>Ejemplo (10,000 muestras)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Training Set</strong></td>
                            <td>60-80%</td>
                            <td>7,000 muestras</td>
                        </tr>
                        <tr>
                            <td><strong>Validation Set</strong></td>
                            <td>10-20%</td>
                            <td>1,500 muestras</td>
                        </tr>
                        <tr>
                            <td><strong>Test Set</strong></td>
                            <td>10-20%</td>
                            <td>1,500 muestras</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Divisi√≥n M√°s Com√∫n: 70/15/15</h3>
            <div class="mermaid">
pie title Divisi√≥n de Dataset (70/15/15)
    "Training Set (70%)" : 70
    "Validation Set (15%)" : 15
    "Test Set (15%)" : 15
            </div>

            <h3>Factores que Influyen en las Proporciones</h3>
            <div class="grid-features">
                <div class="feature-card">
                    <h4>Dataset Grande (&gt;100,000 muestras)</h4>
                    <p>Se puede usar proporci√≥n m√°s peque√±a para validation/test:</p>
                    <ul>
                        <li><strong>80/10/10</strong> o incluso <strong>90/5/5</strong></li>
                        <li>10,000 muestras de test siguen siendo estad√≠sticamente significativas</li>
                    </ul>
                </div>

                <div class="feature-card">
                    <h4>Dataset Peque√±o (&lt;1,000 muestras)</h4>
                    <p>Necesitas maximizar datos de entrenamiento:</p>
                    <ul>
                        <li><strong>60/20/20</strong> para mantener suficientes datos en cada conjunto</li>
                        <li>Considerar <strong>validaci√≥n cruzada</strong> en lugar de validation set fijo</li>
                    </ul>
                </div>

                <div class="feature-card">
                    <h4>Clases Desbalanceadas</h4>
                    <p>Asegurar que cada clase est√© representada en todos los conjuntos:</p>
                    <ul>
                        <li>Usar <strong>stratified split</strong> (stratify=y)</li>
                        <li>Verificar que clases minoritarias tengan suficientes muestras en test</li>
                    </ul>
                </div>
            </div>

            <h3>Ejemplo: Divisi√≥n con Estratificaci√≥n</h3>
            <pre><code>from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Cargar dataset
iris = load_iris()
X, y = iris.data, iris.target

print(f"Dataset total: {len(y)} muestras")
print(f"Distribuci√≥n de clases: {dict(zip(*np.unique(y, return_counts=True)))}")
# Dataset total: 150 muestras
# Distribuci√≥n de clases: {0: 50, 1: 50, 2: 50}

# PASO 1: Separar 70% training, 30% temporal (para val + test)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.30,      # 30% para validation + test
    random_state=42,     # Reproducibilidad
    stratify=y           # Mantener proporci√≥n de clases
)

# PASO 2: Dividir el 30% temporal en 50/50 (15% val, 15% test del total)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.50,      # 50% del temporal = 15% del total
    random_state=42,
    stratify=y_temp
)

# Verificar tama√±os
print(f"\nüìä Divisi√≥n final:")
print(f"  Training:   {len(X_train)} muestras ({len(X_train)/len(X)*100:.0f}%)")
print(f"  Validation: {len(X_val)} muestras ({len(X_val)/len(X)*100:.0f}%)")
print(f"  Test:       {len(X_test)} muestras ({len(X_test)/len(X)*100:.0f}%)")

# Verificar estratificaci√≥n (proporci√≥n de clases)
print(f"\nüéØ Verificaci√≥n de estratificaci√≥n:")
print(f"  Training - Clase 0: {sum(y_train==0)}, Clase 1: {sum(y_train==1)}, Clase 2: {sum(y_train==2)}")
print(f"  Validation - Clase 0: {sum(y_val==0)}, Clase 1: {sum(y_val==1)}, Clase 2: {sum(y_val==2)}")
print(f"  Test - Clase 0: {sum(y_test==0)}, Clase 1: {sum(y_test==1)}, Clase 2: {sum(y_test==2)}")

# Salida:
# üìä Divisi√≥n final:
#   Training:   105 muestras (70%)
#   Validation: 22 muestras (15%)
#   Test:       23 muestras (15%)
#
# üéØ Verificaci√≥n de estratificaci√≥n:
#   Training - Clase 0: 35, Clase 1: 35, Clase 2: 35
#   Validation - Clase 0: 7, Clase 1: 8, Clase 2: 7
#   Test - Clase 0: 8, Clase 1: 7, Clase 2: 8</code></pre>

        </section>

        <!-- Secci√≥n 6: Cross-Validation -->
        <section id="cross-validation" class="content-section">
            <h2>6. Validaci√≥n Cruzada (K-Fold Cross-Validation)</h2>

            <div class="highlight-box" style="border-left: 4px solid #A23B72;">
                <h3>üìê ¬øCu√°ndo Usar Validaci√≥n Cruzada?</h3>
                <p>
                    Cuando el dataset es <strong>peque√±o</strong> y dividir en train/val/test dejar√≠a
                    muy pocas muestras en cada conjunto, la validaci√≥n cruzada permite <strong>maximizar
                    el uso de los datos disponibles</strong>.
                </p>
            </div>

            <h3>C√≥mo Funciona K-Fold Cross-Validation</h3>
            <div class="mermaid">
graph TB
    subgraph "K-Fold Cross-Validation (K=5)"
        F1[Fold 1: VAL | Train | Train | Train | Train]
        F2[Fold 2: Train | VAL | Train | Train | Train]
        F3[Fold 3: Train | Train | VAL | Train | Train]
        F4[Fold 4: Train | Train | Train | VAL | Train]
        F5[Fold 5: Train | Train | Train | Train | VAL]
    end

    F1 --> R1[Score 1: 0.95]
    F2 --> R2[Score 2: 0.90]
    F3 --> R3[Score 3: 0.95]
    F4 --> R4[Score 4: 0.91]
    F5 --> R5[Score 5: 0.93]

    R1 & R2 & R3 & R4 & R5 --> Final[Promedio: 0.928 ¬± 0.022]

    style Final fill:#06A77D,color:#fff
            </div>

            <h3>Proceso de K-Fold</h3>
            <div class="feature-card">
                <ol>
                    <li><strong>Dividir el dataset en K particiones</strong> (folds), t√≠picamente K=5 o K=10</li>
                    <li><strong>Entrenar el modelo K veces</strong>, usando cada vez una partici√≥n diferente como validaci√≥n y las K-1 restantes como entrenamiento</li>
                    <li><strong>Calcular m√©tricas en cada iteraci√≥n</strong></li>
                    <li><strong>Promediar las K m√©tricas</strong> para obtener una estimaci√≥n robusta del rendimiento</li>
                    <li><strong>El test set se mantiene completamente separado</strong> y solo se usa al final</li>
                </ol>
            </div>

            <h3>Implementaci√≥n con scikit-learn</h3>
            <pre><code>from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Crear modelo
modelo = RandomForestClassifier(n_estimators=100, random_state=42)

# Validaci√≥n cruzada con K=5 folds
# Nota: Usamos X_train, y_train (NO incluye test set)
scores = cross_val_score(
    modelo,
    X_train,
    y_train,
    cv=5,                # N√∫mero de folds
    scoring='accuracy'   # M√©trica a calcular
)

print("üìä Resultados de 5-Fold Cross-Validation:")
print(f"   Accuracy en cada fold: {scores}")
print(f"   Accuracy promedio: {scores.mean():.3f}")
print(f"   Desviaci√≥n est√°ndar: {scores.std():.3f}")
print(f"   Intervalo de confianza: {scores.mean():.3f} ¬± {scores.std()*2:.3f}")

# Salida t√≠pica:
# üìä Resultados de 5-Fold Cross-Validation:
#    Accuracy en cada fold: [0.952 0.905 0.952 0.905 0.905]
#    Accuracy promedio: 0.924
#    Desviaci√≥n est√°ndar: 0.022
#    Intervalo de confianza: 0.924 ¬± 0.044

# Entrenar modelo final con todos los datos de entrenamiento
modelo.fit(X_train, y_train)

# Evaluaci√≥n final en TEST SET (una sola vez)
accuracy_final = modelo.score(X_test, y_test)
print(f"\nüéØ Accuracy final en TEST SET: {accuracy_final:.3f}")</code></pre>

            <div class="highlight-box success">
                <h4>‚úÖ Ventajas de K-Fold Cross-Validation</h4>
                <ul>
                    <li><strong>Maximiza uso de datos:</strong> Cada muestra se usa tanto para entrenar como para validar</li>
                    <li><strong>Estimaci√≥n m√°s robusta:</strong> El promedio de K evaluaciones es m√°s fiable que una sola</li>
                    <li><strong>Detecta varianza:</strong> La desviaci√≥n est√°ndar indica qu√© tan estable es el modelo</li>
                    <li><strong>Ideal para datasets peque√±os:</strong> No "desperdicias" datos en un validation set fijo</li>
                </ul>
            </div>

        </section>

        <!-- Secci√≥n 7: Implementaci√≥n -->
        <section id="implementacion" class="content-section">
            <h2>7. Implementaci√≥n Pr√°ctica Completa en Python</h2>

            <h3>Pipeline Completo: De Divisi√≥n a Evaluaci√≥n Final</h3>
            <pre><code>"""
Pipeline completo de divisi√≥n de datos y evaluaci√≥n en Machine Learning
Dataset: Iris (clasificaci√≥n multiclase)
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ============================================
# 1. CARGAR Y EXPLORAR DATOS
# ============================================
print("=" * 60)
print("1. CARGA DE DATOS")
print("=" * 60)

iris = load_iris()
X, y = iris.data, iris.target

print(f"Total de muestras: {len(y)}")
print(f"Caracter√≠sticas: {iris.feature_names}")
print(f"Clases: {iris.target_names}")
print(f"Distribuci√≥n: {dict(zip(iris.target_names, np.bincount(y)))}")

# ============================================
# 2. DIVISI√ìN EN TRAIN / VALIDATION / TEST
# ============================================
print("\n" + "=" * 60)
print("2. DIVISI√ìN DE DATOS (70% / 15% / 15%)")
print("=" * 60)

# Primera divisi√≥n: 70% train, 30% temporal
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)

# Segunda divisi√≥n: 50% validation, 50% test (del temporal)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
)

print(f"Training set:   {len(X_train)} muestras ({len(X_train)/len(X)*100:.0f}%)")
print(f"Validation set: {len(X_val)} muestras ({len(X_val)/len(X)*100:.0f}%)")
print(f"Test set:       {len(X_test)} muestras ({len(X_test)/len(X)*100:.0f}%)")

# ============================================
# 3. B√öSQUEDA DE HIPERPAR√ÅMETROS (usando Validation Set)
# ============================================
print("\n" + "=" * 60)
print("3. B√öSQUEDA DE HIPERPAR√ÅMETROS")
print("=" * 60)

hiperparametros = [
    {'n_estimators': 50, 'max_depth': 5},
    {'n_estimators': 100, 'max_depth': 10},
    {'n_estimators': 200, 'max_depth': 15},
    {'n_estimators': 100, 'max_depth': None},
]

mejores_params = None
mejor_accuracy_val = 0

for params in hiperparametros:
    modelo = RandomForestClassifier(**params, random_state=42)
    modelo.fit(X_train, y_train)

    # Evaluar en VALIDATION (no en test)
    accuracy_val = accuracy_score(y_val, modelo.predict(X_val))
    accuracy_train = accuracy_score(y_train, modelo.predict(X_train))

    # Detectar overfitting: gran diferencia entre train y validation
    gap = accuracy_train - accuracy_val
    status = "‚ö†Ô∏è Posible overfitting" if gap > 0.1 else "‚úÖ OK"

    print(f"Params: {params}")
    print(f"  Train: {accuracy_train:.3f} | Val: {accuracy_val:.3f} | Gap: {gap:.3f} {status}")

    if accuracy_val > mejor_accuracy_val:
        mejor_accuracy_val = accuracy_val
        mejores_params = params

print(f"\nüèÜ Mejores hiperpar√°metros: {mejores_params}")
print(f"üèÜ Mejor accuracy en validation: {mejor_accuracy_val:.3f}")

# ============================================
# 4. VALIDACI√ìN CRUZADA (opcional, para estimaci√≥n m√°s robusta)
# ============================================
print("\n" + "=" * 60)
print("4. VALIDACI√ìN CRUZADA (5-Fold)")
print("=" * 60)

modelo_cv = RandomForestClassifier(**mejores_params, random_state=42)
cv_scores = cross_val_score(modelo_cv, X_train, y_train, cv=5, scoring='accuracy')

print(f"Scores por fold: {cv_scores}")
print(f"Promedio: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")

# ============================================
# 5. ENTRENAMIENTO DEL MODELO FINAL
# ============================================
print("\n" + "=" * 60)
print("5. ENTRENAMIENTO DEL MODELO FINAL")
print("=" * 60)

modelo_final = RandomForestClassifier(**mejores_params, random_state=42)
modelo_final.fit(X_train, y_train)

print(f"Modelo entrenado con {len(X_train)} muestras")
print(f"Hiperpar√°metros: {mejores_params}")

# ============================================
# 6. EVALUACI√ìN FINAL EN TEST SET (UNA SOLA VEZ)
# ============================================
print("\n" + "=" * 60)
print("6. EVALUACI√ìN FINAL EN TEST SET")
print("=" * 60)

y_test_pred = modelo_final.predict(X_test)
accuracy_test = accuracy_score(y_test, y_test_pred)

print(f"\nüéØ ACCURACY FINAL EN TEST SET: {accuracy_test:.3f}")

print("\nüìä Classification Report:")
print(classification_report(y_test, y_test_pred, target_names=iris.target_names))

print("üî¢ Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))

# ============================================
# 7. RESUMEN FINAL
# ============================================
print("\n" + "=" * 60)
print("7. RESUMEN")
print("=" * 60)
print(f"Dataset total: {len(X)} muestras")
print(f"Divisi√≥n: 70% train / 15% val / 15% test")
print(f"Modelo seleccionado: RandomForestClassifier")
print(f"Mejores hiperpar√°metros: {mejores_params}")
print(f"Cross-validation (5-fold): {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")
print(f"Accuracy final en TEST: {accuracy_test:.3f}")</code></pre>

        </section>

        <!-- Secci√≥n 8: Mejores Pr√°cticas -->
        <section id="mejores-practicas" class="content-section">
            <h2>8. Mejores Pr√°cticas y Errores Comunes</h2>

            <h3>Mejores Pr√°cticas</h3>
            <div class="grid-features">
                <div class="feature-card" style="border-left: 3px solid #06A77D;">
                    <h4>1. Usar Estratificaci√≥n</h4>
                    <p>
                        Siempre usar <code>stratify=y</code> en clasificaci√≥n para mantener la proporci√≥n
                        de clases en todos los conjuntos.
                    </p>
                    <pre><code>train_test_split(X, y, stratify=y)</code></pre>
                </div>

                <div class="feature-card" style="border-left: 3px solid #06A77D;">
                    <h4>2. Fijar Semilla Aleatoria</h4>
                    <p>
                        Usar <code>random_state</code> para reproducibilidad. Documenta qu√© semilla usaste.
                    </p>
                    <pre><code>random_state=42  # Reproducible</code></pre>
                </div>

                <div class="feature-card" style="border-left: 3px solid #06A77D;">
                    <h4>3. No Contaminar el Test Set</h4>
                    <p>
                        Nunca aplicar transformaciones (normalizaci√≥n, encoding) usando informaci√≥n del
                        test set. Ajustar transformadores solo con training data.
                    </p>
                    <pre><code># ‚úÖ Correcto
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# ‚ùå Incorrecto
scaler.fit(X)  # Incluye test!</code></pre>
                </div>

                <div class="feature-card" style="border-left: 3px solid #06A77D;">
                    <h4>4. Documentar Divisi√≥n y Resultados</h4>
                    <p>
                        Registrar proporciones, semilla aleatoria, y m√©tricas obtenidas para
                        reproducibilidad y trazabilidad.
                    </p>
                </div>
            </div>

            <h3>Errores Comunes a Evitar</h3>
            <div class="grid-features">
                <div class="highlight-box warning">
                    <h4>‚ùå Error 1: Evaluar con los Mismos Datos de Entrenamiento</h4>
                    <p>
                        <strong>Problema:</strong> Accuracy inflado artificialmente, no refleja rendimiento real.
                    </p>
                    <pre><code># ‚ùå INCORRECTO
modelo.fit(X, y)
accuracy = modelo.score(X, y)  # ¬°Eval√∫a con datos de entrenamiento!</code></pre>
                </div>

                <div class="highlight-box warning">
                    <h4>‚ùå Error 2: Usar Test Set M√∫ltiples Veces</h4>
                    <p>
                        <strong>Problema:</strong> Data leakage - optimizas indirectamente para el test set.
                    </p>
                    <pre><code># ‚ùå INCORRECTO: Ciclo de ajuste basado en test
for params in grid:
    modelo.fit(X_train)
    score = modelo.score(X_test)  # ¬°Ajustas seg√∫n test!
    if score > best:
        best_params = params</code></pre>
                </div>

                <div class="highlight-box warning">
                    <h4>‚ùå Error 3: Olvidar Estratificaci√≥n</h4>
                    <p>
                        <strong>Problema:</strong> Conjuntos desbalanceados, especialmente con clases minoritarias.
                    </p>
                    <pre><code># ‚ùå Sin estratificaci√≥n: puede dejar una clase sin muestras en test
train_test_split(X, y, test_size=0.2)

# ‚úÖ Con estratificaci√≥n: mantiene proporci√≥n de clases
train_test_split(X, y, test_size=0.2, stratify=y)</code></pre>
                </div>

                <div class="highlight-box warning">
                    <h4>‚ùå Error 4: Data Leakage en Preprocesamiento</h4>
                    <p>
                        <strong>Problema:</strong> Normalizar usando todo el dataset "filtra" informaci√≥n del test al modelo.
                    </p>
                    <pre><code># ‚ùå INCORRECTO: fit con todo el dataset
scaler.fit(X)  # Incluye informaci√≥n del test
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# ‚úÖ CORRECTO: fit solo con training
scaler.fit(X_train)  # Solo training
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)</code></pre>
                </div>
            </div>

            <div class="feature-card">
                <h3>Caso de Estudio: Spotify - Sistema de Recomendaci√≥n</h3>
                <p>
                    El sistema de recomendaci√≥n de Spotify utiliza una divisi√≥n rigurosa de datos:
                </p>
                <ul>
                    <li><strong>Training set:</strong> Millones de interacciones hist√≥ricas de usuarios (canciones reproducidas, skips, likes)</li>
                    <li><strong>Validation set:</strong> Usuarios recientes para probar diferentes configuraciones de hiperpar√°metros del modelo de recomendaci√≥n</li>
                    <li><strong>Test set:</strong> Grupo de usuarios completamente separados que nunca participaron en el entrenamiento, para medir el rendimiento real del sistema</li>
                </ul>
                <p>
                    <strong>Resultado:</strong> Esta metodolog√≠a permite a Spotify medir con precisi√≥n si sus recomendaciones realmente mejoran la experiencia del usuario antes de desplegarlas a millones de personas.
                </p>
            </div>

        </section>

    </main>

    <!-- Footer -->
    <footer class="footer-ilerna">
        <div class="footer-content">
            <p>&copy; 2024 iLERNA - Sistemas de Aprendizaje Autom√°tico</p>
            <p class="penguin-signature">üêß Desarrollado con dedicaci√≥n por el Ping√ºino</p>
        </div>
    </footer>

    <script>
        // Smooth scroll para TOC
        document.querySelectorAll('.toc-list a').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                target.scrollIntoView({ behavior: 'smooth', block: 'start' });
            });
        });

        // Inicializar Mermaid
        mermaid.initialize({
            startOnLoad: true,
            theme: 'base',
            themeVariables: {
                fontSize: '16px',
                fontFamily: 'Arial, sans-serif'
            }
        });
    </script>

</body>
</html>