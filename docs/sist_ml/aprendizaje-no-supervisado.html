<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Bjlanza">
    <meta name="organization" content="ILERNA">
    <meta name="description"
        content="Aprendizaje No Supervisado: Clustering, Reducci√≥n de Dimensionalidad y Detecci√≥n de Anomal√≠as. Descubre patrones ocultos en datos sin etiquetas.">
    <meta name="keywords"
        content="aprendizaje no supervisado, clustering, K-Means, DBSCAN, PCA, t-SNE, detecci√≥n de anomal√≠as, machine learning, IA">
    <title>Aprendizaje No Supervisado | iLERNA</title>

    <!-- CSS Com√∫n de Lecciones -->
    <link rel="stylesheet" href="../css/lecciones.css">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,600,700,800&display=swap" rel="stylesheet">
</head>

<body>
    <div class="container">
        <!-- Header con logo -->
        <header>
            <div class="header-container">
                <div class="logo-container">
                    <a href="../index.html">
                        <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                    </a>
                    <div class="logo-text">
                        iLERNA
                        <span>Curso de Especializaci√≥n en IA y Big Data</span>
                    </div>
                </div>
                <div class="breadcrumb">
                    <a href="../index.html">Inicio</a> ‚Ä∫
                    <a href="index.html">Sistemas de Aprendizaje Autom√°tico</a> ‚Ä∫
                    <span>Aprendizaje No Supervisado</span>
                </div>
            </div>
            <h1 class="text-center">Aprendizaje No Supervisado</h1>
            <p class="subtitle text-center">Descubriendo patrones ocultos en datos sin etiquetas</p>
        </header>

        <main>
            <!-- Introducci√≥n -->
            <section class="section">
                <h2 class="section-title">¬øQu√© es el Aprendizaje No Supervisado?</h2>
                <p>El <strong>aprendizaje no supervisado</strong> es un paradigma del Machine Learning donde el
                    algoritmo
                    trabaja con datos <strong>sin etiquetas</strong>. A diferencia del aprendizaje supervisado, aqu√≠ no
                    hay "respuestas correctas" previas. El algoritmo debe descubrir por s√≠ mismo la estructura, patrones
                    y relaciones ocultas en los datos.</p>

                <div class="highlight-box primary">
                    <h3 class="title">üéØ Objetivo Principal</h3>
                    <p class="content">Descubrir la <strong>estructura inherente</strong> de los datos, identificar
                        patrones naturales, agrupar elementos similares o reducir la complejidad mientras se preserva la
                        informaci√≥n relevante.</p>
                </div>

                <h3>Caracter√≠sticas Clave</h3>
                <div class="grid-features">
                    <div class="feature-card primary">
                        <h4 class="color-primary">üîç Sin Etiquetas</h4>
                        <p>Los datos no tienen respuestas predefinidas ni categor√≠as conocidas de antemano</p>
                    </div>
                    <div class="feature-card secondary">
                        <h4 class="color-secondary">üéØ Descubrimiento Aut√≥nomo</h4>
                        <p>El algoritmo encuentra estructura y patrones por s√≠ mismo sin supervisi√≥n humana</p>
                    </div>
                    <div class="feature-card primary">
                        <h4 class="color-primary">üìä Exploraci√≥n de Datos</h4>
                        <p>Ideal para an√°lisis exploratorio cuando no conocemos la estructura subyacente</p>
                    </div>
                    <div class="feature-card secondary">
                        <h4 class="color-secondary">üí° Insights Inesperados</h4>
                        <p>Puede revelar relaciones y patrones que no eran obvios inicialmente</p>
                    </div>
                </div>
            </section>

            <!-- Comparaci√≥n -->
            <section class="section">
                <h2 class="section-title">Supervisado vs No Supervisado</h2>
                <p>Comprender las diferencias fundamentales entre estos dos paradigmas es esencial para elegir el
                    enfoque
                    correcto seg√∫n el problema y los datos disponibles.</p>

                <div class="styled-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Aspecto</th>
                                <th>Aprendizaje Supervisado</th>
                                <th>Aprendizaje No Supervisado</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Datos de entrada</strong></td>
                                <td>Con etiquetas (X, y)</td>
                                <td>Sin etiquetas (solo X)</td>
                            </tr>
                            <tr>
                                <td><strong>Objetivo</strong></td>
                                <td>Predecir salida conocida</td>
                                <td>Descubrir estructura oculta</td>
                            </tr>
                            <tr>
                                <td><strong>Tipo de problema</strong></td>
                                <td>Clasificaci√≥n, Regresi√≥n</td>
                                <td>Clustering, Reducci√≥n dimensionalidad, Detecci√≥n anomal√≠as</td>
                            </tr>
                            <tr>
                                <td><strong>Evaluaci√≥n</strong></td>
                                <td>Comparar con etiquetas reales (accuracy, F1, RMSE)</td>
                                <td>M√©tricas internas (silhouette, inercia, varianza explicada)</td>
                            </tr>
                            <tr>
                                <td><strong>Ejemplo</strong></td>
                                <td>Clasificar emails como spam/no spam con ejemplos previos</td>
                                <td>Agrupar clientes sin saber categor√≠as de antemano</td>
                            </tr>
                            <tr>
                                <td><strong>Coste de datos</strong></td>
                                <td>Alto (requiere etiquetar manualmente)</td>
                                <td>Bajo (datos crudos sin procesar)</td>
                            </tr>
                            <tr>
                                <td><strong>Interpretabilidad</strong></td>
                                <td>Alta (se compara con verdad conocida)</td>
                                <td>Moderada (requiere validaci√≥n del dominio)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <!-- Tipos Principales -->
            <section class="section">
                <h2 class="section-title">Tres Familias del Aprendizaje No Supervisado</h2>
                <p>El aprendizaje no supervisado se divide en tres categor√≠as principales, cada una con objetivos y
                    aplicaciones espec√≠ficas:</p>

                <div class="grid-features">
                    <!-- Clustering -->
                    <div class="feature-card primary">
                        <h4 class="color-primary">1. Clustering (Agrupamiento)</h4>
                        <p><strong>Objetivo:</strong> Dividir datos en grupos (clusters) donde elementos dentro del
                            mismo grupo son m√°s similares entre s√≠ que con elementos de otros grupos.</p>

                        <h5 style="margin-top: 1rem;">Algoritmos Principales</h5>
                        <ul style="line-height: 1.7;">
                            <li><strong>K-Means:</strong> Particiona datos en K clusters bas√°ndose en centroides</li>
                            <li><strong>DBSCAN:</strong> Clustering basado en densidad, detecta formas arbitrarias y
                                outliers</li>
                            <li><strong>Hierarchical Clustering:</strong> Crea una jerarqu√≠a de clusters (dendrograma)
                            </li>
                            <li><strong>Gaussian Mixture Models (GMM):</strong> Modelado probabil√≠stico con
                                distribuciones gaussianas</li>
                        </ul>

                        <h5 style="margin-top: 1rem;">Aplicaciones</h5>
                        <p style="font-size: 0.95rem;">Segmentaci√≥n de clientes, agrupaci√≥n de documentos, compresi√≥n de
                            im√°genes, detecci√≥n de comunidades en redes sociales</p>
                    </div>

                    <!-- Reducci√≥n de Dimensionalidad -->
                    <div class="feature-card secondary">
                        <h4 class="color-secondary">2. Reducci√≥n de Dimensionalidad</h4>
                        <p><strong>Objetivo:</strong> Reducir el n√∫mero de variables/caracter√≠sticas preservando la
                            mayor cantidad de informaci√≥n relevante posible.</p>

                        <h5 style="margin-top: 1rem;">Algoritmos Principales</h5>
                        <ul style="line-height: 1.7;">
                            <li><strong>PCA (Principal Component Analysis):</strong> Proyecci√≥n lineal a componentes
                                principales ortogonales</li>
                            <li><strong>t-SNE:</strong> Visualizaci√≥n de datos de alta dimensi√≥n en 2D/3D</li>
                            <li><strong>UMAP:</strong> Alternativa moderna a t-SNE, m√°s r√°pida y escalable</li>
                            <li><strong>Autoencoders:</strong> Redes neuronales que aprenden representaciones
                                comprimidas</li>
                        </ul>

                        <h5 style="margin-top: 1rem;">Aplicaciones</h5>
                        <p style="font-size: 0.95rem;">Visualizaci√≥n de datos complejos, preprocesamiento para ML,
                            compresi√≥n de datos, eliminaci√≥n de ruido</p>
                    </div>

                    <!-- Detecci√≥n de Anomal√≠as -->
                    <div class="feature-card" style="border-top: 4px solid #DD6B20;">
                        <h4 style="color: #DD6B20;">3. Detecci√≥n de Anomal√≠as</h4>
                        <p><strong>Objetivo:</strong> Identificar puntos de datos que se desv√≠an significativamente del
                            patr√≥n normal o esperado.</p>

                        <h5 style="margin-top: 1rem;">Algoritmos Principales</h5>
                        <ul style="line-height: 1.7;">
                            <li><strong>Isolation Forest:</strong> A√≠sla anomal√≠as mediante √°rboles aleatorios</li>
                            <li><strong>One-Class SVM:</strong> Aprende la frontera de datos "normales"</li>
                            <li><strong>LOF (Local Outlier Factor):</strong> Basado en densidad local</li>
                            <li><strong>Autoencoders:</strong> Detectan anomal√≠as por alto error de reconstrucci√≥n</li>
                        </ul>

                        <h5 style="margin-top: 1rem;">Aplicaciones</h5>
                        <p style="font-size: 0.95rem;">Detecci√≥n de fraude financiero, identificaci√≥n de fallos en
                            sistemas, detecci√≥n de intrusiones en ciberseguridad</p>
                    </div>
                </div>
            </section>

            <!-- K-Means en Profundidad -->
            <section class="section">
                <h2 class="section-title">K-Means: El Algoritmo de Clustering M√°s Popular</h2>
                <p>K-Means es un algoritmo iterativo que divide N observaciones en K clusters, donde cada observaci√≥n
                    pertenece al cluster con el centroide m√°s cercano.</p>

                <div class="highlight-box secondary">
                    <h3 class="title">Algoritmo K-Means (K Clusters)</h3>
                    <ol style="line-height: 2;">
                        <li><strong>Inicializaci√≥n:</strong> Seleccionar K centroides iniciales aleatoriamente</li>
                        <li><strong>Asignaci√≥n:</strong> Asignar cada punto al centroide m√°s cercano (formando K
                            clusters)</li>
                        <li><strong>Actualizaci√≥n:</strong> Recalcular centroides como la media de puntos en cada
                            cluster</li>
                        <li><strong>Repetir</strong> pasos 2-3 hasta convergencia (centroides dejan de moverse)</li>
                    </ol>
                </div>

                <h3 style="margin-top: 2rem;">Ventajas y Limitaciones</h3>
                <div class="grid-features">
                    <div class="feature-card" style="border-left: 4px solid #4CAF50;">
                        <h4 style="color: #4CAF50;">‚úÖ Ventajas</h4>
                        <ul style="line-height: 1.7;">
                            <li>Simple e intuitivo de entender</li>
                            <li>R√°pido y escalable para grandes datasets</li>
                            <li>Funciona bien con clusters esf√©ricos y del mismo tama√±o</li>
                            <li>F√°cil de implementar</li>
                        </ul>
                    </div>

                    <div class="feature-card" style="border-left: 4px solid #E65100;">
                        <h4 style="color: #E65100;">‚ö†Ô∏è Limitaciones</h4>
                        <ul style="line-height: 1.7;">
                            <li>Hay que especificar K de antemano (m√©todo del codo ayuda)</li>
                            <li>Sensible a inicializaci√≥n (usar K-Means++)</li>
                            <li>Solo detecta clusters convexos/esf√©ricos</li>
                            <li>Sensible a outliers y escalas diferentes</li>
                        </ul>
                    </div>
                </div>

                <div class="curiosity-box" style="margin-top: 2rem;">
                    <h4>üí° Determinando el N√∫mero √ìptimo de Clusters (K)</h4>
                    <p><strong>M√©todo del Codo (Elbow Method):</strong></p>
                    <ol style="line-height: 1.8;">
                        <li>Ejecutar K-Means con diferentes valores de K (por ejemplo, K=1 a K=10)</li>
                        <li>Calcular la inercia (suma de distancias cuadradas al centroide m√°s cercano) para cada K</li>
                        <li>Graficar K vs Inercia</li>
                        <li>Buscar el "codo" donde la reducci√≥n de inercia empieza a disminuir significativamente</li>
                    </ol>
                    <p style="margin-top: 1rem;"><strong>Otras m√©tricas:</strong> Silhouette Score (mide cohesi√≥n
                        intra-cluster y separaci√≥n inter-cluster), Davies-Bouldin Index, Calinski-Harabasz Index</p>
                </div>
            </section>

            <!-- DBSCAN -->
            <section class="section">
                <h2 class="section-title">DBSCAN: Clustering Basado en Densidad</h2>
                <p><strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong> es un algoritmo
                    que agrupa puntos que est√°n estrechamente agrupados, marcando como outliers puntos en regiones de
                    baja densidad.</p>

                <div class="highlight-box primary">
                    <h3 class="title">Conceptos Clave de DBSCAN</h3>
                    <ul style="line-height: 1.8;">
                        <li><strong>Œµ (epsilon):</strong> Radio de vecindad alrededor de un punto</li>
                        <li><strong>MinPts:</strong> N√∫mero m√≠nimo de puntos dentro de Œµ para considerar un punto como
                            "core point"</li>
                        <li><strong>Core Point:</strong> Punto con al menos MinPts vecinos dentro de Œµ</li>
                        <li><strong>Border Point:</strong> No es core point, pero est√° en la vecindad de un core point
                        </li>
                        <li><strong>Noise Point (Outlier):</strong> Ni core ni border point</li>
                    </ul>
                </div>

                <h3 style="margin-top: 2rem;">Ventajas de DBSCAN sobre K-Means</h3>
                <div class="grid-features">
                    <div class="feature-card primary">
                        <h4 class="color-primary">üî¢ No Requiere K</h4>
                        <p>Descubre autom√°ticamente el n√∫mero de clusters sin especificarlo de antemano</p>
                    </div>
                    <div class="feature-card secondary">
                        <h4 class="color-secondary">üåÄ Formas Arbitrarias</h4>
                        <p>Detecta clusters de formas no convexas (c√≠rculos, lunas, espirales)</p>
                    </div>
                    <div class="feature-card primary">
                        <h4 class="color-primary">üéØ Detecta Outliers</h4>
                        <p>Identifica y etiqueta puntos de ruido que no pertenecen a ning√∫n cluster</p>
                    </div>
                    <div class="feature-card secondary">
                        <h4 class="color-secondary">üí™ Robusto a Ruido</h4>
                        <p>No fuerza todos los puntos a pertenecer a un cluster</p>
                    </div>
                </div>

                <div class="curiosity-box" style="margin-top: 2rem;">
                    <h4>üí° Cu√°ndo Usar DBSCAN vs K-Means</h4>
                    <p><strong>Usa K-Means cuando:</strong></p>
                    <ul style="line-height: 1.7;">
                        <li>Los clusters son aproximadamente esf√©ricos y de tama√±o similar</li>
                        <li>Conoces el n√∫mero de clusters de antemano</li>
                        <li>Necesitas velocidad en datasets muy grandes</li>
                    </ul>
                    <p style="margin-top: 1rem;"><strong>Usa DBSCAN cuando:</strong></p>
                    <ul style="line-height: 1.7;">
                        <li>No conoces el n√∫mero de clusters</li>
                        <li>Esperas clusters de formas irregulares</li>
                        <li>Necesitas identificar outliers expl√≠citamente</li>
                        <li>Los datos tienen variaciones de densidad</li>
                    </ul>
                </div>
            </section>

            <!-- PCA -->
            <section class="section">
                <h2 class="section-title">PCA: Reducci√≥n de Dimensionalidad</h2>
                <p><strong>PCA (Principal Component Analysis)</strong> es una t√©cnica de reducci√≥n de dimensionalidad
                    que transforma datos a un nuevo sistema de coordenadas donde las nuevas variables (componentes
                    principales) capturan la m√°xima varianza de los datos.</p>

                <div class="highlight-box secondary">
                    <h3 class="title">¬øC√≥mo Funciona PCA?</h3>
                    <ol style="line-height: 2;">
                        <li><strong>Estandarizaci√≥n:</strong> Normalizar datos (media 0, desviaci√≥n est√°ndar 1)</li>
                        <li><strong>Matriz de covarianza:</strong> Calcular covarianzas entre todas las variables</li>
                        <li><strong>Autovectores y autovalores:</strong> Encontrar direcciones de m√°xima varianza</li>
                        <li><strong>Ordenar componentes:</strong> Por autovalor descendente (m√°s varianza primero)</li>
                        <li><strong>Proyecci√≥n:</strong> Transformar datos a los primeros k componentes principales</li>
                    </ol>
                </div>

                <h3 style="margin-top: 2rem;">Aplicaciones de PCA</h3>
                <div class="grid-features">
                    <div class="feature-card primary">
                        <h4 class="color-primary">üìâ Visualizaci√≥n</h4>
                        <p>Reducir datos de 100+ dimensiones a 2D/3D para visualizar</p>
                    </div>
                    <div class="feature-card secondary">
                        <h4 class="color-secondary">‚ö° Aceleraci√≥n</h4>
                        <p>Reducir dimensiones antes de entrenar modelos (menos features = m√°s r√°pido)</p>
                    </div>
                    <div class="feature-card primary">
                        <h4 class="color-primary">üßπ Eliminaci√≥n de Ruido</h4>
                        <p>Remover componentes de baja varianza que suelen ser ruido</p>
                    </div>
                    <div class="feature-card secondary">
                        <h4 class="color-secondary">üîó Multicolinealidad</h4>
                        <p>Eliminar correlaci√≥n entre variables transform√°ndolas a componentes ortogonales</p>
                    </div>
                </div>

                <div class="highlight-box primary" style="margin-top: 2rem;">
                    <h4 class="title">üìä Interpretando Componentes Principales</h4>
                    <p style="line-height: 1.7;" class="content">Cada componente principal es una <strong>combinaci√≥n
                            lineal</strong> de las variables originales. Los autovalores indican cu√°nta varianza captura
                        cada componente:</p>
                    <ul style="margin-top: 0.5rem; line-height: 1.8;">
                        <li><strong>PC1:</strong> Direcci√≥n de m√°xima varianza (normalmente explica 30-70% de varianza
                            total)</li>
                        <li><strong>PC2:</strong> Segunda direcci√≥n de m√°xima varianza, ortogonal a PC1</li>
                        <li><strong>PC3, PC4...:</strong> Componentes subsiguientes en orden descendente de varianza
                        </li>
                    </ul>
                    <p style="margin-top: 1rem;" class="content">Se suele seleccionar el n√∫mero de componentes que
                        expliquen al
                        menos el <strong>80-95% de la varianza acumulada</strong>.</p>
                </div>
            </section>

            <!-- Detecci√≥n de Anomal√≠as -->
            <section class="section">
                <h2 class="section-title">Detecci√≥n de Anomal√≠as</h2>
                <p>La detecci√≥n de anomal√≠as identifica observaciones que se desv√≠an significativamente del
                    comportamiento normal. Es crucial en aplicaciones donde identificar "lo raro" es m√°s importante que
                    clasificar "lo com√∫n".</p>

                <h3>M√©todos Principales</h3>
                <div class="grid-features">
                    <div class="feature-card primary">
                        <h4 class="color-primary">Isolation Forest</h4>
                        <p><strong>Idea:</strong> Anomal√≠as son m√°s f√°ciles de aislar que puntos normales</p>
                        <ul style="font-size: 0.95rem; line-height: 1.7; margin-top: 0.5rem;">
                            <li>Construye √°rboles de decisi√≥n aleatorios</li>
                            <li>Anomal√≠as requieren menos divisiones para aislarse</li>
                            <li>R√°pido y escalable</li>
                            <li>Funciona bien en alta dimensionalidad</li>
                        </ul>
                    </div>

                    <div class="feature-card secondary">
                        <h4 class="color-secondary">One-Class SVM</h4>
                        <p><strong>Idea:</strong> Aprender la frontera que rodea datos normales</p>
                        <ul style="font-size: 0.95rem; line-height: 1.7; margin-top: 0.5rem;">
                            <li>Entrena solo con datos "normales"</li>
                            <li>Encuentra hipersuperficie que separa normal de an√≥malo</li>
                            <li>Efectivo en espacios de alta dimensi√≥n</li>
                            <li>Puede usar kernels para fronteras no lineales</li>
                        </ul>
                    </div>

                    <div class="feature-card primary">
                        <h4 class="color-primary">LOF (Local Outlier Factor)</h4>
                        <p><strong>Idea:</strong> Comparar densidad local con vecinos</p>
                        <ul style="font-size: 0.95rem; line-height: 1.7; margin-top: 0.5rem;">
                            <li>Calcula densidad de puntos en vecindad</li>
                            <li>Detecta anomal√≠as contextuales (baja densidad relativa)</li>
                            <li>Identifica outliers locales, no solo globales</li>
                            <li>Sensible a elecci√≥n de k-vecinos</li>
                        </ul>
                    </div>
                </div>

                <div class="curiosity-box" style="margin-top: 2rem;">
                    <h4>üí° Aplicaciones Cr√≠ticas de Detecci√≥n de Anomal√≠as</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>Fraude Financiero:</strong> Transacciones con tarjeta sospechosas (monto inusual,
                            ubicaci√≥n at√≠pica, horario extra√±o)</li>
                        <li><strong>Ciberseguridad:</strong> Detecci√≥n de intrusiones en redes (tr√°fico an√≥malo, accesos
                            no autorizados)</li>
                        <li><strong>Salud:</strong> Identificar resultados m√©dicos anormales que requieren atenci√≥n
                            inmediata</li>
                        <li><strong>Manufactura:</strong> Detectar productos defectuosos en l√≠neas de producci√≥n</li>
                        <li><strong>IoT:</strong> Fallos en sensores o comportamiento an√≥malo de dispositivos</li>
                    </ul>
                </div>
            </section>

            <!-- Conclusi√≥n -->
            <section class="section">
                <h2 class="section-title">Conclusi√≥n: El Poder del Aprendizaje No Supervisado</h2>
                <p>El aprendizaje no supervisado es fundamental en el ecosistema del Machine Learning moderno,
                    especialmente en la era del Big Data donde etiquetar millones de observaciones es inviable.</p>

                <div class="highlight-box primary">
                    <h4 class="title">üîë Puntos Clave para Recordar</h4>
                    <ul style="line-height: 2;">
                        <li><strong>No requiere etiquetas:</strong> Ideal cuando etiquetar datos es costoso, imposible o
                            desconocemos las categor√≠as</li>
                        <li><strong>Descubrimiento exploratorio:</strong> Revela estructura oculta y patrones no obvios
                            en los datos</li>
                        <li><strong>Tres pilares:</strong> Clustering (agrupar), Reducci√≥n de Dimensionalidad
                            (simplificar), Detecci√≥n de Anomal√≠as (encontrar rarezas)</li>
                        <li><strong>Complementa el supervisado:</strong> Se usa frecuentemente como preprocesamiento
                            antes de aprendizaje supervisado</li>
                    </ul>
                </div>

                <h3 style="margin-top: 2rem;">Flujo de Trabajo T√≠pico</h3>
                <ol style="font-size: 1.1rem; line-height: 2;">
                    <li><strong>Exploraci√≥n:</strong> Usar PCA o t-SNE para visualizar y entender datos</li>
                    <li><strong>Clustering:</strong> Agrupar datos con K-Means o DBSCAN seg√∫n caracter√≠sticas del
                        problema</li>
                    <li><strong>Validaci√≥n:</strong> Evaluar clusters con m√©tricas (silhouette, inercia) y conocimiento
                        del dominio</li>
                    <li><strong>Detecci√≥n de anomal√≠as:</strong> Identificar outliers con Isolation Forest o LOF</li>
                    <li><strong>Iteraci√≥n:</strong> Refinar hiperpar√°metros bas√°ndose en resultados</li>
                </ol>

                <div class="highlight-box secondary" style="margin-top: 2rem;">
                    <p style="font-size: 1.1rem; line-height: 1.8;" class="content"><strong>Mensaje final:</strong> El
                        aprendizaje no supervisado no reemplaza al supervisado, sino que lo complementa. Mientras el
                        supervisado es una "linterna" que ilumina donde apuntas (categor√≠as conocidas), el no
                        supervisado es un "radar" que descubre lo que no sab√≠as que exist√≠a. Ambos son herramientas
                        esenciales en el arsenal del cient√≠fico de datos.</p>
                </div>
            </section>
        </main>

        <!-- FOOTER -->
        <footer>
            <div class="footer-content">
                <h3>iLERNA</h3>
                <p class="subtitle">Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</p>
                <a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a>
            </div>
            <p class="description">Centro oficial de FP online y presencial. Ciclos formativos de Grado Medio y Grado
                Superior.</p>
            <p class="description">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>

            <div class="penguin">
                <span>üêß</span>
            </div>
        </footer>
    </div>

    <!-- Scripts -->
    <script src="../js/lecciones.js"></script>
</body>

</html>