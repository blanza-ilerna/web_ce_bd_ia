<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Algoritmos de Bagging (Bootstrap Aggregating): t√©cnica de Ensemble Learning para reducir varianza y overfitting. Random Forest y ejemplos pr√°cticos.">
    <meta name="keywords"
        content="Bagging, Bootstrap Aggregating, Random Forest, Ensemble Learning, Machine Learning, Overfitting, Reducci√≥n de Varianza">
    <meta name="author" content="Bjlanza">
    <meta name="organization" content="ILERNA">
    <meta property="og:title" content="Algoritmos de Bagging | iLERNA">
    <meta property="og:description"
        content="Aprende c√≥mo funcionan los algoritmos de Bagging, Random Forest y c√≥mo reducir overfitting mediante Bootstrap Aggregating.">
    <meta property="og:type" content="article">
    <title>Algoritmos de Bagging | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>

<body>
    <div class="container">
        <header>
            <div class="header-container">
                <div class="logo-container">
                    <a href="../index.html">
                        <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                    </a>
                    <div class="logo-text">
                        iLERNA
                        <span>Curso de Especializaci√≥n en IA y Big Data</span>
                    </div>
                </div>
                <div class="breadcrumb">
                    <a href="../index.html">Inicio</a> ‚Ä∫
                    <a href="index.html">Sistemas de Aprendizaje Autom√°tico</a> ‚Ä∫
                    <span>Algoritmos de Bagging</span>
                </div>
            </div>
            <h1 class="text-center">Algoritmos de Bagging</h1>
            <p class="subtitle text-center">La fuerza de la uni√≥n: Bootstrap Aggregating explicado</p>
        </header>

        <main>
            <!-- SECCI√ìN 1: INTRODUCCI√ìN -->
            <section class="section">
                <h2 class="section-title">¬øQu√© es Bagging?</h2>
                <p>
                    <strong>Bagging</strong> (acr√≥nimo de <em>Bootstrap Aggregating</em>) es una t√©cnica de <strong>Ensemble
                    Learning</strong> que busca mejorar la estabilidad y precisi√≥n de los algoritmos de Machine Learning. La idea central es
                    simple pero poderosa: en lugar de confiar en un solo modelo "experto", confiamos en la <strong>"sabidur√≠a de la
                    multitud"</strong> de muchos modelos m√°s simples.
                </p>
                <p>
                    Su principal objetivo es <strong>reducir la varianza</strong> y evitar el <strong>overfitting</strong>
                    (sobreajuste), especialmente en modelos inestables como los √Årboles de Decisi√≥n.
                </p>

                <div class="comparison-grid">
                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">Bootstrap</h4>
                        <p>
                            Muestreo aleatorio <strong>con reemplazo</strong>. Creamos m√∫ltiples "versiones" diferentes de
                            nuestro dataset original.
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #8A7AAF;">Aggregating</h4>
                        <p>
                            Combinaci√≥n de resultados. <strong>Votaci√≥n</strong> para clasificaci√≥n (mayor√≠a gana) o
                            <strong>Promedio</strong> para regresi√≥n.
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">Paralelismo</h4>
                        <p>
                            Cada modelo se entrena de forma <strong>independiente</strong>, lo que permite aprovechar al
                            m√°ximo los procesadores modernos.
                        </p>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 2: DIAGRAMA DEL PROCESO -->
            <section class="section">
                <h2 class="section-title">El Proceso Paso a Paso</h2>

                <div style="margin: 2rem 0;">
                    <svg viewBox="0 0 900 450" style="width: 100%; max-width: 100%; display: block; border: 2px solid #e5e5e5; border-radius: 1rem; background: #fafafa; padding: 1rem;">
                        <defs>
                            <marker id="arrowBag" markerWidth="10" markerHeight="10" refX="6" refY="3" orient="auto"
                                markerUnits="strokeWidth">
                                <path d="M0,0 L0,6 L6,3 z" fill="#333333" />
                            </marker>
                        </defs>

                        <!-- Dataset Original -->
                        <rect x="350" y="20" width="200" height="50" rx="10" fill="#333333" />
                        <text x="450" y="50" text-anchor="middle" font-family="Montserrat" font-size="16" font-weight="700"
                            fill="#ffffff">
                            Dataset Original
                        </text>

                        <!-- Flechas a Samples -->
                        <line x1="450" y1="70" x2="200" y2="120" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />
                        <line x1="450" y1="70" x2="450" y2="120" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />
                        <line x1="450" y1="70" x2="700" y2="120" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />

                        <!-- Bootstrap Samples -->
                        <rect x="100" y="120" width="200" height="60" rx="10" fill="#E8F7FA" stroke="#49B9CE" stroke-width="2" />
                        <text x="200" y="145" text-anchor="middle" font-family="Montserrat" font-size="14" font-weight="700"
                            fill="#49B9CE">Sample 1</text>
                        <text x="200" y="165" text-anchor="middle" font-family="Montserrat" font-size="12" fill="#555555">(con
                            reemplazo)</text>

                        <rect x="350" y="120" width="200" height="60" rx="10" fill="#E8F7FA" stroke="#49B9CE" stroke-width="2" />
                        <text x="450" y="145" text-anchor="middle" font-family="Montserrat" font-size="14" font-weight="700"
                            fill="#49B9CE">Sample 2</text>
                        <text x="450" y="165" text-anchor="middle" font-family="Montserrat" font-size="12" fill="#555555">(con
                            reemplazo)</text>

                        <rect x="600" y="120" width="200" height="60" rx="10" fill="#E8F7FA" stroke="#49B9CE" stroke-width="2" />
                        <text x="700" y="145" text-anchor="middle" font-family="Montserrat" font-size="14" font-weight="700"
                            fill="#49B9CE">Sample N</text>
                        <text x="700" y="165" text-anchor="middle" font-family="Montserrat" font-size="12" fill="#555555">(con
                            reemplazo)</text>

                        <!-- Flechas a Modelos -->
                        <line x1="200" y1="180" x2="200" y2="220" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />
                        <line x1="450" y1="180" x2="450" y2="220" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />
                        <line x1="700" y1="180" x2="700" y2="220" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />

                        <!-- Modelos -->
                        <rect x="125" y="220" width="150" height="50" rx="5" fill="#F0EDF5" stroke="#8A7AAF" stroke-width="2" />
                        <text x="200" y="250" text-anchor="middle" font-family="Montserrat" font-size="14" font-weight="700"
                            fill="#8A7AAF">Modelo 1</text>

                        <rect x="375" y="220" width="150" height="50" rx="5" fill="#F0EDF5" stroke="#8A7AAF" stroke-width="2" />
                        <text x="450" y="250" text-anchor="middle" font-family="Montserrat" font-size="14" font-weight="700"
                            fill="#8A7AAF">Modelo 2</text>

                        <rect x="625" y="220" width="150" height="50" rx="5" fill="#F0EDF5" stroke="#8A7AAF" stroke-width="2" />
                        <text x="700" y="250" text-anchor="middle" font-family="Montserrat" font-size="14" font-weight="700"
                            fill="#8A7AAF">Modelo N</text>

                        <!-- Flechas a Agregaci√≥n -->
                        <line x1="200" y1="270" x2="350" y2="330" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />
                        <line x1="450" y1="270" x2="450" y2="330" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />
                        <line x1="700" y1="270" x2="550" y2="330" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />

                        <!-- Agregaci√≥n -->
                        <rect x="300" y="330" width="300" height="60" rx="10" fill="#FFF8DC" stroke="#FFA726" stroke-width="3" />
                        <text x="450" y="355" text-anchor="middle" font-family="Montserrat" font-size="16" font-weight="700"
                            fill="#E65100">
                            AGREGACI√ìN
                        </text>
                        <text x="450" y="375" text-anchor="middle" font-family="Montserrat" font-size="13" fill="#E65100">
                            Votaci√≥n (Clasificaci√≥n) / Promedio (Regresi√≥n)
                        </text>

                        <!-- Resultado Final -->
                        <line x1="450" y1="390" x2="450" y2="410" stroke="#333333" stroke-width="2" marker-end="url(#arrowBag)" />
                        <rect x="350" y="410" width="200" height="40" rx="5" fill="#49B9CE" />
                        <text x="450" y="435" text-anchor="middle" font-family="Montserrat" font-size="16" font-weight="700"
                            fill="#ffffff">
                            Predicci√≥n Final
                        </text>
                    </svg>
                </div>

                <div class="highlight-box" style="background: #FFF8DC; border-left: 4px solid #FFA726;">
                    <h3 style="color: #E65100; margin-top: 0;">¬øPor qu√© funciona?</h3>
                    <p>
                        Imagina que pides consejo m√©dico. Un solo doctor puede equivocarse (alta varianza). Pero si
                        consultas a <strong>100 doctores</strong> y tomas la opini√≥n mayoritaria, es mucho menos probable que el
                        diagn√≥stico sea err√≥neo. Bagging hace exactamente esto con modelos matem√°ticos.
                    </p>
                </div>
            </section>

            <!-- SECCI√ìN 3: RANDOM FOREST -->
            <section class="section">
                <h2 class="section-title">Random Forest: El rey del Bagging</h2>
                <p>
                    El algoritmo m√°s famoso que utiliza Bagging es <strong>Random Forest</strong>. Lleva la idea un paso m√°s
                    all√°: no solo hace muestreo aleatorio de los <strong>datos</strong> (filas), sino tambi√©n de las
                    <strong>caracter√≠sticas</strong> (columnas) en cada divisi√≥n del √°rbol.
                </p>

                <div class="comparison-grid" style="margin-bottom: 2rem;">
                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">√Årbol de Decisi√≥n</h4>
                        <p>
                            Modelo base. Tiende a ser muy profundo y complejo, memorizando los datos (overfitting).
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #8A7AAF;">Random Forest</h4>
                        <p>
                            Cientos de √°rboles entrenados en subconjuntos diferentes. Al promediar sus errores, se obtiene
                            un modelo robusto.
                        </p>
                    </div>
                </div>

                <h3 style="color: #49B9CE; margin-top: 2rem;">Ejemplo en Python - Random Forest con scikit-learn</h3>
                <div class="code-block">
                    <div class="code-header">Python</div>
                    <pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 1. Cargar datos
data = load_wine()
X, y = data.data, data.target

# 2. Dividir en train y test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. Crear el modelo Random Forest
# n_estimators=100 -> Creamos 100 √°rboles de decisi√≥n
modelo = RandomForestClassifier(n_estimators=100, random_state=42)

# 4. Entrenar (Bagging en acci√≥n)
modelo.fit(X_train, y_train)

# 5. Predecir y evaluar
y_pred = modelo.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Precisi√≥n del Random Forest: {accuracy:.2f}")
# Salida t√≠pica: Precisi√≥n del Random Forest: 1.00 (o muy alta)</code></pre>
                </div>

                <div class="highlight-box" style="background: linear-gradient(to right, #E8F7FA, #F0EDF5); margin-top: 2rem; border-left: 4px solid #49B9CE;">
                    <h4 style="color: #49B9CE; margin-top: 0;">Caracter√≠sticas Clave de Random Forest</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>Muestreo doble:</strong> Bootstrap de filas + selecci√≥n aleatoria de columnas en cada divisi√≥n</li>
                        <li><strong>Decorrelaci√≥n:</strong> Los √°rboles son m√°s diversos entre s√≠, mejorando el ensemble</li>
                        <li><strong>Out-of-Bag (OOB) Score:</strong> Validaci√≥n autom√°tica sin necesidad de conjunto de validaci√≥n</li>
                        <li><strong>Importancia de caracter√≠sticas:</strong> Identifica qu√© variables son m√°s relevantes para las predicciones</li>
                    </ul>
                </div>
            </section>

            <!-- SECCI√ìN 4: VENTAJAS Y DESVENTAJAS -->
            <section class="section">
                <h2 class="section-title">Ventajas y Desventajas</h2>

                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 2rem;">
                    <div class="feature-card">
                        <h3 style="color: #4CAF50; margin-top: 0;">Ventajas</h3>
                        <ul style="line-height: 1.8; margin-left: 1.5rem;">
                            <li>Reduce significativamente el <strong>overfitting</strong>.</li>
                            <li>Funciona muy bien con datos de <strong>alta dimensionalidad</strong>.</li>
                            <li>Maneja autom√°ticamente valores faltantes y mantiene la precisi√≥n.</li>
                            <li>Es f√°cil de paralelizar (entrenamiento r√°pido en clusters).</li>
                            <li>Robusto ante datos ruidosos y outliers.</li>
                            <li>Proporciona estimaci√≥n de importancia de caracter√≠sticas.</li>
                        </ul>
                    </div>

                    <div class="feature-card secondary">
                        <h3 style="color: #C62828; margin-top: 0;">Desventajas</h3>
                        <ul style="line-height: 1.8; margin-left: 1.5rem;">
                            <li>Pierde <strong>interpretabilidad</strong>. Es dif√≠cil visualizar 100 √°rboles a la vez (Caja Negra).</li>
                            <li>Puede ser computacionalmente costoso en tiempo de <strong>predicci√≥n</strong> (no entrenamiento).</li>
                            <li>No mejora mucho si los modelos base son malos (bias alto).</li>
                            <li>Requiere m√°s memoria para almacenar m√∫ltiples modelos.</li>
                            <li>No es √≥ptimo para problemas de regresi√≥n con extrapolaci√≥n.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 5: HIPERPAR√ÅMETROS CLAVE -->
            <section class="section">
                <h2 class="section-title">Hiperpar√°metros Clave en Random Forest</h2>
                <p>
                    Optimizar los hiperpar√°metros es crucial para obtener el mejor rendimiento de Random Forest. Estos son los m√°s importantes:
                </p>

                <div class="comparison-grid">
                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">n_estimators</h4>
                        <p>
                            N√∫mero de √°rboles en el bosque. M√°s √°rboles = mejor rendimiento pero mayor costo computacional.
                            T√≠picamente: 100-500.
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #8A7AAF;">max_depth</h4>
                        <p>
                            Profundidad m√°xima de cada √°rbol. Controla el overfitting. None = sin l√≠mite (√°rboles crecen hasta pureza).
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">max_features</h4>
                        <p>
                            N√∫mero de caracter√≠sticas consideradas en cada divisi√≥n. 'sqrt' o 'log2' son valores comunes que aumentan diversidad.
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #8A7AAF;">min_samples_split</h4>
                        <p>
                            M√≠nimo de muestras para dividir un nodo. Valores m√°s altos previenen overfitting. T√≠picamente: 2-20.
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">min_samples_leaf</h4>
                        <p>
                            M√≠nimo de muestras en hojas. Similar a min_samples_split. Controla el tama√±o de las hojas.
                        </p>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #8A7AAF;">bootstrap</h4>
                        <p>
                            Si True, usa bootstrap sampling. Si False, usa todo el dataset (no es Bagging entonces). Siempre True para RF.
                        </p>
                    </div>
                </div>

                <div class="warning-box" style="margin-top: 2rem;">
                    <h4 style="margin-top: 0;">Recomendaciones de Configuraci√≥n</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>Configuraci√≥n inicial:</strong> n_estimators=100, max_features='sqrt', sin l√≠mite de profundidad</li>
                        <li><strong>Si hay overfitting:</strong> Reduce max_depth (ej: 10-20) o aumenta min_samples_split/leaf</li>
                        <li><strong>Si hay underfitting:</strong> Aumenta n_estimators o max_depth, reduce restricciones</li>
                        <li><strong>Para datasets grandes:</strong> Reduce max_features para acelerar entrenamiento</li>
                        <li><strong>Optimizaci√≥n:</strong> Usa GridSearchCV o RandomizedSearchCV para encontrar mejores valores</li>
                    </ul>
                </div>
            </section>

            <!-- SECCI√ìN 6: APLICACIONES PR√ÅCTICAS -->
            <section class="section">
                <h2 class="section-title">Aplicaciones Pr√°cticas del Bagging</h2>
                <p>
                    El Bagging y Random Forest son utilizados en una amplia variedad de aplicaciones del mundo real gracias a su
                    robustez y precisi√≥n:
                </p>

                <div class="grid-features">
                    <div class="feature-card">
                        <h4 style="color: #49B9CE;">Sector Financiero</h4>
                        <p>
                            Detecci√≥n de fraude en transacciones, evaluaci√≥n de riesgo crediticio, predicci√≥n de defaults,
                            y an√°lisis de trading algor√≠tmico.
                        </p>
                    </div>

                    <div class="feature-card secondary">
                        <h4 style="color: #8A7AAF;">Medicina y Salud</h4>
                        <p>
                            Diagn√≥stico m√©dico asistido, predicci√≥n de enfermedades, an√°lisis de im√°genes m√©dicas,
                            y personalizaci√≥n de tratamientos.
                        </p>
                    </div>

                    <div class="feature-card">
                        <h4 style="color: #49B9CE;">E-commerce</h4>
                        <p>
                            Sistemas de recomendaci√≥n, predicci√≥n de churn (abandono de clientes), segmentaci√≥n de usuarios,
                            y optimizaci√≥n de precios din√°micos.
                        </p>
                    </div>

                    <div class="feature-card secondary">
                        <h4 style="color: #8A7AAF;">Industria</h4>
                        <p>
                            Mantenimiento predictivo de maquinaria, control de calidad automatizado, optimizaci√≥n de procesos,
                            y detecci√≥n de anomal√≠as en producci√≥n.
                        </p>
                    </div>
                </div>

                <div class="highlight-box" style="background: linear-gradient(to right, #E8F7FA, #F0EDF5); margin-top: 2rem; border-left: 4px solid #8A7AAF;">
                    <h4 style="color: #8A7AAF; margin-top: 0;">Caso de √âxito: Microsoft Kinect</h4>
                    <p>
                        El sensor Kinect de Microsoft utiliza Random Forest para reconocer la postura corporal humana en tiempo real.
                        El modelo procesa im√°genes de profundidad a 30 FPS, identificando 31 puntos articulares del cuerpo con
                        alta precisi√≥n. El sistema fue entrenado con millones de poses sint√©ticas generadas, demostrando la
                        capacidad de Random Forest para manejar grandes vol√∫menes de datos y proporcionar predicciones r√°pidas.
                    </p>
                </div>
            </section>

            <!-- SECCI√ìN 7: COMPARACI√ìN CON OTRAS T√âCNICAS DE ENSEMBLE -->
            <section class="section">
                <h2 class="section-title">Bagging vs. Otras T√©cnicas de Ensemble</h2>
                <p>
                    Bagging es una de varias t√©cnicas de Ensemble Learning. Aqu√≠ comparamos las principales:
                </p>

                <div style="overflow-x: auto;">
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Caracter√≠stica</th>
                                <th>Bagging</th>
                                <th>Boosting</th>
                                <th>Stacking</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Objetivo principal</strong></td>
                                <td>Reducir varianza</td>
                                <td>Reducir bias</td>
                                <td>Maximizar precisi√≥n</td>
                            </tr>
                            <tr>
                                <td><strong>Entrenamiento</strong></td>
                                <td>Paralelo (independiente)</td>
                                <td>Secuencial (iterativo)</td>
                                <td>Multi-nivel</td>
                            </tr>
                            <tr>
                                <td><strong>Muestreo de datos</strong></td>
                                <td>Bootstrap (con reemplazo)</td>
                                <td>Ponderaci√≥n adaptativa</td>
                                <td>Validaci√≥n cruzada</td>
                            </tr>
                            <tr>
                                <td><strong>Modelos base</strong></td>
                                <td>Generalmente iguales</td>
                                <td>Generalmente iguales</td>
                                <td>Diferentes tipos</td>
                            </tr>
                            <tr>
                                <td><strong>Agregaci√≥n</strong></td>
                                <td>Votaci√≥n/Promedio simple</td>
                                <td>Votaci√≥n ponderada</td>
                                <td>Meta-modelo entrenado</td>
                            </tr>
                            <tr>
                                <td><strong>Sensibilidad a outliers</strong></td>
                                <td>Baja (robusto)</td>
                                <td>Alta (sensible)</td>
                                <td>Media</td>
                            </tr>
                            <tr>
                                <td><strong>Riesgo de overfitting</strong></td>
                                <td>Bajo</td>
                                <td>Medio-Alto</td>
                                <td>Medio</td>
                            </tr>
                            <tr>
                                <td><strong>Velocidad</strong></td>
                                <td>R√°pido (paralelizable)</td>
                                <td>M√°s lento (secuencial)</td>
                                <td>Variable</td>
                            </tr>
                            <tr>
                                <td><strong>Ejemplos</strong></td>
                                <td>Random Forest, Bagged Trees</td>
                                <td>AdaBoost, XGBoost, LightGBM</td>
                                <td>Blending, Stacked Generalization</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="warning-box" style="margin-top: 2rem;">
                    <h4 style="margin-top: 0;">¬øCu√°ndo usar cada t√©cnica?</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>Bagging:</strong> Cuando tienes modelos de alta varianza (ej: √°rboles profundos), datos con ruido, o necesitas paralelizaci√≥n</li>
                        <li><strong>Boosting:</strong> Cuando tienes modelos de alto bias (ej: √°rboles poco profundos) o buscas m√°xima precisi√≥n y puedes tolerar mayor tiempo de entrenamiento</li>
                        <li><strong>Stacking:</strong> Cuando quieres combinar modelos muy diferentes (ej: √°rboles + redes neuronales + SVM) y tienes experiencia en ML</li>
                    </ul>
                </div>
            </section>

            <!-- SECCI√ìN 8: C√ìDIGO AVANZADO -->
            <section class="section">
                <h2 class="section-title">Ejemplo Completo: Random Forest con Validaci√≥n y Tuning</h2>
                <p>
                    Veamos un ejemplo m√°s completo que incluye validaci√≥n cruzada, optimizaci√≥n de hiperpar√°metros y an√°lisis de importancia de caracter√≠sticas:
                </p>

                <div class="code-block">
                    <div class="code-header">Python - Random Forest Avanzado</div>
                    <pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import numpy as np

# 1. Cargar dataset
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names

# 2. Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 3. Definir grid de hiperpar√°metros
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# 4. Grid Search con validaci√≥n cruzada
rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(
    rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1
)
grid_search.fit(X_train, y_train)

# 5. Mejor modelo
best_rf = grid_search.best_estimator_
print(f"Mejores par√°metros: {grid_search.best_params_}")
print(f"Mejor score CV: {grid_search.best_score_:.4f}")

# 6. Evaluaci√≥n en test
y_pred = best_rf.predict(X_test)
print("\nReporte de clasificaci√≥n:")
print(classification_report(y_test, y_pred, target_names=data.target_names))

# 7. Importancia de caracter√≠sticas
importances = best_rf.feature_importances_
indices = np.argsort(importances)[::-1]

print("\nTop 10 caracter√≠sticas m√°s importantes:")
for i in range(10):
    print(f"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")

# 8. Out-of-Bag Score (opcional)
rf_oob = RandomForestClassifier(
    n_estimators=200, oob_score=True, random_state=42
)
rf_oob.fit(X_train, y_train)
print(f"\nOOB Score: {rf_oob.oob_score_:.4f}")

# 9. Validaci√≥n cruzada adicional
cv_scores = cross_val_score(best_rf, X_train, y_train, cv=10, scoring='accuracy')
print(f"\nCV Scores (10-fold): {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")</code></pre>
                </div>

                <div class="highlight-box" style="margin-top: 1.5rem; background: #E8F7FA; border-left: 4px solid #49B9CE;">
                    <h4 style="color: #49B9CE; margin-top: 0;">Explicaci√≥n del C√≥digo</h4>
                    <ul style="line-height: 1.8;">
                        <li><strong>GridSearchCV:</strong> Prueba todas las combinaciones de hiperpar√°metros autom√°ticamente</li>
                        <li><strong>cv=5:</strong> Validaci√≥n cruzada de 5 folds para evaluar cada configuraci√≥n</li>
                        <li><strong>n_jobs=-1:</strong> Utiliza todos los cores disponibles del procesador</li>
                        <li><strong>stratify=y:</strong> Mantiene la proporci√≥n de clases en train/test</li>
                        <li><strong>feature_importances_:</strong> Muestra qu√© variables contribuyen m√°s a las predicciones</li>
                        <li><strong>oob_score:</strong> Validaci√≥n autom√°tica usando muestras out-of-bag (no usadas en cada √°rbol)</li>
                    </ul>
                </div>
            </section>

            <!-- SECCI√ìN 9: DATO CURIOSO -->
            <section class="section">
                <div class="warning-box">
                    <h3 style="margin-top: 0;">Dato Curioso: El Origen de Random Forest</h3>
                    <p>
                        <strong>Random Forest</strong> fue inventado por <strong>Leo Breiman</strong> en 2001, el mismo
                        investigador que co-desarroll√≥ el algoritmo CART (√°rboles de decisi√≥n) en 1984. Su art√≠culo original
                        "Random Forests" publicado en Machine Learning tiene m√°s de 85,000 citas y es uno de los papers
                        m√°s influyentes en la historia del ML.
                    </p>
                    <div class="highlight-box" style="margin-top: 1rem; background: linear-gradient(to right, #E8F7FA, #F0EDF5);">
                        <p style="margin-bottom: 0;">
                            <strong>Curiosidad adicional:</strong> El nombre original de Breiman para la t√©cnica era
                            "Random Input Selection" (Selecci√≥n Aleatoria de Entradas), pero cambi√≥ a "Random Forests"
                            porque sonaba mejor y era m√°s memorable. ¬°La mercadotecnia importa incluso en ciencia! Hoy
                            Random Forest es uno de los 3 algoritmos m√°s usados en Kaggle competitions, junto a XGBoost
                            y redes neuronales.
                        </p>
                    </div>
                </div>
            </section>
        </main>

        <footer>
            <div class="footer-content">
                <img src="../img/logo-ilerna.svg" alt="ILERNA" style="height: 40px; margin-bottom: 1rem;">
                <h3>Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</h3>
                <p><a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a></p>
                <p style="font-size: 0.9rem; color: #777; margin-top: 1rem;">Centro oficial de FP online y presencial.
                    Ciclos formativos de Grado Medio y Grado Superior.</p>
                <p style="font-size: 0.9rem; color: #777;">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>
            </div>
            <div class="penguin">
                <span>üêß</span>
            </div>
        </footer>
    </div>

    <!-- Prism.js para syntax highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <!-- Script para copiar c√≥digo -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const codeBlocks = document.querySelectorAll('pre code');

            codeBlocks.forEach((block) => {
                const pre = block.parentElement;
                const wrapper = document.createElement('div');
                wrapper.style.position = 'relative';

                pre.parentNode.insertBefore(wrapper, pre);
                wrapper.appendChild(pre);

                const button = document.createElement('button');
                button.className = 'copy-code-btn';
                button.innerHTML = `
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                    </svg>
                    <span>Copiar c√≥digo</span>
                `;

                button.addEventListener('click', async () => {
                    const code = block.textContent;

                    try {
                        await navigator.clipboard.writeText(code);
                        button.innerHTML = `
                            <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
                            </svg>
                            <span>¬°Copiado!</span>
                        `;
                        button.style.background = '#43A047';

                        setTimeout(() => {
                            button.innerHTML = `
                                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
                                </svg>
                                <span>Copiar c√≥digo</span>
                            `;
                            button.style.background = '#49B9CE';
                        }, 2000);
                    } catch (err) {
                        console.error('Error al copiar:', err);
                    }
                });

                wrapper.appendChild(button);
            });
        });
    </script>
</body>

</html>