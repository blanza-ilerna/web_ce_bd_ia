<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Entornos de Modelado de IA: Bibliotecas y Frameworks - iLERNA">
    <meta name="keywords"
        content="IA, TensorFlow, PyTorch, Scikit-learn, OpenCV, Hugging Face, NLP, Computer Vision, Machine Learning">
    <meta name="author" content="Bjlanza">
    <meta name="organization" content="ILERNA">
    <title>Bibliotecas y Frameworks de IA | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</head>

<body>
    <header>
        <div class="header-container">
            <div class="logo-container">
                <a href="../index.html">
                    <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                </a>
                <div class="logo-text">
                    iLERNA
                    <span>Curso de Especializaci√≥n en IA y Big Data</span>
                </div>
            </div>
            <nav class="breadcrumb">
                <a href="../index.html">Inicio</a> ‚Ä∫
                <a href="index.html">Programaci√≥n IA</a> ‚Ä∫
                <span>Bibliotecas y Frameworks</span>
            </nav>
        </div>
    </header>

    <main class="container">
        <!-- Hero Section -->
        <div class="hero">
            <h1 class="color-primary">Bibliotecas y Frameworks de IA</h1>
            <p class="subtitle">El pilar t√©cnico del modelado inteligente</p>
        </div>

        <!-- SECCI√ìN 1: INTRODUCCI√ìN -->
        <section class="section">
            <h2 class="section-title">El Ecosistema del Modelado</h2>
            <p class="mb-4">
                El desarrollo de Inteligencia Artificial ha evolucionado desde la escritura manual de algoritmos
                matem√°ticos complejos hacia el uso de sofisticados frameworks y bibliotecas. Estas herramientas act√∫an
                como cimientos robustos que abstraen la complejidad del c√°lculo tensorial, la diferenciaci√≥n autom√°tica
                y la optimizaci√≥n de hardware, permitiendo a ingenieros y cient√≠ficos de datos centrarse en el dise√±o de
                arquitecturas y la resoluci√≥n de problemas de negocio.
            </p>
            <p class="mb-4">
                Un framework de IA moderno no es solo una colecci√≥n de funciones; es un ecosistema completo que abarca
                desde la preparaci√≥n de datos y el entrenamiento distribuido en cl√∫steres de GPUs, hasta el despliegue
                de modelos optimizados en dispositivos m√≥viles o servidores en la nube. La elecci√≥n de la herramienta
                adecuada puede determinar la velocidad de experimentaci√≥n, la escalabilidad del producto final y la
                facilidad de mantenimiento a largo plazo.
            </p>

            <div class="highlight-box primary">
                <p>
                    A continuaci√≥n, exploraremos en profundidad las herramientas m√°s influyentes del panorama actual,
                    detallando sus caracter√≠sticas, filosof√≠as de dise√±o y casos de uso ideales, acompa√±ados de ejemplos
                    pr√°cticos.
                </p>
            </div>
        </section>

        <!-- SECCI√ìN 2: DEEP LEARNING -->
        <section class="section">
            <h2 class="section-title">Redes Neuronales y Aprendizaje Profundo</h2>

            <!-- TensorFlow -->
            <div class="card secondary mb-6">
                <h3 class="color-secondary">TensorFlow y Keras (Google)</h3>
                <p class="mb-2">
                    <strong>TensorFlow</strong>, desarrollado por el equipo de Google Brain, es una plataforma de c√≥digo
                    abierto de extremo a extremo para el aprendizaje autom√°tico. Desde su lanzamiento en 2015, se ha
                    establecido como el est√°ndar industrial para el desarrollo de productos de IA a gran escala. Su
                    arquitectura permite desplegar c√≥mputo no solo en CPUs y GPUs, sino tambi√©n en TPUs (Tensor
                    Processing Units), hardware especializado de Google para acelerar cargas de trabajo de ML.
                </p>
                <p class="mb-2">
                    <strong>Keras</strong>, integrada oficialmente en TensorFlow 2.0, act√∫a como la API de alto nivel
                    del framework. Dise√±ada con el ser humano en mente y no la m√°quina, Keras prioriza la experiencia
                    del desarrollador, ofreciendo una sintaxis clara, consistente y modular. Esto permite prototipar
                    redes neuronales complejas en cuesti√≥n de minutos, apilando capas como si fueran bloques de
                    construcci√≥n, sin sacrificar la potencia subyacente de TensorFlow.
                </p>
                <p class="mb-4">
                    M√°s all√° del entrenamiento, el ecosistema de TensorFlow brilla en la fase de producci√≥n (MLOps).
                    Herramientas como TensorFlow Extended (TFX) para pipelines de datos, TensorFlow Lite para ejecutar
                    modelos en dispositivos m√≥viles y IoT, y TensorFlow.js para ejecutar modelos directamente en el
                    navegador, lo convierten en la opci√≥n predilecta para empresas que necesitan llevar sus modelos del
                    laboratorio al mundo real de forma robusta y escalable.
                </p>

                <h4 class="font-bold mb-2">Ejemplo de Red Neuronal Simple:</h4>
                <pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

# Definici√≥n de una red neuronal secuencial simple
# Input: vectores de 784 dimensiones (ej. im√°genes aplanadas de 28x28)
# Capas ocultas: Una densa con 128 neuronas y activaci√≥n ReLU, seguida de Dropout para evitar overfitting
# Output: 10 neuronas (probabilidades para 10 clases)
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# Compilaci√≥n definiendo optimizador, funci√≥n de p√©rdida y m√©tricas
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

print("Modelo TensorFlow compilado correctamente y listo para entrenar.")</code></pre>
            </div>

            <!-- PyTorch -->
            <div class="card primary mb-6">
                <h3 class="color-primary">PyTorch (Meta AI)</h3>
                <p class="mb-2">
                    <strong>PyTorch</strong>, nacido en el laboratorio de investigaci√≥n de IA de Facebook (ahora Meta),
                    ha ganado una popularidad explosiva debido a su filosof√≠a de "Python primero". A diferencia de los
                    grafos est√°ticos de las primeras versiones de TensorFlow, PyTorch introdujo el concepto de grafos
                    computacionales din√°micos. Esto significa que la estructura de la red neuronal se puede modificar
                    sobre la marcha durante la ejecuci√≥n, lo que es invaluable para trabajar con datos de longitud
                    variable (como texto) y arquitecturas complejas.
                </p>
                <p class="mb-2">
                    Esta flexibilidad, sumada a una interfaz que se siente natural para cualquier programador de Python
                    y una facilidad superior para la depuraci√≥n (debugging), lo ha convertido en el framework dominante
                    en el √°mbito acad√©mico y de investigaci√≥n. La mayor√≠a de los "papers" cient√≠ficos modernos y los
                    avances de vanguardia en IA se publican con implementaciones en PyTorch, gracias a su capacidad para
                    facilitar la experimentaci√≥n r√°pida y la innovaci√≥n.
                </p>
                <p class="mb-4">
                    Aunque inicialmente se le consideraba menos apto para producci√≥n que TensorFlow, PyTorch ha madurado
                    enormemente con la introducci√≥n de <strong>TorchScript</strong> y <strong>TorchServe</strong>. Estas
                    herramientas permiten serializar modelos y optimizarlos para entornos de alta eficiencia, cerrando
                    la brecha entre la investigaci√≥n y el despliegue industrial. Grandes compa√±√≠as como OpenAI
                    (creadores de GPT) y Tesla utilizan PyTorch como base para sus sistemas de inteligencia artificial.
                </p>

                <h4 class="font-bold mb-2">Ejemplo de Clase de Modelo en PyTorch:</h4>
                <pre><code class="language-python">import torch
import torch.nn as nn

# Definici√≥n de una red neuronal como una clase de Python
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        # Definici√≥n de capas
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # Definici√≥n del flujo de datos (Forward Pass)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# Instanciaci√≥n y verificaci√≥n
net = SimpleNet()
print(net)</code></pre>
            </div>
        </section>

        <!-- SECCI√ìN 3: MACHINE LEARNING CL√ÅSICO -->
        <section class="section">
            <h2 class="section-title">Aprendizaje Autom√°tico Cl√°sico</h2>

            <!-- Scikit-Learn -->
            <div class="card secondary mb-6">
                <h3 class="color-secondary">Scikit-learn (Sklearn)</h3>
                <p class="mb-2">
                    <strong>Scikit-learn</strong> es la piedra angular del aprendizaje autom√°tico cl√°sico en Python.
                    Construida sobre las s√≥lidas bases de NumPy, SciPy y Matplotlib, ofrece una colecci√≥n coherente y
                    eficiente de herramientas para miner√≠a y an√°lisis de datos. Su mayor virtud es su API consistente:
                    pr√°cticamente todos los algoritmos, desde una regresi√≥n lineal simple hasta un complejo bosque
                    aleatorio, se utilizan con los mismos m√©todos `.fit()`, `.predict()` y `.transform()`.
                </p>
                <p class="mb-2">
                    Aunque no est√° dise√±ada para Deep Learning ni para el uso intensivo de GPUs, Scikit-learn sigue
                    siendo imbatible para datos estructurados (tablas de Excel, bases de datos SQL). Cubre una gama
                    inmensa de necesidades: algoritmos de clasificaci√≥n (SVM, KNN), regresi√≥n, clustering (K-Means,
                    DBSCAN), reducci√≥n de dimensionalidad (PCA) y selecci√≥n de modelos. Es a menudo la primera
                    herramienta que se utiliza en un proyecto para establecer un "baseline" o l√≠nea base de rendimiento.
                </p>
                <p class="mb-4">
                    Adem√°s de los algoritmos de predicci√≥n, Scikit-learn incluye utilidades excepcionales para el
                    <strong>preprocesamiento de datos</strong>. Funciones para imputar valores nulos, escalar variables
                    num√©ricas, codificar variables categ√≥ricas y dividir datasets en conjuntos de entrenamiento y prueba
                    son esenciales en cualquier pipeline de datos, incluso si el modelado final se realiza luego con una
                    red neuronal en otro framework.
                </p>

                <h4 class="font-bold mb-2">Entrenamiento de un Random Forest:</h4>
                <pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Cargar datos de ejemplo y dividir en entrenamiento/prueba
data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)

# Crear y entrenar un Bosque Aleatorio con 100 √°rboles
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Evaluar el modelo
print(f"Precisi√≥n en test: {clf.score(X_test, y_test):.2f}")</code></pre>
            </div>

            <!-- XGBoost -->
            <div class="card primary mb-6">
                <h3 class="color-primary">XGBoost (Extreme Gradient Boosting)</h3>
                <p class="mb-2">
                    <strong>XGBoost</strong> es una biblioteca optimizada de "Gradient Boosting" distribuido, dise√±ada
                    para ser altamente eficiente, flexible y port√°til. Naci√≥ con un objetivo claro: velocidad de
                    ejecuci√≥n y rendimiento del modelo. Su algoritmo principal se basa en construir un ensamble de
                    √°rboles de decisi√≥n de manera secuencial, donde cada nuevo √°rbol intenta corregir los errores
                    cometidos por los anteriores, logrando una precisi√≥n que a menudo supera a otros algoritmos
                    cl√°sicos.
                </p>
                <p class="mb-2">
                    Esta librer√≠a se ha convertido en una leyenda en la comunidad de ciencia de datos, dominando las
                    competiciones de Kaggle y otros torneos de modelado predictivo durante a√±os. Su capacidad para
                    manejar valores faltantes de forma nativa, su soporte para regularizaci√≥n (que evita el sobreajuste)
                    y su implementaci√≥n de "tree pruning" (poda de √°rboles) la hacen extremadamente robusta para una
                    gran variedad de problemas de datos tabulares.
                </p>
                <p class="mb-4">
                    T√©cnicamente, XGBoost destaca por su capacidad de escalado. Puede ejecutarse en una sola m√°quina, en
                    clusters distribuidos como Hadoop y Spark, o aprovechar la aceleraci√≥n por GPU. Proporciona
                    interfaces para m√∫ltiples lenguajes (C++, Python, R, Java, Julia), lo que facilita su integraci√≥n en
                    casi cualquier stack tecnol√≥gico existente en una empresa.
                </p>

                <pre><code class="language-python">import xgboost as xgb
from sklearn.datasets import load_diabetes

# Cargar dataset de diabetes
data = load_diabetes()
X, y = data.data, data.target

# Configurar y entrenar el modelo XGBoost regressor
# objective='reg:squarederror' indica que es un problema de regresi√≥n
model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=50, learning_rate=0.1)
model.fit(X, y)

print("Entrenamiento completado. El modelo est√° listo para predecir.")</code></pre>
            </div>
        </section>

        <!-- SECCI√ìN 4: PROCESAMIENTO DEL LENGUAJE NATURAL (NLP) -->
        <section class="section">
            <h2 class="section-title">Procesamiento del Lenguaje Natural (NLP)</h2>

            <!-- Hugging Face -->
            <div class="card secondary mb-6">
                <h3 class="color-secondary">Hugging Face Transformers</h3>
                <p class="mb-2">
                    Conocida a menudo como el "GitHub del Machine Learning", <strong>Hugging Face</strong> ha
                    democratizado el acceso a la Inteligencia Artificial de vanguardia. Su biblioteca `transformers`
                    permite a cualquier desarrollador descargar, configurar y utilizar modelos de √∫ltima generaci√≥n
                    (State-of-the-Art) para procesamiento de texto, imagen y audio con apenas unas pocas l√≠neas de
                    c√≥digo.
                </p>
                <p class="mb-2">
                    Esta biblioteca proporciona acceso unificado a miles de modelos preentrenados creados por gigantes
                    tecnol√≥gicos y la comunidad, incluyendo arquitecturas famosas como <strong>BERT</strong>,
                    <strong>GPT-2</strong>, <strong>T5</strong>, <strong>Llama</strong> y <strong>Whisper</strong>. La
                    filosof√≠a es facilitar el "Transfer Learning": tomar un modelo que ya "sabe" leer y escribir gracias
                    a haber sido entrenado con todo internet, y ajustarlo finamente (fine-tuning) para una tarea
                    espec√≠fica con un dataset propio y peque√±o.
                </p>
                <p class="mb-4">
                    Adem√°s de los modelos, el ecosistema de Hugging Face incluye `Datasets` para acceder a corpus de
                    datos masivos y `Spaces` para desplegar demos interactivas. Se ha convertido en el centro de
                    colaboraci√≥n m√°s importante del mundo para la comunidad de c√≥digo abierto en IA, fomentando la
                    transparencia y la reproducibilidad de la ciencia.
                </p>

                <pre><code class="language-python">from transformers import pipeline

# 'pipeline' descarga autom√°ticamente el modelo y tokenizador necesarios
# En este caso, un modelo preentrenado para an√°lisis de sentimiento
classifier = pipeline('sentiment-analysis')

# Uso inmediato del modelo
texto = "¬°El curso de IA de iLERNA est√° superando mis expectativas!"
resultado = classifier(texto)

print(f"Texto: {texto}")
print(f"Resultado: {resultado}")</code></pre>
            </div>

            <!-- SpaCy -->
            <div class="card primary mb-6">
                <h3 class="color-primary">SpaCy</h3>
                <p class="mb-2">
                    A diferencia de otras librer√≠as acad√©micas que ofrecen docenas de algoritmos para la misma tarea,
                    <strong>SpaCy</strong> es una biblioteca de NLP "con opini√≥n": ofrece la <em>mejor</em>
                    implementaci√≥n posible para cada tarea, optimizada al m√°ximo para el rendimiento. Est√° escrita en
                    Cython (una extensi√≥n de C para Python), lo que le permite procesar grandes vol√∫menes de texto a
                    velocidades vertiginosas, convirti√©ndola en el est√°ndar para sistemas de NLP en producci√≥n
                    industrial.
                </p>
                <p class="mb-2">
                    SpaCy destaca en tareas de procesamiento ling√º√≠stico estructural: tokenizaci√≥n robusta, etiquetado
                    gramatical (POS tagging), an√°lisis de dependencias sint√°cticas y reconocimiento de entidades
                    nombradas (NER). Sus modelos preentrenados soportan m√°s de 60 idiomas y son capaces de identificar
                    personas, organizaciones, pa√≠ses y fechas en un texto con gran precisi√≥n "out-of-the-box".
                </p>
                <p class="mb-4">
                    Su dise√±o modular permite integrarla f√°cilmente en pipelines de Deep Learning m√°s grandes,
                    conect√°ndose con TensorFlow o PyTorch si es necesario. Es la herramienta ideal cuando el objetivo no
                    es investigar nuevos modelos de lenguaje, sino construir aplicaciones que necesiten entender,
                    limpiar y estructurar texto masivo de forma eficiente y fiable.
                </p>

                <pre><code class="language-python">import spacy

# Cargar el modelo peque√±o de espa√±ol (requiere: python -m spacy download es_core_news_sm)
# nlp es el objeto que procesa el texto a trav√©s del pipeline
nlp = spacy.blank("es") 

texto = "Apple est√° considerando abrir una nueva sede en Madrid por 50 millones de euros."
doc = nlp(texto)

print("Entidades detectadas:")
# Iterar sobre las entidades reconocidas (Organizaciones, Lugares, Cantidades...)
for ent in doc.ents:
    print(f" - {ent.text}: {ent.label_}")</code></pre>
            </div>

            <!-- LangChain -->
            <div class="card secondary mb-6">
                <h3 class="color-secondary">LangChain</h3>
                <p class="mb-2">
                    Con la explosi√≥n de los Grandes Modelos de Lenguaje (LLMs) como GPT-4, surgi√≥ la necesidad de
                    herramientas para orquestar su comportamiento. <strong>LangChain</strong> es un framework dise√±ado
                    para desarrollar aplicaciones impulsadas por modelos de lenguaje, permitiendo que estos no sean solo
                    generadores de texto aislados, sino componentes conscientes de su entorno y capaces de razonar.
                </p>
                <p class="mb-2">
                    Su concepto central son las "Cadenas" (Chains), que permiten unir m√∫ltiples llamadas a LLMs y otras
                    herramientas en secuencias l√≥gicas. Por ejemplo, una cadena podr√≠a: 1) buscar informaci√≥n en Google,
                    2) resumir los resultados con un LLM, y 3) traducir el resumen a otro idioma. LangChain abstrae la
                    complejidad de gestionar prompts (instrucciones), "memoria" (historial de chat) y "agentes" (modelos
                    que deciden qu√© acciones tomar).
                </p>
                <p class="mb-4">
                    Una de sus aplicaciones m√°s potentes es la Generaci√≥n Aumentada por Recuperaci√≥n (RAG). LangChain
                    facilita conectar un LLM (como ChatGPT) a tus propios datos privados (PDFs, bases de datos),
                    permitiendo que el modelo responda preguntas sobre documentaci√≥n interna de una empresa sin haber
                    sido entrenado expl√≠citamente con ella.
                </p>

                <pre><code class="language-python">from langchain.prompts import PromptTemplate

# Definici√≥n de una plantilla para estructurar la entrada al modelo
# Esto permite reutilizar la l√≥gica con diferentes variables
template = "Eres un experto en nomenclatura. Sugiere 3 nombres creativos para una empresa que fabrica: {producto}."

prompt = PromptTemplate(input_variables=["producto"], template=template)

# Al formatear el prompt se inyecta el valor de la variable
prompt_final = prompt.format(producto="calcetines inteligentes con calefacci√≥n")

print(prompt_final)
# El siguiente paso ser√≠a pasar este string a un modelo LLM conectado</code></pre>
            </div>
        </section>

        <!-- SECCI√ìN 5: VISI√ìN ARTIFICIAL Y CIENT√çFICA -->
        <section class="section">
            <h2 class="section-title">Visi√≥n y Computaci√≥n Cient√≠fica</h2>

            <!-- OpenCV -->
            <div class="card primary mb-6">
                <h3 class="color-primary">OpenCV (Open Source Computer Vision)</h3>
                <p class="mb-2">
                    Iniciada por Intel en 1999, <strong>OpenCV</strong> es la biblioteca de visi√≥n por computadora m√°s
                    grande y utilizada del mundo. Con m√°s de 2500 algoritmos optimizados, cubre todo el espectro del
                    procesamiento de im√°genes cl√°sico y moderno: desde operaciones b√°sicas como filtrado, detecci√≥n de
                    bordes y transformaciones geom√©tricas, hasta tareas complejas como detecci√≥n de rostros, seguimiento
                    de objetos en movimiento y reconstrucci√≥n 3D.
                </p>
                <p class="mb-2">
                    Su principal fortaleza es la eficiencia en tiempo real. Escrita en C/C++ optimizado (con wrappers
                    para Python, Java y MATLAB), OpenCV aprovecha las instrucciones hardware del procesador y la
                    aceleraci√≥n GPU (v√≠a CUDA) para procesar flujos de video a altas tasas de fotogramas. Esto la hace
                    indispensable en sistemas de vigilancia, rob√≥tica aut√≥noma e inspecci√≥n industrial.
                </p>
                <p class="mb-4">
                    Aunque el Deep Learning ha reemplazado a OpenCV en tareas de clasificaci√≥n compleja, la biblioteca
                    sigue siendo fundamental para el preprocesamiento de im√°genes (recortar, redimensionar, normalizar
                    color) antes de alimentar una red neuronal, y para el post-procesamiento de los resultados.
                </p>

                <h4 class="font-bold mb-2">Creaci√≥n de una imagen sint√©tica:</h4>
                <pre><code class="language-python">import cv2
import numpy as np

# Crear un lienzo negro de 512x512 p√≠xeles con 3 canales de color (RGB)
imagen = np.zeros((512, 512, 3), np.uint8)

# Dibujar elementos geom√©tricos
# L√≠nea azul diagonal (grosor 5px)
cv2.line(imagen, (0, 0), (511, 511), (255, 0, 0), 5)
# Rect√°ngulo verde
cv2.rectangle(imagen, (384, 0), (510, 128), (0, 255, 0), 3)
# C√≠rculo rojo relleno
cv2.circle(imagen, (447, 63), 63, (0, 0, 255), -1)

print("Operaciones geom√©tricas realizadas. La imagen es ahora una matriz NumPy.")</code></pre>
            </div>

            <!-- Pandas y NumPy -->
            <div class="card secondary mb-6">
                <h3 class="color-secondary">Pandas y NumPy</h3>
                <p class="mb-2">
                    Aunque t√©cnicamente no son "librer√≠as de IA", <strong>NumPy</strong> y <strong>Pandas</strong>
                    constituyen la base sobre la que se asienta todo el ecosistema de Data Science en Python.
                    <strong>NumPy</strong> proporciona el objeto `ndarray`, una estructura de datos ultra-eficiente para
                    el c√°lculo num√©rico vectorial y matricial. Bibliotecas como TensorFlow o PyTorch operan internamente
                    con tensores que son, en esencia, generalizaciones de los arrays de NumPy.
                </p>
                <p class="mb-2">
                    <strong>Pandas</strong>, construida sobre NumPy, proporciona el `DataFrame`, una estructura tabular
                    similar a una hoja de c√°lculo o una tabla SQL, pero con esteroides program√°ticos. Es la herramienta
                    est√°ndar <em>de facto</em> para la ingesti√≥n, limpieza, filtrado, transformaci√≥n y an√°lisis
                    exploratorio de datos (EDA). Antes de que cualquier dato llegue a un modelo de IA, casi con
                    seguridad ha pasado por un DataFrame de Pandas.
                </p>
                <p class="mb-4">
                    La sinergia entre ambas es total. Mientras NumPy maneja la matem√°tica de bajo nivel (√°lgebra lineal,
                    transformadas de Fourier, generaci√≥n de n√∫meros aleatorios), Pandas ofrece las herramientas de alto
                    nivel para manejar datos del mundo real (series temporales, datos faltantes, uni√≥n de tablas,
                    lectura de CSV/Excel). Sin el dominio de estas dos librer√≠as, el modelado eficaz en IA es imposible.
                </p>

                <pre><code class="language-python">import pandas as pd
import numpy as np

# Crear un DataFrame con datos sint√©ticos
# 5 filas, 3 columnas (A, B, C) con enteros aleatorios entre 0 y 100
df = pd.DataFrame(np.random.randint(0, 100, size=(5, 3)), columns=['A', 'B', 'C'])

# Operaciones de manipulaci√≥n de datos
df['D'] = df['A'] + df['B']  # Nueva columna calculada
filtro = df[df['C'] > 50]    # Filtrar filas donde C es mayor que 50

print("DataFrame Original:")
print(df)
print("\nEstad√≠sticas descriptivas:")
print(df.describe())</code></pre>
            </div>
        </section>

        <section class="section">
            <div class="highlight-box primary text-center">
                <h3 class="color-white">Conclusi√≥n</h3>
                <p>
                    El panorama de herramientas de IA es vasto y din√°mico. La elecci√≥n del framework depende
                    fundamentalmente del problema a resolver: <strong>PyTorch</strong> domina la investigaci√≥n flexible,
                    <strong>TensorFlow</strong> brilla en despliegues productivos robustos, y
                    <strong>Scikit-learn</strong> sigue siendo el rey insustituible para datos tabulares. Dominar estas
                    herramientas es dominar el lenguaje en el que se escribe el futuro de la tecnolog√≠a.
                </p>
            </div>
        </section>
    </main>

    <footer>
        <h3>iLERNA</h3>
        <p class="footer-course">Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</p>
        <a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a>
        <div class="penguin">
            <span>üêß</span>
        </div>
    </footer>

    <script src="../js/lecciones.js"></script>
</body>

</html>