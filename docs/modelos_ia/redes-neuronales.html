<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Introducci√≥n a las Redes Neuronales Artificiales: funcionamiento, algoritmos de optimizaci√≥n y aplicaciones pr√°cticas.">
    <meta name="keywords"
        content="Redes Neuronales, Deep Learning, Backpropagation, Gradient Descent, SGD, Adam, Machine Learning">
    <meta name="author" content="Bjlanza">
    <meta name="organization" content="ILERNA">
    <meta property="og:title" content="Redes Neuronales Artificiales | iLERNA">
    <meta property="og:description"
        content="Aprende c√≥mo funcionan las redes neuronales artificiales, sus algoritmos de optimizaci√≥n y sus aplicaciones en el mundo real.">
    <meta property="og:type" content="article">
    <title>Redes Neuronales Artificiales | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&display=swap" rel="stylesheet">
</head>

<body>
    <div class="container">
        <header>
            <div class="header-container">
                <div class="logo-container">
                    <a href="../index.html">
                        <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                    </a>
                    <div class="logo-text">
                        iLERNA
                        <span>Curso de Especializaci√≥n en IA y Big Data</span>
                    </div>
                </div>
                <div class="breadcrumb">
                    <a href="../index.html">Inicio</a> ‚Ä∫
                    <a href="index.html">Modelos de IA</a> ‚Ä∫
                    <span>Redes Neuronales</span>
                </div>
            </div>
            <h1 class="text-center">Redes Neuronales Artificiales</h1>
            <p class="subtitle text-center">Modelos inspirados en el cerebro humano para el aprendizaje autom√°tico</p>
        </header>

        <main>
            <!-- SECCI√ìN 1: INTRODUCCI√ìN -->
            <section class="section">
                <h2 class="section-title">¬øQu√© son las Redes Neuronales Artificiales?</h2>
                <p>
                    Las <strong>redes neuronales artificiales (RNA)</strong> son modelos matem√°ticos formados por capas
                    de unidades llamadas <strong>neuronas artificiales</strong>. Cada una de estas neuronas transforma
                    sus entradas mediante una <strong>funci√≥n de activaci√≥n</strong> y transmite la salida a otras
                    neuronas.
                </p>
                <div class="highlight-box" style="border-left: 5px solid #49B9CE;">
                    <p style="font-size: 1.1rem;">
                        Destacan por su capacidad para <strong>aproximar funciones complejas</strong> y detectar
                        <strong>patrones no lineales</strong>, aunque su interpretabilidad es limitada (a menudo se
                        consideran "cajas negras").
                    </p>
                </div>
            </section>

            <!-- SECCI√ìN 2: FUNCIONAMIENTO -->
            <section class="section">
                <h2 class="section-title">‚öôÔ∏è Funcionamiento B√°sico</h2>
                <p style="margin-bottom: 2rem;">
                    El aprendizaje en una red neuronal se basa principalmente en dos fases fundamentales:
                </p>

                <div class="grid-features">
                    <!-- Propagaci√≥n -->
                    <div class="feature-card">
                        <h3>‚û°Ô∏è Propagaci√≥n (Forward Propagation)</h3>
                        <p>
                            Las entradas se combinan y avanzan a trav√©s de las capas de la red (capa de entrada, capas
                            ocultas, capa de salida) hasta generar una predicci√≥n final.
                        </p>
                        <div class="highlight-box" style="background-color: #E8F7FA; border-left: 3px solid #49B9CE;">
                            <p style="font-size: 0.95rem;">
                                <strong>Proceso:</strong> Cada neurona recibe se√±ales de entrada, las multiplica por
                                pesos espec√≠ficos, suma los resultados y aplica una funci√≥n de activaci√≥n para producir
                                su salida.
                            </p>
                        </div>
                    </div>

                    <!-- Retro-propagaci√≥n -->
                    <div class="feature-card secondary">
                        <h3>‚¨ÖÔ∏è Retro-propagaci√≥n (Backpropagation)</h3>
                        <p>
                            El sistema calcula el error entre la salida real y la esperada, y ajusta los
                            <strong>pesos de conexi√≥n</strong> hacia atr√°s para minimizar este error en futuras
                            iteraciones.
                        </p>
                        <div class="highlight-box" style="background-color: #F3F0F7; border-left: 3px solid #8A7AAF;">
                            <p style="font-size: 0.95rem;">
                                <strong>Objetivo:</strong> Utilizar el gradiente del error para actualizar los pesos de
                                forma que la red mejore progresivamente sus predicciones.
                            </p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 3: ARQUITECTURA -->
            <section class="section">
                <h2 class="section-title">üèóÔ∏è Arquitectura de una Red Neuronal</h2>
                <div class="comparison-grid">
                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">Capa de Entrada</h4>
                        <p>
                            Recibe los datos originales (p√≠xeles de una imagen, palabras de un texto, valores
                            num√©ricos, etc.).
                        </p>
                        <ul style="font-size: 0.9rem; margin-top: 0.5rem;">
                            <li>N√∫mero de neuronas = n√∫mero de caracter√≠sticas de entrada</li>
                            <li>No realiza procesamiento, solo distribuye datos</li>
                        </ul>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #8A7AAF;">Capas Ocultas</h4>
                        <p>
                            Transforman progresivamente las entradas mediante funciones de activaci√≥n no lineales.
                        </p>
                        <ul style="font-size: 0.9rem; margin-top: 0.5rem;">
                            <li>Pueden ser una o muchas capas</li>
                            <li>Extraen caracter√≠sticas de diferentes niveles de abstracci√≥n</li>
                            <li>Aqu√≠ reside el "aprendizaje profundo"</li>
                        </ul>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #DD6B20;">Capa de Salida</h4>
                        <p>
                            Produce la predicci√≥n final: clasificaci√≥n, regresi√≥n, probabilidades, etc.
                        </p>
                        <ul style="font-size: 0.9rem; margin-top: 0.5rem;">
                            <li>Clasificaci√≥n binaria: 1 neurona (sigmoid)</li>
                            <li>Clasificaci√≥n multi-clase: N neuronas (softmax)</li>
                            <li>Regresi√≥n: 1 neurona (lineal)</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 4: EJEMPLOS DE APLICACI√ìN -->
            <section class="section">
                <h2 class="section-title">üöÄ Ejemplos de Aplicaci√≥n</h2>
                <div class="grid-features">
                    <div class="feature-card">
                        <h3>üìù Reconocimiento de Patrones</h3>
                        <p>
                            Reconocimiento de escritura manual (OCR), reconocimiento de voz (dictado en m√≥viles,
                            asistentes virtuales), identificaci√≥n de rostros en fotograf√≠as.
                        </p>
                        <div class="code-example" style="margin-top: 1rem; font-size: 0.85rem;">
                            <strong>Ejemplo:</strong> Google Lens, sistemas de dictado de Apple/Android, Face ID
                        </div>
                    </div>

                    <div class="feature-card secondary">
                        <h3>üìà Predicci√≥n y An√°lisis</h3>
                        <p>
                            Predicci√≥n de demanda energ√©tica, forecasting de series temporales, predicci√≥n de precios
                            burs√°tiles, diagn√≥stico m√©dico asistido.
                        </p>
                        <div class="code-example" style="margin-top: 1rem; font-size: 0.85rem;">
                            <strong>Ejemplo:</strong> Predicci√≥n de consumo el√©ctrico, pron√≥stico meteorol√≥gico,
                            detecci√≥n de anomal√≠as card√≠acas
                        </div>
                    </div>

                    <div class="feature-card">
                        <h3>üöó Visi√≥n Artificial</h3>
                        <p>
                            Sistemas de visi√≥n artificial en veh√≠culos aut√≥nomos para detectar obst√°culos, se√±ales de
                            tr√°fico, peatones y otros veh√≠culos en tiempo real.
                        </p>
                        <div class="code-example" style="margin-top: 1rem; font-size: 0.85rem;">
                            <strong>Ejemplo:</strong> Tesla Autopilot, Waymo, detecci√≥n de objetos con YOLO
                        </div>
                    </div>

                    <div class="feature-card secondary">
                        <h3>üí¨ Procesamiento del Lenguaje</h3>
                        <p>
                            Traducci√≥n autom√°tica, generaci√≥n de texto, an√°lisis de sentimientos, chatbots
                            conversacionales y sistemas de preguntas y respuestas.
                        </p>
                        <div class="code-example" style="margin-top: 1rem; font-size: 0.85rem;">
                            <strong>Ejemplo:</strong> ChatGPT, Google Translate, an√°lisis de rese√±as de productos
                        </div>
                    </div>

                    <div class="feature-card">
                        <h3>üé® Generaci√≥n de Contenido</h3>
                        <p>
                            Generaci√≥n de im√°genes (DALL-E, Midjourney), m√∫sica, deepfakes, s√≠ntesis de voz y
                            composici√≥n autom√°tica de contenido creativo.
                        </p>
                        <div class="code-example" style="margin-top: 1rem; font-size: 0.85rem;">
                            <strong>Ejemplo:</strong> Stable Diffusion, Midjourney, DALL-E 3, s√≠ntesis de voz ElevenLabs
                        </div>
                    </div>

                    <div class="feature-card secondary">
                        <h3>üéÆ Juegos y Simulaci√≥n</h3>
                        <p>
                            Agentes que aprenden a jugar videojuegos (Atari, Dota 2, StarCraft), sistemas de
                            recomendaci√≥n y optimizaci√≥n de estrategias.
                        </p>
                        <div class="code-example" style="margin-top: 1rem; font-size: 0.85rem;">
                            <strong>Ejemplo:</strong> AlphaGo, OpenAI Five, DeepMind AlphaStar
                        </div>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 5: ALGORITMOS DE OPTIMIZACI√ìN -->
            <section class="section">
                <h2 class="section-title">‚ö° Algoritmos de Optimizaci√≥n</h2>
                <p style="margin-bottom: 2rem;">
                    Para minimizar el error durante el entrenamiento, se utilizan algoritmos que ajustan los pesos de
                    la red. Los m√°s comunes son:
                </p>

                <div class="grid-features">
                    <!-- Gradient Descent -->
                    <div class="feature-card">
                        <h3>üìâ Gradient Descent (Descenso del Gradiente)</h3>
                        <p>
                            Calcula el gradiente de la funci√≥n de error utilizando <strong>todo el conjunto de
                                datos</strong> para actualizar los pesos. Es preciso pero lento en grandes datasets.
                        </p>
                        <div class="highlight-box" style="background-color: #E8F7FA; border-left: 3px solid #49B9CE; margin-top: 1rem;">
                            <p style="font-size: 0.9rem;">
                                <strong>Ventajas:</strong> Convergencia suave y estable<br>
                                <strong>Desventajas:</strong> Muy lento con millones de datos, puede quedar atrapado en
                                m√≠nimos locales
                            </p>
                        </div>
                        <div class="code-example" style="margin-top: 1rem;">
                            <strong>F√≥rmula:</strong> Œ∏ = Œ∏ - Œ± ‚àáJ(Œ∏)<br>
                            <span style="font-size: 0.85rem;">Donde Œ± es la tasa de aprendizaje y ‚àáJ(Œ∏) es el
                                gradiente</span>
                        </div>
                    </div>

                    <!-- SGD -->
                    <div class="feature-card secondary">
                        <h3>üé≤ SGD (Stochastic Gradient Descent)</h3>
                        <p>
                            Actualiza los pesos usando <strong>una sola muestra</strong> (o un peque√±o lote/batch) a la
                            vez. Es mucho m√°s r√°pido y ayuda a escapar de m√≠nimos locales, aunque introduce m√°s
                            variabilidad.
                        </p>
                        <div class="highlight-box" style="background-color: #F3F0F7; border-left: 3px solid #8A7AAF; margin-top: 1rem;">
                            <p style="font-size: 0.9rem;">
                                <strong>Ventajas:</strong> R√°pido, puede escapar de m√≠nimos locales, ideal para datasets
                                grandes<br>
                                <strong>Desventajas:</strong> Convergencia ruidosa, puede oscilar cerca del m√≠nimo
                            </p>
                        </div>
                        <div class="code-example" style="margin-top: 1rem;">
                            <strong>Variante popular:</strong> Mini-batch SGD<br>
                            <span style="font-size: 0.85rem;">Usa lotes peque√±os (32-256 muestras) para balancear
                                velocidad y estabilidad</span>
                        </div>
                    </div>

                    <!-- Adam -->
                    <div class="feature-card">
                        <h3>üöÄ Adam (Adaptive Moment Estimation)</h3>
                        <p>
                            Combina las ventajas de otros m√©todos. Adapta la tasa de aprendizaje para cada peso de forma
                            individual, manteniendo promedios m√≥viles de gradientes pasados y sus cuadrados.
                        </p>
                        <div class="highlight-box" style="background-color: #FFF3E0; border-left: 3px solid #DD6B20; margin-top: 1rem;">
                            <p style="font-size: 0.9rem;">
                                <strong>Por qu√© es popular:</strong> Funciona bien "out of the box" sin necesidad de
                                ajustar muchos hiperpar√°metros. Es el optimizador por defecto en muchos frameworks.<br>
                                <strong>Ventajas:</strong> Converge r√°pido, tasa de aprendizaje adaptativa, maneja ruido
                                eficientemente
                            </p>
                        </div>
                        <div class="code-example" style="margin-top: 1rem;">
                            <strong>Hiperpar√°metros t√≠picos:</strong><br>
                            <span style="font-size: 0.85rem;">
                                Œ≤‚ÇÅ = 0.9 (momento de primer orden)<br>
                                Œ≤‚ÇÇ = 0.999 (momento de segundo orden)<br>
                                Œµ = 10‚Åª‚Å∏ (estabilidad num√©rica)
                            </span>
                        </div>
                    </div>

                    <!-- RMSprop -->
                    <div class="feature-card secondary">
                        <h3>üìä RMSprop (Root Mean Square Propagation)</h3>
                        <p>
                            Ajusta la tasa de aprendizaje para cada par√°metro dividiendo el gradiente por una media
                            m√≥vil de sus magnitudes recientes. Especialmente √∫til en redes recurrentes.
                        </p>
                        <div class="highlight-box" style="background-color: #E8F5E9; border-left: 3px solid #4CAF50; margin-top: 1rem;">
                            <p style="font-size: 0.9rem;">
                                <strong>Ventajas:</strong> Bueno para problemas no estacionarios, funciona bien con RNNs<br>
                                <strong>Uso com√∫n:</strong> Recomendado por Geoffrey Hinton para redes recurrentes
                            </p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 6: FUNCIONES DE ACTIVACI√ìN -->
            <section class="section">
                <h2 class="section-title">‚ö° Funciones de Activaci√≥n</h2>
                <p style="margin-bottom: 2rem;">
                    Las funciones de activaci√≥n introducen <strong>no linealidad</strong> en la red, permiti√©ndole
                    aprender patrones complejos. Sin ellas, una red neuronal profunda ser√≠a equivalente a una regresi√≥n
                    lineal simple.
                </p>

                <div class="comparison-grid">
                    <div class="comparison-card">
                        <h4 style="color: #49B9CE;">ReLU (Rectified Linear Unit)</h4>
                        <p style="font-size: 0.95rem; margin-bottom: 0.5rem;">
                            f(x) = max(0, x)
                        </p>
                        <p>La m√°s popular actualmente. R√°pida de calcular y evita el problema de gradientes que
                            desaparecen.</p>
                        <ul style="font-size: 0.85rem; margin-top: 0.5rem;">
                            <li>‚úÖ R√°pida computacionalmente</li>
                            <li>‚úÖ Evita gradientes desvanecientes</li>
                            <li>‚ö†Ô∏è Puede sufrir "neuronas muertas" (siempre devuelven 0)</li>
                        </ul>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #8A7AAF;">Sigmoid</h4>
                        <p style="font-size: 0.95rem; margin-bottom: 0.5rem;">
                            f(x) = 1 / (1 + e‚ÅªÀ£)
                        </p>
                        <p>Mapea valores a un rango (0, 1). √ötil en la capa de salida para clasificaci√≥n binaria.</p>
                        <ul style="font-size: 0.85rem; margin-top: 0.5rem;">
                            <li>‚úÖ Salida interpretable como probabilidad</li>
                            <li>‚ö†Ô∏è Sufre de gradientes desvanecientes</li>
                            <li>‚ö†Ô∏è No es zero-centered</li>
                        </ul>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #DD6B20;">Tanh (Tangente Hiperb√≥lica)</h4>
                        <p style="font-size: 0.95rem; margin-bottom: 0.5rem;">
                            f(x) = (eÀ£ - e‚ÅªÀ£) / (eÀ£ + e‚ÅªÀ£)
                        </p>
                        <p>Similar a sigmoid pero mapea a (-1, 1). Zero-centered, lo que facilita el entrenamiento.</p>
                        <ul style="font-size: 0.85rem; margin-top: 0.5rem;">
                            <li>‚úÖ Zero-centered</li>
                            <li>‚úÖ Mejor que sigmoid en capas ocultas</li>
                            <li>‚ö†Ô∏è Tambi√©n sufre gradientes desvanecientes</li>
                        </ul>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #4CAF50;">Softmax</h4>
                        <p style="font-size: 0.95rem; margin-bottom: 0.5rem;">
                            f(x·µ¢) = eÀ£‚Å± / Œ£‚±º eÀ£ ≤
                        </p>
                        <p>Convierte un vector de valores en una distribuci√≥n de probabilidad. Ideal para clasificaci√≥n
                            multi-clase.</p>
                        <ul style="font-size: 0.85rem; margin-top: 0.5rem;">
                            <li>‚úÖ Salidas suman 1 (probabilidades)</li>
                            <li>‚úÖ Est√°ndar para clasificaci√≥n multi-clase</li>
                            <li>Se usa solo en la capa de salida</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 7: DESAF√çOS Y CONSIDERACIONES -->
            <section class="section">
                <h2 class="section-title">‚ö†Ô∏è Desaf√≠os y Consideraciones</h2>
                <div class="warning-box">
                    <h3 style="margin-top: 0;">Problemas Comunes en el Entrenamiento</h3>
                    <div class="grid-features" style="margin-top: 1rem;">
                        <div class="feature-card">
                            <h4 style="color: #D32F2F;">Sobreajuste (Overfitting)</h4>
                            <p>
                                La red memoriza los datos de entrenamiento en lugar de aprender patrones generales.
                                Funciona muy bien en entrenamiento pero mal en datos nuevos.
                            </p>
                            <p style="font-size: 0.9rem; margin-top: 0.5rem;">
                                <strong>Soluciones:</strong> Dropout, regularizaci√≥n L1/L2, m√°s datos de entrenamiento,
                                data augmentation
                            </p>
                        </div>

                        <div class="feature-card">
                            <h4 style="color: #D32F2F;">Gradientes Desvanecientes</h4>
                            <p>
                                En redes profundas, los gradientes se vuelven extremadamente peque√±os al retropropagarse,
                                haciendo que las capas iniciales no aprendan.
                            </p>
                            <p style="font-size: 0.9rem; margin-top: 0.5rem;">
                                <strong>Soluciones:</strong> Usar ReLU, inicializaci√≥n adecuada (Xavier/He), Batch
                                Normalization, arquitecturas residuales (ResNet)
                            </p>
                        </div>

                        <div class="feature-card">
                            <h4 style="color: #D32F2F;">Gradientes Explosivos</h4>
                            <p>
                                Los gradientes crecen exponencialmente, causando que los pesos se actualicen de forma
                                descontrolada (valores NaN o Inf).
                            </p>
                            <p style="font-size: 0.9rem; margin-top: 0.5rem;">
                                <strong>Soluciones:</strong> Gradient clipping, tasas de aprendizaje m√°s peque√±as,
                                inicializaci√≥n cuidadosa
                            </p>
                        </div>

                        <div class="feature-card">
                            <h4 style="color: #D32F2F;">Caja Negra</h4>
                            <p>
                                Las redes neuronales son dif√≠ciles de interpretar. No es claro por qu√© toman ciertas
                                decisiones, lo que es problem√°tico en dominios cr√≠ticos (medicina, finanzas).
                            </p>
                            <p style="font-size: 0.9rem; margin-top: 0.5rem;">
                                <strong>Soluciones:</strong> XAI (Explainable AI), LIME, SHAP, visualizaci√≥n de
                                activaciones, attention maps
                            </p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 8: FRAMEWORKS Y HERRAMIENTAS -->
            <section class="section">
                <h2 class="section-title">üõ†Ô∏è Frameworks y Herramientas Populares</h2>
                <div class="comparison-grid">
                    <div class="comparison-card">
                        <h4 style="color: #FF6F00;">TensorFlow / Keras</h4>
                        <p style="font-size: 0.9rem;">
                            Framework de c√≥digo abierto desarrollado por Google. Keras ofrece una API de alto nivel
                            sobre TensorFlow.
                        </p>
                        <ul style="font-size: 0.85rem; margin-top: 0.5rem;">
                            <li>Ecosistema completo: TensorFlow Lite (m√≥viles), TensorFlow.js (web)</li>
                            <li>Ideal para producci√≥n a gran escala</li>
                            <li>Excelente documentaci√≥n y comunidad</li>
                        </ul>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #EE4C2C;">PyTorch</h4>
                        <p style="font-size: 0.9rem;">
                            Framework desarrollado por Meta (Facebook). Muy popular en investigaci√≥n por su flexibilidad
                            y estilo "pyth√≥nico".
                        </p>
                        <ul style="font-size: 0.85rem; margin-top: 0.5rem;">
                            <li>Grafos de computaci√≥n din√°micos</li>
                            <li>Depuraci√≥n m√°s intuitiva</li>
                            <li>Preferido en entornos acad√©micos</li>
                        </ul>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #76B900;">scikit-learn</h4>
                        <p style="font-size: 0.9rem;">
                            Biblioteca de machine learning cl√°sico, incluye implementaciones de redes neuronales simples
                            (MLPClassifier).
                        </p>
                        <ul style="font-size: 0.85rem; margin-top: 0.5rem;">
                            <li>Ideal para problemas peque√±os/medianos</li>
                            <li>Interfaz consistente y simple</li>
                            <li>No optimizado para deep learning</li>
                        </ul>
                    </div>

                    <div class="comparison-card">
                        <h4 style="color: #00C4CC;">JAX</h4>
                        <p style="font-size: 0.9rem;">
                            Framework de Google para computaci√≥n num√©rica de alto rendimiento, usado en investigaci√≥n
                            avanzada de ML.
                        </p>
                        <ul style="font-size: 0.85rem; margin-top: 0.5rem;">
                            <li>Diferenciaci√≥n autom√°tica avanzada</li>
                            <li>Compilaci√≥n XLA para aceleraci√≥n</li>
                            <li>Ideal para investigaci√≥n cutting-edge</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 9: CONCLUSI√ìN -->
            <section class="section">
                <h2 class="section-title">üéØ Conclusi√≥n</h2>
                <div class="highlight-box" style="border-left: 5px solid #4CAF50;">
                    <p style="font-size: 1.05rem; line-height: 1.8;">
                        Las <strong>redes neuronales artificiales</strong> han revolucionado campos como la visi√≥n por
                        computadora, el procesamiento del lenguaje natural y los sistemas de recomendaci√≥n. Su capacidad
                        para aprender representaciones jer√°rquicas de datos las convierte en herramientas extremadamente
                        poderosas.
                    </p>
                    <p style="font-size: 1.05rem; line-height: 1.8; margin-top: 1rem;">
                        Sin embargo, requieren grandes cantidades de datos, recursos computacionales significativos y un
                        ajuste cuidadoso de hiperpar√°metros. Adem√°s, su naturaleza de "caja negra" plantea desaf√≠os
                        √©ticos y pr√°cticos en aplicaciones cr√≠ticas.
                    </p>
                    <p style="font-size: 1.05rem; line-height: 1.8; margin-top: 1rem;">
                        El futuro del deep learning pasa por redes m√°s eficientes, interpretables y que requieran menos
                        datos para entrenarse, acerc√°ndose m√°s a c√≥mo aprenden los humanos.
                    </p>
                </div>
            </section>
        </main>

        <footer>
            <div class="footer-content">
                <img src="../img/logo-ilerna.svg" alt="ILERNA" style="height: 40px; margin-bottom: 1rem;">
                <h3>Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</h3>
                <p><a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a></p>
                <p style="font-size: 0.9rem; color: #777; margin-top: 1rem;">Centro oficial de FP online y presencial.
                    Ciclos formativos de Grado Medio y Grado Superior.</p>
                <p style="font-size: 0.9rem; color: #777;">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>
            </div>
            <div class="penguin">
                <span>üêß</span>
            </div>
        </footer>
    </div>
</body>

</html>
