<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Migraci√≥n de entornos y compatibilidad en Big Data. Tipos de migraci√≥n, desaf√≠os y casos de √©xito.">
    <title>Migraci√≥n de Entornos y Compatibilidad | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&display=swap" rel="stylesheet">
</head>

<body>
    <div class="container">
        <header>
            <div class="header-container">
                <div class="logo-container">
                    <a href="../index.html">
                        <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                    </a>
                    <div class="logo-text">
                        iLERNA
                        <span>Curso de Especializaci√≥n en IA y Big Data</span>
                    </div>
                </div>
                <div class="breadcrumb">
                    <a href="../index.html">Inicio</a> ‚Ä∫
                    <a href="index.html">Big Data Aplicado</a> ‚Ä∫
                    <span>Migraci√≥n de Entornos</span>
                </div>
            </div>
        </header>

        <main>
            <!-- Hero Section -->
            <div class="hero">
                <h1>Migraci√≥n de Entornos y Compatibilidad</h1>
                <p class="subtitle">Garantizando la continuidad y la integridad en la evoluci√≥n tecnol√≥gica</p>
            </div>

            <!-- TOC -->
            <nav class="toc-container">
                <h3>üìë Contenido de la Lecci√≥n</h3>
                <ul class="toc-list">
                    <li><a href="#introduccion">1. Concepto de Migraci√≥n</a></li>
                    <li><a href="#tipos">2. Tipos de Migraci√≥n</a></li>
                    <li><a href="#desafios">3. Desaf√≠os Principales</a></li>
                    <li><a href="#ejemplo-practico">4. Ejemplo Pr√°ctico: Blue-Green</a></li>
                    <li><a href="#caso-estudio">5. Caso de Estudio: Sector Bancario</a></li>
                    <li><a href="#resumen">6. Tabla Resumen de Migraci√≥n</a></li>
                    <li><a href="#herramientas">7. Cat√°logo de Herramientas</a></li>
                </ul>
            </nav>

            <!-- INTRODUCCI√ìN -->
            <section class="section" id="introduccion">
                <p>
                    La migraci√≥n de datos y entornos implica trasladar informaci√≥n, configuraciones y metadatos entre
                    infraestructuras, ya sea por renovaci√≥n de hardware, actualizaci√≥n de software o adopci√≥n de la
                    nube.
                </p>
                <div class="highlight-box primary">
                    <p class="content">
                        Este proceso es una necesidad cr√≠tica en el ciclo de vida de cualquier sistema Big Data, ya que
                        las
                        tecnolog√≠as evolucionan m√°s r√°pido que la vida √∫til de los datos. El √©xito de una migraci√≥n
                        depende de
                        garantizar tres pilares: <strong>integridad</strong>, <strong>compatibilidad</strong> y
                        <strong>continuidad del servicio</strong>.
                    </p>
                </div>
            </section>

            <!-- TIPOS DE MIGRACI√ìN -->
            <section class="section" id="tipos">
                <h2 class="section-title">2. Tipos de Migraci√≥n</h2>
                <p>Dependiendo del objetivo y el destino, las migraciones se clasifican en cuatro grandes categor√≠as:
                </p>

                <div class="grid-2-cols">
                    <!-- Entre versiones -->
                    <div class="card primary">
                        <h4 class="color-primary">üîÑ Entre versiones (Upgrade)</h4>
                        <p><strong>Descripci√≥n:</strong> Actualizaci√≥n a una nueva versi√≥n del mismo sistema (ej. Hadoop
                            2.x a 3.x).</p>
                        <p><strong>Requerimientos:</strong> Compatibilidad con formatos de bloques, metadatos y APIs;
                            validaci√≥n de jobs existentes.</p>
                        <div class="scenario-box white" style="margin-top: 1rem; padding: 1rem;">
                            <small><strong>Ejemplo:</strong> Una empresa migra de Hadoop 2.7 a 3.3, ejecutando pruebas
                                paralelas para validar jobs Spark antes de desactivar el cl√∫ster antiguo.</small>
                        </div>
                    </div>

                    <!-- Heterog√©neas -->
                    <div class="card secondary">
                        <h4 class="color-secondary">üîå Plataformas Heterog√©neas</h4>
                        <p><strong>Descripci√≥n:</strong> Transferencia de datos entre sistemas diferentes (ej. HDFS a
                            Amazon S3).</p>
                        <p><strong>Herramientas:</strong> Apache NiFi, Talend, S3DistCp.</p>
                        <div class="scenario-box white" style="margin-top: 1rem; padding: 1rem;">
                            <small><strong>Ejemplo:</strong> Una organizaci√≥n transfiere datos de ventas desde HDFS a
                                Azure Data Lake, utilizando NiFi para transformar formatos en el proceso.</small>
                        </div>
                    </div>

                    <!-- Cloud/H√≠bridos -->
                    <div class="card secondary">
                        <h4 class="color-secondary">‚òÅÔ∏è Entornos Cloud o H√≠bridos</h4>
                        <p><strong>Descripci√≥n:</strong> Traslado de datos on-premise a la nube o entre proveedores
                            cloud.</p>
                        <p><strong>Herramientas:</strong> AWS DataSync, Azure Data Factory, Google Transfer Service.</p>
                        <div class="scenario-box white" style="margin-top: 1rem; padding: 1rem;">
                            <small><strong>Ejemplo:</strong> Una empresa migra 1 PB de datos a AWS S3 utilizando
                                DataSync, con cifrado AES-256 y verificaci√≥n de integridad.</small>
                        </div>
                    </div>

                    <!-- Workloads -->
                    <div class="card primary">
                        <h4 class="color-primary">‚öôÔ∏è Migraci√≥n de Workloads</h4>
                        <p><strong>Descripci√≥n:</strong> Transferencia de datos, scripts y dependencias anal√≠ticas.</p>
                        <p><strong>Requerimientos:</strong> Validaci√≥n de compatibilidad con frameworks (Spark, Hive,
                            Flink).</p>
                        <div class="scenario-box white" style="margin-top: 1rem; padding: 1rem;">
                            <small><strong>Ejemplo:</strong> Migrar un pipeline Airflow con jobs Spark desde un cl√∫ster
                                local a Google Cloud Dataproc.</small>
                        </div>
                    </div>
                </div>
            </section>

            <!-- DESAF√çOS -->
            <section class="section" id="desafios">
                <h2 class="section-title">3. Desaf√≠os Principales</h2>
                <p>Las migraciones a gran escala enfrentan retos t√©cnicos que pueden comprometer la calidad del dato:
                </p>

                <div class="grid-features">
                    <div class="feature-card">
                        <h4>üß© Formatos y Codificaci√≥n</h4>
                        <p>Diferencias en estructuras de datos o codificaciones entre sistemas (ej. Avro vs Parquet, o
                            cambios en el esquema de compresi√≥n).</p>
                    </div>
                    <div class="feature-card">
                        <h4>üö´ Incompatibilidades</h4>
                        <p>Versiones de software o APIs no compatibles que pueden romper los procesos anal√≠ticos
                            existentes.</p>
                    </div>
                    <div class="feature-card">
                        <h4>üè∑Ô∏è P√©rdida de Metadatos</h4>
                        <p>Riesgo de perder informaci√≥n cr√≠tica sobre linaje, permisos (ACLs) o etiquetas de gobernanza
                            durante el traslado.</p>
                    </div>
                    <div class="feature-card">
                        <h4>‚öñÔ∏è Consistencia</h4>
                        <p>Garantizar que los datos en el destino son id√©nticos a los del origen mediante verificaciones
                            bit a bit.</p>
                    </div>
                </div>

                <div class="highlight-box warning">
                    <p><strong>Atenci√≥n:</strong> Ignorar la validaci√≥n de la consistencia puede llevar a "datos
                        fantasma" o "corrupci√≥n silenciosa", donde el sistema no reporta errores pero los informes
                        financieros o anal√≠ticos devuelven valores incorrectos.</p>
                </div>
            </section>

            <!-- EJEMPLO PR√ÅCTICO -->
            <section class="section" id="ejemplo-practico">
                <h2 class="section-title">4. Ejemplo Pr√°ctico: Amazon EMR</h2>
                <div class="scenario-box primary">
                    <p>Una empresa migra un cl√∫ster Hadoop on-premise a <strong>Amazon EMR</strong> utilizando una
                        estrategia <strong>blue-green deployment</strong>.</p>
                    <ul class="list-disc-padded">
                        <li><strong>Proceso:</strong> Ambos entornos permanecen activos, ejecutando jobs Spark en
                            paralelo para validar resultados.</li>
                        <li><strong>M√©trica:</strong> La migraci√≥n de 500 TB se completa en 15 d√≠as.</li>
                        <li><strong>Resultado:</strong> Cero interrupciones de servicio y validaci√≥n del 100% de los
                            pipelines cr√≠ticos.</li>
                    </ul>
                </div>
            </section>

            <!-- CASO DE ESTUDIO -->
            <section class="section" id="caso-estudio">
                <h2 class="section-title">5. Caso de Estudio: Sector Bancario</h2>
                <div class="expert-quote">
                    <p class="quote-text">
                        "Un banco migr√≥ 1 PB de datos financieros desde un cl√∫ster HDFS on-premise a <strong>Azure Data
                            Lake Storage Gen2</strong>.
                        Se utiliz√≥ <strong>Azure Data Factory</strong> para transferencias incrementales, validando cada
                        lote con checksums SHA-256."
                    </p>
                    <div class="grid-2-cols" style="margin-top: 1rem; border-top: 1px solid #ddd; padding-top: 1rem;">
                        <div>
                            <p><strong>Gobernanza:</strong> Los metadatos se gestionaron con <strong>Azure
                                    Purview</strong>, asegurando la trazabilidad completa.</p>
                        </div>
                        <div>
                            <p><strong>Impacto:</strong> El proceso, ejecutado en 20 d√≠as, cumpli√≥ con normativas GDPR y
                                mejor√≥ el rendimiento anal√≠tico en un 40%.</p>
                        </div>
                    </div>
                    <span class="quote-author">‚Äî Arquitecto de Infraestructura Financiera</span>
                </div>
            </section>

            <!-- TABLA RESUMEN -->
            <section class="section" id="resumen">
                <h2 class="section-title">6. Resumen de Estrategias de Migraci√≥n</h2>
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Tipo de Migraci√≥n</th>
                                <th>Descripci√≥n Operativa</th>
                                <th>Herramientas T√≠picas</th>
                                <th>Ejemplo Real</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>üîÑ Entre versiones</strong></td>
                                <td>Actualizaci√≥n de software (Hadoop 2.x ‚Üí 3.x) sin p√©rdida de historial.</td>
                                <td>Rolling upgrade, DistCp.</td>
                                <td>Validaci√≥n paralela de jobs Spark antes del corte.</td>
                            </tr>
                            <tr>
                                <td><strong>üîå Entre plataformas</strong></td>
                                <td>Movimiento de datos entre sistemas distintos (HDFS ‚Üí S3/Azure).</td>
                                <td>NiFi, Talend, S3DistCp.</td>
                                <td>Migraci√≥n de almac√©n de ventas a Azure Data Lake.</td>
                            </tr>
                            <tr>
                                <td><strong>‚òÅÔ∏è A la nube (Lift & Shift)</strong></td>
                                <td>Traslado masivo de on-premise a infraestructura Cloud.</td>
                                <td>AWS DataSync, Azure Factory.</td>
                                <td>Transferencia de 1 PB con cifrado AES-256 a AWS.</td>
                            </tr>
                            <tr>
                                <td><strong>‚öôÔ∏è De workloads</strong></td>
                                <td>Migraci√≥n de la l√≥gica: scripts, pipelines y dependencias.</td>
                                <td>Airflow, Dataproc.</td>
                                <td>Despliegue de pipelines Spark en Google Dataproc.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <!-- HERRAMIENTAS -->
            <section class="section" id="herramientas">
                <h2 class="section-title">7. Cat√°logo de Herramientas de Migraci√≥n</h2>
                <p>Para ejecutar con √©xito las estrategias anteriores, los ingenieros de Big Data se apoyan en un
                    ecosistema de herramientas especializadas seg√∫n el origen y destino del dato:</p>

                <div class="grid-2-cols">
                    <div class="feature-card">
                        <h4 class="color-primary">üì¶ Herramientas Nativas</h4>
                        <ul class="list-disc-padded">
                            <li><strong>DistCp:</strong> Copia distribuida para Hadoop que aprovecha MapReduce para
                                paralelizar el movimiento entre cl√∫steres HDFS.</li>
                            <li><strong>S3DistCp:</strong> Variante optimizada para mover datos entre HDFS y Amazon S3,
                                permitiendo agregaci√≥n de archivos peque√±os.</li>
                        </ul>
                    </div>
                    <div class="feature-card">
                        <h4 class="color-secondary">üîÑ Orquestadores de Datos</h4>
                        <ul class="list-disc-padded">
                            <li><strong>Apache NiFi:</strong> El "cuchillo suizo" de la migraci√≥n; permite procesar y
                                transformar datos en tiempo real mientras se mueven entre sistemas heterog√©neos.</li>
                            <li><strong>Apache Airflow:</strong> Vital para la migraci√≥n de workloads, gestionando las
                                dependencias y el reintento de jobs durante la transici√≥n.</li>
                        </ul>
                    </div>
                    <div class="feature-card">
                        <h4 class="color-secondary">‚òÅÔ∏è Servicios Cloud de Transferencia</h4>
                        <ul class="list-disc-padded">
                            <li><strong>AWS DataSync:</strong> Servicio online para simplificar y acelerar el movimiento
                                de datos entre sistemas on-premise y servicios AWS.</li>
                            <li><strong>Azure Data Factory:</strong> Servicio de integraci√≥n de datos para crear,
                                programar y orquestar flujos de trabajo ETL/ELT a escala.</li>
                        </ul>
                    </div>
                    <div class="feature-card">
                        <h4 class="color-primary">üèóÔ∏è Plataformas Gestionadas</h4>
                        <ul class="list-disc-padded">
                            <li><strong>Google Colab / Dataproc:</strong> Facilita la migraci√≥n de workloads de Spark y
                                Hadoop al ecosistema de Google Cloud con m√≠nima reconfiguraci√≥n.</li>
                            <li><strong>Talend:</strong> Soluci√≥n de grado empresarial para integraci√≥n de metadatos
                                complejos y limpieza de datos durante la migraci√≥n.</li>
                        </ul>
                    </div>
                </div>
            </section>

        </main>

        <footer>
            <h3>iLERNA</h3>
            <p class="footer-course">Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</p>
            <a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a>
            <p class="footer-info">Centro oficial de FP online y presencial. Ciclos formativos de Grado Medio y Grado
                Superior.</p>
            <p class="footer-info">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>
            <div class="penguin">
                <span>üêß</span>
            </div>
        </footer>
    </div>

    <script src="../js/lecciones.js"></script>
</body>

</html>