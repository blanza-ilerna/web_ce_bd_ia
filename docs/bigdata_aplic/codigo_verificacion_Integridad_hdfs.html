<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Aprende a interactuar con HDFS desde Python utilizando la librer√≠a Snakebite. Verificaci√≥n de integridad de bloques y gesti√≥n de archivos sin dependencias de Java.">
    <meta name="author" content="iLERNA">
    <title>Integridad HDFS con Snakebite | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
</head>

<body>
    <div class="container">
        <header>
            <div class="header-container">
                <div class="logo-container">
                    <a href="../index.html">
                        <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                    </a>
                    <div class="logo-text">
                        iLERNA
                        <span>Curso de Especializaci√≥n en IA y Big Data</span>
                    </div>
                </div>
                <div class="breadcrumb">
                    <a href="../index.html">Inicio</a> ‚Ä∫
                    <a href="index.html">Aplicaci√≥n de Big Data</a> ‚Ä∫
                    <span>Integridad HDFS con Snakebite</span>
                </div>
            </div>
        </header>

        <main>
            <!-- T√çTULO PRINCIPAL -->
            <div class="hero">
                <h1>Verificaci√≥n de Integridad en HDFS con Snakebite</h1>
                <p class="subtitle">Interacci√≥n eficiente con Hadoop usando Python puro</p>
            </div>

            <!-- SECCI√ìN 1: CONTEXTO HDFS -->
            <section class="section">
                <h2 class="section-title">Contexto: HDFS y Bloques</h2>
                <div class="grid-features">
                    <div class="feature-card primary">
                        <h4>üì¶ Bloques de Datos</h4>
                        <p>HDFS (Hadoop Distributed File System) divide los archivos grandes en piezas m√°s peque√±as
                            llamadas <strong>bloques</strong> (por defecto 128 MB). Estos bloques se distribuyen a
                            trav√©s de varios nodos (DataNodes) en el cl√∫ster.</p>
                    </div>
                    <div class="feature-card secondary">
                        <h4>üîÑ Replicaci√≥n</h4>
                        <p>Para garantizar la tolerancia a fallos, cada bloque se replica varias veces (por defecto 3).
                            Si un nodo falla, HDFS puede recuperar los datos de una r√©plica en otro nodo. La
                            verificaci√≥n de integridad asegura que estos bloques no est√©n corruptos.</p>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 2: LIBRER√çA SNAKEBITE -->
            <section class="section">
                <h2 class="section-title">¬øQu√© es Snakebite?</h2>
                <p>
                    <strong>Snakebite</strong> es una librer√≠a cliente de HDFS escrita totalmente en Python (Pure Python
                    Client), creada originalmente por <strong>Spotify</strong>. A diferencia de otras librer√≠as que
                    envuelven la API de Java o usan REST, Snakebite se comunica directamente con el protocolo RPC de
                    Hadoop.
                </p>

                <div class="highlight-box primary">
                    <p class="title">üöÄ Ventajas Principales</p>
                    <p class="content">
                        La gran ventaja de Snakebite es que <strong>no requiere cargar la JVM (Java Virtual
                            Machine)</strong>. Esto hace que sea mucho m√°s r√°pido y ligero para scripts de Python que
                        necesitan realizar operaciones r√°pidas de sistema de archivos (listar, borrar, leer, verificar).
                    </p>
                </div>

                <h3>M√©todos Principales</h3>
                <p>Al igual que <code class="code-badge">Great Expectations</code>, Snakebite ofrece m√©todos intuitivos
                    para gestionar HDFS. Aqu√≠
                    tienes los m√°s esenciales:</p>

                <div style="display: flex; flex-direction: column; gap: 0.75rem;">
                    <div class="example-item" style="border-left-color: var(--color-ml);">
                        <p class="title" style="color: var(--color-ml); font-family: monospace;">Client(host, port)</p>
                        <p class="description"><strong>üîå Conexi√≥n:</strong> Crea la conexi√≥n con el NameNode. No
                            necesita configuraci√≥n XML compleja, solo host y puerto (default 8020).</p>
                    </div>

                    <div class="example-item" style="border-left-color: var(--color-ml);">
                        <p class="title" style="color: var(--color-ml); font-family: monospace;">ls(['/path'])</p>
                        <p class="description"><strong>üìÇ Listado:</strong> Devuelve un generador con la informaci√≥n de
                            archivos y directorios. Mucho m√°s r√°pido que <code class="code-badge">hdfs dfs -ls</code>.
                        </p>
                    </div>

                    <div class="example-item" style="border-left-color: var(--color-ml);">
                        <p class="title" style="color: var(--color-ml); font-family: monospace;">stat(['/path'])</p>
                        <p class="description"><strong>üìä Metadatos:</strong> Obtiene informaci√≥n detallada: tama√±o,
                            permisos, propietario y factor de replicaci√≥n.</p>
                    </div>

                    <div class="example-item" style="border-left-color: var(--color-ml);">
                        <p class="title" style="color: var(--color-ml); font-family: monospace;">fsck(['/path'])</p>
                        <p class="description"><strong>üè• Diagn√≥stico:</strong> (Extensi√≥n) Permite verificar el estado
                            de salud de los bloques de un archivo, vital para detectar corrupci√≥n.</p>
                    </div>

                    <div class="example-item" style="border-left-color: var(--color-ml);">
                        <p class="title" style="color: var(--color-ml); font-family: monospace;">cat(['/path'])</p>
                        <p class="description"><strong>üìñ Lectura:</strong> Lee el contenido de los archivos
                            directamente en un stream de Python.</p>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 3: C√ìDIGO DE EJEMPLO -->
            <section class="section">
                <h2 class="section-title">üíª Implementaci√≥n: Verificaci√≥n de Integridad</h2>
                <p>
                    El siguiente script utiliza <code class="code-badge">snakebite</code> para auditar un archivo
                    gen√≥mico cr√≠tico, verificando sus
                    metadatos y buscando bloques corruptos que necesiten reparaci√≥n.
                </p>

                <div class="code-container">
                    <pre><code class="language-python">from snakebite.client import Client

# Configurar cliente HDFS
# Conectamos al NameNode en el puerto est√°ndar 8020. use_trash=False borra permanentemente si se solicitara.
hdfs_client = Client("namenode_host", 8020, use_trash=False)

# Funci√≥n para verificar integridad de bloques
def check_block_integrity(file_path):
    try:
        # Obtener informaci√≥n del archivo (stat devuelve un iterador/lista)
        file_info = hdfs_client.stat([file_path])[0]
        print(f"Verificando integridad de: {file_path}")
        print(f"Tama√±o: {file_info['length']} bytes, R√©plicas: {file_info['replication']}")

        # Verificar bloques y detectar corrupciones usando fsck
        # corrupt=True filtra para buscar espec√≠ficamente problemas
        for block in hdfs_client.fsck([file_path], corrupt=True):
            if block['corrupt']:
                print(f"Bloque corrupto detectado en {file_path}: {block['block_id']}")
                
                # Iniciar reparaci√≥n autom√°tica
                # Nota: repair() solicitar√≠a al NameNode regenerar las r√©plicas desde nodos sanos
                hdfs_client.repair([file_path])
                print(f"Reparaci√≥n iniciada para {file_path}")
            else:
                print(f"Bloque {block['block_id']} intacto")
                
    except Exception as e:
        print(f"Error al verificar integridad: {e}")

# Ejemplo de uso
file_path = "/data/genomics_data.fasta"
check_block_integrity(file_path)</code></pre>
                </div>
            </section>

            <!-- SECCI√ìN 4: EXPLICACI√ìN DEL C√ìDIGO -->
            <section class="section">
                <h2 class="section-title">üîç An√°lisis del Script</h2>

                <div style="display: flex; flex-direction: column; gap: 0.75rem;">
                    <div class="example-item" style="border-left-color: var(--color-primary);">
                        <p class="title" style="color: var(--color-primary); font-family: monospace;">
                            Client("namenode_host", 8020)</p>
                        <p class="description"><strong>üîå Instanciaci√≥n:</strong> Crea un socket persistente con el
                            Cluster. Al ser Python puro, esta conexi√≥n es casi instant√°nea comparada con la JVM, ideal
                            para scripts r√°pidos o funciones Lambda.</p>
                    </div>

                    <div class="example-item" style="border-left-color: var(--color-secondary);">
                        <p class="title" style="color: var(--color-secondary); font-family: monospace;">
                            hdfs_client.stat([file_path])</p>
                        <p class="description"><strong>üìä Estad√≠sticas:</strong> Consultar metadatos (tama√±o, r√©plicas)
                            es el primer paso vital. Permite validar si el archivo cumple las pol√≠ticas del cl√∫ster
                            antes de leerlo.</p>
                    </div>

                    <div class="example-item" style="border-left-color: var(--color-primary);">
                        <p class="title" style="color: var(--color-primary); font-family: monospace;">
                            hdfs_client.fsck(...) </p>
                        <p class="description"><strong>üè• Chequeo de Salud:</strong> La funci√≥n clave del script. Al
                            iterar sobre los bloques, Snakebite comprueba los checksums sin transferir todo el dato,
                            solo metadatos.</p>
                    </div>

                    <div class="example-item" style="border-left-color: var(--color-secondary);">
                        <p class="title" style="color: var(--color-secondary); font-family: monospace;">
                            hdfs_client.repair(...)</p>
                        <p class="description"><strong>üõ†Ô∏è Reparaci√≥n:</strong> Abstracci√≥n poderosa. Env√≠a un comando
                            al NameNode para que marque los bloques corruptos y active la re-replicaci√≥n autom√°tica
                            desde copias sanas.</p>
                    </div>
                </div>

                <div class="highlight-box warning">
                    <p class="title">‚ö†Ô∏è Nota sobre Snakebite</p>
                    <p class="content">
                        Snakebite original (de Spotify) fue revolucionario por implementar el <strong>protocolo RPC de
                            Hadoop nativamente en Python</strong>, eliminando la necesidad de la JVM (Java Virtual
                        Machine). Esto lo hac√≠a extremadamente r√°pido para operaciones de metadatos. Sin embargo,
                        soporta principalmente Python 2 y versiones antiguas de Hadoop.
                    </p>
                </div>

                <h3 class="mt-4">Alternativa Moderna: PyArrow</h3>
                <p>
                    Hoy en d√≠a, el est√°ndar para interactuar con sistemas HDFS desde Python es <strong>PyArrow</strong>.
                    Aunque bajo el cap√≥ suele utilizar librer√≠as C++ (`libhdfs3`) o JNI para comunicarse, ofrece una
                    interfaz mucho m√°s robusta y compatible con el ecosistema moderno (Pandas, Parquet).
                </p>

                <div class="highlight-box secondary">
                    <p class="title">üöÄ La evoluci√≥n del RPC</p>
                    <p class="content">
                        Mientras Snakebite "hablaba" el protocolo RPC de Hadoop directamente, herramientas modernas como
                        PyArrow abstraen esta complejidad usando <strong>Apache Arrow</strong> como formato de memoria
                        com√∫n. Esto permite transferencias de datos "Zero-Copy" entre sistemas, aunque a menudo
                        reintroducen dependencias binarias (C++) o de Java, perdiendo la pureza "Python-only" de
                        Snakebite pero ganando en rendimiento de transferencia de datos masivos.
                    </p>
                </div>
            </section>

            <!-- SECCI√ìN 5: PYARROW -->
            <section class="section">
                <h2 class="section-title">üèπ Implementaci√≥n Moderna con PyArrow</h2>
                <p>
                    A diferencia de Snakebite, <strong>PyArrow</strong> utiliza las librer√≠as nativas de Hadoop (o JNI)
                    para conectar, lo que asegura compatibilidad total con Hadoop 3.x.
                </p>

                <div class="code-container">
                    <pre><code class="language-python">from pyarrow import fs

# 1. Conexi√≥n al sistema de archivos HDFS
# PyArrow detecta autom√°ticamente la configuraci√≥n de Hadoop si las variables de entorno est√°n seteadas
hdfs = fs.HadoopFileSystem(host='namenode_host', port=8020)

# 2. Listar archivos y obtener informaci√≥n
file_info = hdfs.get_file_info("/data/ventas.parquet")

if file_info.type == fs.FileType.File:
    print(f"Archivo encontrado: {file_info.path}")
    print(f"Tama√±o: {file_info.size} bytes")
    
    # 3. Lectura eficiente (Zero-Copy si es posible)
    with hdfs.open_input_stream("/data/ventas.parquet") as stream:
        print("Le√≠dos primeros 100 bytes:", stream.read(100))
        
# 4. Operaciones avanzadas (PyArrow se integra nativamente con Parquet)
import pyarrow.parquet as pq

# Lee directamente desde HDFS a una tabla Arrow (s√∫per eficiente)
table = pq.read_table("/data/ventas.parquet", filesystem=hdfs)
print(f"Esquema del dataset:\n{table.schema}")</code></pre>
                </div>

                <div class="example-item" style="border-left-color: var(--color-ml); margin-top: 1rem;">
                    <p class="title" style="color: var(--color-ml); font-family: monospace;">fs.HadoopFileSystem(host,
                        port)</p>
                    <p class="description"><strong>üîó Conector Oficial:</strong> Utiliza el driver C++ oficial de Arrow,
                        garantizando que si funciona en Hadoop, funciona en Python.</p>
                </div>
            </section>

            <!-- RESUMEN -->
            <section class="section">
                <h2 class="section-title">üìö Resumen</h2>
                <div class="scenario-box neutral">
                    <p>
                        La elecci√≥n de la herramienta depende del entorno: <strong>Snakebite</strong> es ideal para
                        scripts de mantenimiento ligeros y sin dependencias de Java en cl√∫steres antiguos. Para entornos
                        modernos de producci√≥n data-intensive (Hadoop 3.x), <strong>PyArrow</strong> es el est√°ndar
                        gracias a su robustez, integraci√≥n con Pandas/Parquet y su arquitectura de memoria eficiente.
                    </p>
                </div>
            </section>
        </main>

        <footer>
            <h3>iLERNA</h3>
            <p class="footer-course">Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</p>
            <a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a>
            <p class="footer-info">Centro oficial de FP online y presencial. Ciclos formativos de Grado Medio y Grado
                Superior.</p>
            <p class="footer-info">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>

            <div class="penguin">
                <span>üêß</span>
            </div>
        </footer>
    </div>

    <script src="../js/lecciones.js"></script>
</body>

</html>