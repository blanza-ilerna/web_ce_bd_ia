<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Ejemplo pr√°ctico de automatizaci√≥n de pipelines de datos utilizando Apache Airflow para orquestar tareas en Apache Hive.">
    <meta name="author" content="iLERNA">
    <title>Pipeline Autom√°tico: Airflow + Hive | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
</head>

<body>
    <div class="container">
        <header>
            <div class="header-container">
                <div class="logo-container">
                    <a href="../index.html">
                        <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                    </a>
                    <div class="logo-text">
                        iLERNA
                        <span>Curso de Especializaci√≥n en IA y Big Data</span>
                    </div>
                </div>
                <div class="breadcrumb">
                    <a href="../index.html">Inicio</a> ‚Ä∫
                    <a href="index.html">Aplicaci√≥n de Big Data</a> ‚Ä∫
                    <span>Pipeline Autom√°tico: Airflow + Hive</span>
                </div>
            </div>
        </header>

        <main>
            <!-- T√çTULO PRINCIPAL -->
            <div class="hero">
                <h1>Pipeline Autom√°tico: Airflow + Hive</h1>
                <p class="subtitle">Orquestaci√≥n de procesos ETL y An√°lisis de Datos</p>
            </div>

            <!-- SECCI√ìN 1: INTRODUCCI√ìN Y CONTEXTO -->
            <section class="section">
                <h2 class="section-title">Automatizaci√≥n en Big Data</h2>

                <div class="grid-features">
                    <div class="feature-card primary">
                        <h4>üå¨Ô∏è Apache Airflow</h4>
                        <p>Es una plataforma de c√≥digo abierto para crear, programar y monitorear flujos de trabajo
                            (pipelines) de manera program√°tica. En Airflow, los flujos se definen como c√≥digo Python, lo
                            que permite un mantenimiento, versionado y pruebas m√°s sencillos. Usa el concepto de
                            <strong>DAG (Directed Acyclic Graph)</strong> para gestionar dependencias entre tareas.</p>
                    </div>
                    <div class="feature-card secondary">
                        <h4>üêù Apache Hive</h4>
                        <p>Como hemos visto, Hive es un sistema de data warehouse sobre Hadoop que facilita la lectura,
                            escritura y administraci√≥n de grandes conjuntos de datos almacenados en almacenamiento
                            distribuido mediante SQL. Es ideal para tareas de an√°lisis batch sobre grandes vol√∫menes de
                            datos.</p>
                    </div>
                </div>

                <div class="highlight-box primary">
                    <p class="title">üéØ ¬øPor qu√© usarlos juntos?</p>
                    <p class="content">
                        La combinaci√≥n es poderosa: <strong>Airflow act√∫a como el director de orquesta</strong>,
                        decidiendo cu√°ndo y en qu√© orden se ejecutan las tareas, mientras que <strong>Hive act√∫a como el
                            motor de procesamiento</strong> pesado, ejecutando las consultas SQL complejas sobre el
                        cluster. Esto permite automatizar reportes diarios, procesos ETL complejos y actualizaciones de
                        Data Lakes sin intervenci√≥n manual.
                    </p>
                </div>
            </section>

            <!-- SECCI√ìN 2: EL C√ìDIGO DEL PIPELINE -->
            <section class="section">
                <h2 class="section-title">üíª C√≥digo del Pipeline (DAG)</h2>
                <p>
                    A continuaci√≥n se muestra un ejemplo completo de un DAG de Airflow que primero importa datos desde
                    una base de datos MySQL usando <strong>Sqoop</strong> y luego realiza un an√°lisis de ventas
                    utilizando <strong>Hive</strong>.
                </p>

                <div class="code-container">
                    <pre><code class="language-python">from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.hive_operator import HiveOperator
from datetime import datetime, timedelta

# Definir argumentos del DAG
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Definir DAG
with DAG(
    'sales_pipeline',
    default_args=default_args,
    description='Pipeline para an√°lisis de ventas',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2025, 10, 13),
    catchup=False,
) as dag:

    # Tarea 1: Importar datos con Sqoop
    import_data = BashOperator(
        task_id='import_sales',
        bash_command='sqoop import --connect jdbc:mysql://localhost/sales_db --table ventas --target-dir /data/ventas_hdfs --split-by id_venta',
    )

    # Tarea 2: Analizar datos con Hive
    analyze_data = HiveOperator(
        task_id='analyze_sales',
        hql="""
        SELECT region, SUM(precio) AS total_ventas
        FROM ventas
        GROUP BY region
        """,
        hive_cli_conn_id='hive_conn',
    )

    # Definir dependencias
    import_data >> analyze_data</code></pre>
                </div>
            </section>

            <!-- SECCI√ìN 3: EXPLICACI√ìN DEL C√ìDIGO -->
            <section class="section">
                <h2 class="section-title">üîç Explicaci√≥n Paso a Paso</h2>
                <p>Analicemos qu√© hace exactamente cada bloque del c√≥digo anterior:</p>

                <div class="grid-features">
                    <div class="feature-card primary">
                        <h4>1. Importaciones y Configuraci√≥n</h4>
                        <p>Importamos las librer√≠as necesarias, incluyendo los operadores espec√≠ficos (`BashOperator`
                            para comandos de terminal y `HiveOperator` para consultas Hive). Definimos `default_args`
                            para establecer reglas globales como el propietario del proceso y la pol√≠tica de reintentos
                            (1 reintento tras 5 minutos si falla).</p>
                    </div>

                    <div class="feature-card secondary">
                        <h4>2. Definici√≥n del DAG</h4>
                        <p>Instanciamos el objeto `DAG` con el nombre <code>'sales_pipeline'</code>.
                        <ul>
                            <li><strong>schedule_interval:</strong> Se ejecutar√° una vez al d√≠a (`days=1`).</li>
                            <li><strong>start_date:</strong> Comienza a ejecutarse a partir del 13 de octubre de 2025.
                            </li>
                            <li><strong>catchup=False:</strong> Evita que Airflow ejecute tareas pasadas si el DAG se
                                activa despu√©s de la fecha de inicio.</li>
                        </ul>
                        </p>
                    </div>

                    <div class="feature-card primary">
                        <h4>3. Tarea 1: Importaci√≥n (ETL)</h4>
                        <p>La tarea <code>import_data</code> usa <strong>BashOperator</strong> para ejecutar un comando
                            de sistema. En este caso, llama a <strong>Apache Sqoop</strong> para traer datos de una
                            tabla MySQL (`ventas`) y volcarlos en HDFS. Es el paso de "Extracci√≥n" y "Carga" inicial.
                        </p>
                    </div>

                    <div class="feature-card secondary">
                        <h4>4. Tarea 2: An√°lisis (Hive)</h4>
                        <p>La tarea <code>analyze_data</code> usa <strong>HiveOperator</strong>. Ejecuta una consulta
                            HQL (Hive Query Language) que agrega las ventas por regi√≥n. Este c√°lculo se realiza sobre
                            los datos que acabamos de importar. Requiere una conexi√≥n configurada en Airflow
                            (`hive_conn`).</p>
                    </div>
                </div>

                <div class="highlight-box warning">
                    <p class="title">üîó Definici√≥n de Dependencias</p>
                    <p class="content">
                        La l√≠nea final <code>import_data >> analyze_data</code> es crucial. Utiliza el operador de
                        bitwise shift (`>>`) para establecer que la tarea de an√°lisis <strong>NO debe comenzar</strong>
                        hasta que la importaci√≥n de datos haya finalizado con √©xito.
                    </p>
                </div>
            </section>

            <!-- RESUMEN FINAL -->
            <section class="section">
                <h2 class="section-title">üìö Resumen</h2>
                <div class="scenario-box neutral">
                    <p>
                        Este script representa un flujo de trabajo t√≠pico en ingenier√≠a de datos:
                        <strong>Ingesta ‚Üí Procesamiento</strong>.
                        Airflow garantiza que si la ingesta falla, el procesamiento no se ejecute (evitando c√°lculos
                        err√≥neos o vac√≠os) y notifique al equipo, proporcionando robustez y confiabilidad al ecosistema
                        de Big Data.
                    </p>
                </div>
            </section>
        </main>

        <footer>
            <h3>iLERNA</h3>
            <p class="footer-course">Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</p>
            <a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a>
            <p class="footer-info">Centro oficial de FP online y presencial. Ciclos formativos de Grado Medio y Grado
                Superior.</p>
            <p class="footer-info">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>

            <div class="penguin">
                <span>üêß</span>
            </div>
        </footer>
    </div>

    <script src="../js/lecciones.js"></script>
</body>

</html>