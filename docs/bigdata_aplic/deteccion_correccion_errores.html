<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Detecci√≥n y Correcci√≥n de Errores en Big Data: Validaciones, outliers, imputaci√≥n y reconciliaci√≥n. Estrategias con Spark y Machine Learning">
    <meta name="author" content="iLERNA">
    <title>Detecci√≥n y Correcci√≥n de Errores | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <link rel="stylesheet" href="../css/mermaid-ilerna.css">
</head>

<body>
    <div class="container">
        <header>
            <div class="header-container">
                <div class="logo-container">
                    <a href="../index.html">
                        <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                    </a>
                    <div class="logo-text">
                        iLERNA
                        <span>Curso de Especializaci√≥n en IA y Big Data</span>
                    </div>
                </div>
                <div class="breadcrumb">
                    <a href="../index.html">Inicio</a> ‚Ä∫
                    <a href="index.html">Aplicaciones Big Data</a> ‚Ä∫
                    <span>Detecci√≥n y Correcci√≥n de Errores</span>
                </div>
            </div>
        </header>

        <main>
            <!-- T√çTULO PRINCIPAL -->
            <div class="hero">
                <h1>Detecci√≥n y Correcci√≥n de Errores</h1>
                <p class="subtitle">Estrategias de Validaci√≥n, Limpieza y Reconciliaci√≥n en Big Data</p>
            </div>

            <!-- TABLE OF CONTENTS -->
            <section class="section">
                <div class="highlight-box primary">
                    <h3 class="color-primary">üìë √çndice de Contenidos</h3>
                    <div class="grid-features">
                        <div class="feature-card toc-card">
                            <a href="#introduccion">
                                <h4>1. Introducci√≥n</h4>
                                <p>Importancia de la detecci√≥n de errores</p>
                            </a>
                        </div>
                        <div class="feature-card toc-card">
                            <a href="#tipos-errores">
                                <h4>2. Tipos de Errores</h4>
                                <p>Sint√°cticos, sem√°nticos, duplicaci√≥n...</p>
                            </a>
                        </div>
                        <div class="feature-card toc-card">
                            <a href="#metodos-deteccion">
                                <h4>3. M√©todos de Detecci√≥n</h4>
                                <p>Validaciones, ML, checksums</p>
                            </a>
                        </div>
                        <div class="feature-card toc-card">
                            <a href="#estrategias-correccion">
                                <h4>4. Estrategias de Correcci√≥n</h4>
                                <p>Imputaci√≥n, deduplicaci√≥n, reconciliaci√≥n</p>
                            </a>
                        </div>
                        <div class="feature-card toc-card">
                            <a href="#caso-estudio">
                                <h4>5. Caso de Estudio</h4>
                                <p>Universidad de Barcelona</p>
                            </a>
                        </div>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 1: INTRODUCCI√ìN -->
            <section class="section" id="introduccion">
                <h2 class="section-title">Importancia de la Detecci√≥n y Correcci√≥n de Errores</h2>
                <p>La <strong>detecci√≥n y correcci√≥n de errores</strong> son esenciales para garantizar la integridad de
                    los datos en Big Data, donde los errores pueden originarse en m√∫ltiples etapas: captura,
                    integraci√≥n, transformaci√≥n o almacenamiento. En entornos educativos, donde se gestionan millones
                    de registros estudiantiles, acad√©micos y administrativos, un solo error puede propagarse y afectar
                    decisiones cr√≠ticas.</p>

                <div class="highlight-box secondary">
                    <p class="title">üéØ Caso Real - MIT</p>
                    <p class="content">El MIT detect√≥ que el 12% de las calificaciones importadas desde sistemas legacy
                        conten√≠an errores sint√°cticos (caracteres especiales, formatos incorrectos) que imped√≠an el
                        c√°lculo correcto de promedios acad√©micos. Implementaron un pipeline de validaci√≥n autom√°tica con
                        Apache Spark que detecta y corrige errores en tiempo real, reduciendo incidencias del 12% al
                        0.3% y eliminando 400+ horas/a√±o de correcci√≥n manual.</p>
                </div>

                <h3>Impacto de los Errores No Detectados</h3>
                <div class="grid-features">
                    <div class="disadvantage-box">
                        <h4>‚ùå Decisiones Acad√©micas Err√≥neas</h4>
                        <p>Asignaci√≥n incorrecta de becas, matr√≠culas de honor o admisiones por datos defectuosos.</p>
                    </div>
                    <div class="disadvantage-box">
                        <h4>‚ùå Reportes Institucionales Inv√°lidos</h4>
                        <p>M√©tricas de rendimiento, tasas de graduaci√≥n y rankings universitarios distorsionados.</p>
                    </div>
                    <div class="disadvantage-box">
                        <h4>‚ùå P√©rdida de Acreditaciones</h4>
                        <p>Auditor√≠as educativas fallan por inconsistencias en registros hist√≥ricos.</p>
                    </div>
                    <div class="disadvantage-box">
                        <h4>‚ùå Modelos Predictivos Sesgados</h4>
                        <p>Predicciones de abandono o rendimiento estudiantil con errores del 20-30%.</p>
                    </div>
                </div>
            </section>

            <!-- SECCI√ìN 2: TIPOS DE ERRORES -->
            <section class="section" id="tipos-errores">
                <h2 class="section-title">üîç Tipos Comunes de Errores en Big Data</h2>

                <!-- ERROR 1: SINT√ÅCTICOS -->
                <div class="highlight-box primary mb-2">
                    <h3 class="color-primary">1. Errores Sint√°cticos</h3>
                    <p><strong>Definici√≥n:</strong> Formatos inv√°lidos, tipos de datos incorrectos o caracteres no
                        reconocidos que violan el esquema de datos.</p>

                    <div class="styled-table">
                        <table>
                            <thead>
                                <tr>
                                    <th>Campo</th>
                                    <th>Valor Err√≥neo</th>
                                    <th>Tipo de Error</th>
                                    <th>Impacto</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>student_age</code></td>
                                    <td>"veinte"</td>
                                    <td>Texto en campo num√©rico</td>
                                    <td>Falla procesamiento estad√≠stico</td>
                                </tr>
                                <tr>
                                    <td><code>enrollment_date</code></td>
                                    <td>"32/15/2025"</td>
                                    <td>Formato fecha inv√°lido</td>
                                    <td>Imposible ordenar cronol√≥gicamente</td>
                                </tr>
                                <tr>
                                    <td><code>email</code></td>
                                    <td>"estudiante@universidad"</td>
                                    <td>Formato email incompleto</td>
                                    <td>Notificaciones no llegan</td>
                                </tr>
                                <tr>
                                    <td><code>final_grade</code></td>
                                    <td>"8.5#"</td>
                                    <td>Caracter especial no v√°lido</td>
                                    <td>Conversi√≥n a n√∫mero falla</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="color-primary mt-1">Detecci√≥n con Spark SQL</h4>
                    <pre><code class="language-sql">-- Detectar errores sint√°cticos en edad
SELECT student_id, age
FROM students
WHERE age NOT RLIKE '^[0-9]+$'  -- No es n√∫mero
   OR CAST(age AS INT) IS NULL  -- No se puede convertir
   OR CAST(age AS INT) < 0      -- Valor negativo
   OR CAST(age AS INT) > 120;   -- Valor imposible

-- Resultado: 1,250 registros con errores (2.5% del total)</code></pre>
                </div>

                <!-- ERROR 2: SEM√ÅNTICOS -->
                <div class="highlight-box secondary mb-2">
                    <h3 class="color-secondary">2. Errores Sem√°nticos</h3>
                    <p><strong>Definici√≥n:</strong> Incongruencias l√≥gicas que violan reglas de negocio, aunque el
                        formato sea t√©cnicamente v√°lido.</p>

                    <div class="example-box example-purple">
                        <h4>Ejemplos Acad√©micos de Errores Sem√°nticos</h4>
                        <ul>
                            <li><strong>Fechas imposibles:</strong> Fecha de graduaci√≥n anterior a fecha de ingreso</li>
                            <li><strong>Contradicciones:</strong> Estado "Graduado" pero cr√©ditos_completados = 60/240
                            </li>
                            <li><strong>Rangos inv√°lidos:</strong> Calificaci√≥n final = 12.5 (escala 0-10)</li>
                            <li><strong>L√≥gica quebrada:</strong> Estudiante inscrito en "F√≠sica Cu√°ntica III" sin
                                aprobar I y II</li>
                            <li><strong>Inconsistencia temporal:</strong> Edad = 18 a√±os pero a√±o_nacimiento = 1985</li>
                        </ul>
                    </div>

                    <h4 class="color-secondary mt-1">Validaci√≥n Sem√°ntica con PySpark</h4>
                    <pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, current_date, datediff

spark = SparkSession.builder.appName("SemanticValidation").getOrCreate()
df = spark.read.parquet("/data/students/")

# Regla 1: Fecha de graduaci√≥n no puede ser anterior a ingreso
semantic_errors_1 = df.filter(
    col("graduation_date") < col("enrollment_date")
)
print(f"Error temporal: {semantic_errors_1.count()} registros")

# Regla 2: Edad calculada debe coincidir con edad declarada
df_with_calc_age = df.withColumn(
    "calculated_age",
    (year(current_date()) - col("birth_year"))
)

semantic_errors_2 = df_with_calc_age.filter(
    abs(col("age") - col("calculated_age")) > 1  # Tolerancia 1 a√±o
)
print(f"Error edad: {semantic_errors_2.count()} registros")

# Regla 3: Cr√©ditos cursados no pueden exceder plan de estudios
semantic_errors_3 = df.filter(
    col("credits_completed") > col("total_credits_required")
)
print(f"Error cr√©ditos: {semantic_errors_3.count()} registros")

# Consolidar todos los errores sem√°nticos
all_semantic_errors = semantic_errors_1.union(semantic_errors_2).union(semantic_errors_3)
all_semantic_errors.write.parquet("/output/semantic_errors/")</code></pre>
                </div>

                <!-- ERROR 3: DUPLICACI√ìN -->
                <div class="highlight-box primary mb-2">
                    <h3 class="color-primary">3. Errores de Duplicaci√≥n</h3>
                    <p><strong>Definici√≥n:</strong> Registros repetidos con variaciones m√≠nimas (typos, abreviaciones,
                        espacios extra) que representan la misma entidad.</p>

                    <div class="comparison-grid">
                        <div class="highlight-box secondary">
                            <h4 class="color-secondary text-center">Duplicados Exactos</h4>
                            <p>Todos los campos id√©nticos. <strong>F√°ciles de detectar.</strong></p>
                            <pre style="font-size: 0.85rem;">ID: 12345
Nombre: Mar√≠a Garc√≠a L√≥pez
Email: maria.garcia@uni.edu
DNI: 12345678A</pre>
                            <pre style="font-size: 0.85rem;">ID: 67890
Nombre: Mar√≠a Garc√≠a L√≥pez
Email: maria.garcia@uni.edu
DNI: 12345678A</pre>
                        </div>
                        <div class="warning-box">
                            <h4 class="color-warning text-center">Duplicados Fuzzy</h4>
                            <p>Variaciones menores. <strong>Requieren similitud textual.</strong></p>
                            <pre style="font-size: 0.85rem;">Nombre: Mar√≠a Garc√≠a L√≥pez
Email: maria.garcia@uni.edu</pre>
                            <pre style="font-size: 0.85rem;">Nombre: Maria Garcia Lopez (sin tildes)
Email: m.garcia@uni.edu (abreviado)</pre>
                        </div>
                    </div>

                    <h4 class="color-primary mt-1">Detecci√≥n Fuzzy con Levenshtein</h4>
                    <pre><code class="language-python">from pyspark.sql.functions import levenshtein, lower, trim

# Normalizar nombres
df_normalized = df.withColumn(
    "name_normalized",
    lower(trim(col("first_name")))
)

# Detectar duplicados con similitud > 85%
from pyspark.sql import Window
from pyspark.sql.functions import row_number

# Self-join para comparar cada registro con los dem√°s
df_pairs = df_normalized.alias("a").crossJoin(
    df_normalized.alias("b")
).filter(
    (col("a.student_id") < col("b.student_id")) &  # Evitar duplicados del join
    (levenshtein(col("a.name_normalized"), col("b.name_normalized")) <= 2)  # M√°ximo 2 caracteres diferentes
)

# Resultado: 850 pares de posibles duplicados
df_pairs.select(
    "a.student_id", "a.name_normalized",
    "b.student_id", "b.name_normalized",
    levenshtein("a.name_normalized", "b.name_normalized").alias("distance")
).show()

# +----------+-------------------+----------+-------------------+--------+
# |student_id|name_normalized    |student_id|name_normalized    |distance|
# +----------+-------------------+----------+-------------------+--------+
# |10001     |maria garcia lopez |10234     |maria garcia lopex |1       |
# |10002     |john smith         |10567     |jon smith          |1       |
# +----------+-------------------+----------+-------------------+--------+</code></pre>
                </div>

                <!-- ERROR 4: INTEGRACI√ìN -->
                <div class="highlight-box secondary mb-2">
                    <h3 class="color-secondary">4. Errores de Integraci√≥n</h3>
                    <p><strong>Definici√≥n:</strong> Inconsistencias al combinar datos de fuentes heterog√©neas con
                        diferentes formatos, unidades o convenciones.</p>

                    <div class="example-box example-purple">
                        <h4>Escenario: Fusi√≥n de 3 Campus</h4>
                        <p>Una universidad fusiona bases de datos de 3 campus aut√≥nomos:</p>
                        <ul>
                            <li><strong>Campus A:</strong> Notas en escala 0-100</li>
                            <li><strong>Campus B:</strong> Notas en escala 0-10</li>
                            <li><strong>Campus C:</strong> Notas en letras (A, B, C, D, F)</li>
                        </ul>
                        <p><strong>Problema:</strong> Sin normalizaci√≥n, el promedio global es inv√°lido.</p>
                    </div>

                    <h4 class="color-secondary mt-1">Normalizaci√≥n Multi-Fuente</h4>
                    <pre><code class="language-python">from pyspark.sql.functions import when, regexp_extract

# Funci√≥n de normalizaci√≥n a escala 0-10
def normalize_grade(df, source_column, campus):
    if campus == "A":  # 0-100 ‚Üí 0-10
        return df.withColumn("normalized_grade", col(source_column) / 10)
    elif campus == "B":  # Ya en 0-10
        return df.withColumn("normalized_grade", col(source_column))
    elif campus == "C":  # Letras ‚Üí 0-10
        return df.withColumn(
            "normalized_grade",
            when(col(source_column) == "A", 10.0)
            .when(col(source_column) == "B", 8.0)
            .when(col(source_column) == "C", 6.0)
            .when(col(source_column) == "D", 4.0)
            .when(col(source_column) == "F", 0.0)
            .otherwise(None)
        )

# Aplicar normalizaci√≥n
df_campus_a = normalize_grade(df_a, "grade", "A")
df_campus_b = normalize_grade(df_b, "grade", "B")
df_campus_c = normalize_grade(df_c, "grade", "C")

# Unificar datasets
df_unified = df_campus_a.union(df_campus_b).union(df_campus_c)

# Calcular promedio global v√°lido
avg_grade = df_unified.agg({"normalized_grade": "avg"}).collect()[0][0]
print(f"Promedio global normalizado: {avg_grade:.2f}/10")</code></pre>
                </div>

                <!-- ERROR 5: TRANSMISI√ìN -->
                <div class="warning-box mb-2">
                    <h3 class="color-warning">5. Errores de Transmisi√≥n/Almacenamiento</h3>
                    <p><strong>Definici√≥n:</strong> P√©rdida o corrupci√≥n de datos durante la replicaci√≥n en sistemas
                        distribuidos (HDFS, Cassandra, S3).</p>

                    <h4 class="color-warning">Checksum en HDFS</h4>
                    <pre><code class="language-python">import hashlib
from hdfs import InsecureClient

client = InsecureClient('http://namenode:50070', user='hdfs')

def verify_file_integrity(hdfs_path):
    # Leer archivo de HDFS
    with client.read(hdfs_path) as reader:
        content = reader.read()

    # Calcular checksum
    checksum_calculated = hashlib.md5(content).hexdigest()

    # Obtener checksum almacenado
    file_status = client.status(hdfs_path)
    checksum_stored = file_status.get('checksum', None)

    if checksum_calculated == checksum_stored:
        print(f"‚úì Integridad verificada: {hdfs_path}")
        return True
    else:
        print(f"‚úó CORRUPCI√ìN DETECTADA: {hdfs_path}")
        print(f"  Esperado: {checksum_stored}")
        print(f"  Calculado: {checksum_calculated}")
        return False

# Verificar lote de archivos
files = client.list('/data/grades/2025/')
corrupted = [f for f in files if not verify_file_integrity(f'/data/grades/2025/{f}')]

print(f"Archivos corruptos: {len(corrupted)}/{len(files)}")</code></pre>
                </div>
            </section>

            <!-- SECCI√ìN 3: M√âTODOS DE DETECCI√ìN -->
            <section class="section" id="metodos-deteccion">
                <h2 class="section-title">üî¨ M√©todos de Detecci√≥n de Errores</h2>

                <!-- M√âTODO 1: VALIDACIONES -->
                <div class="highlight-box primary mb-2">
                    <h3 class="color-primary">1. Validaciones Autom√°ticas con Reglas de Negocio</h3>
                    <p>Se aplican <strong>constraints</strong> y <strong>reglas l√≥gicas</strong> para identificar datos
                        inv√°lidos seg√∫n el dominio acad√©mico.</p>

                    <h4 class="color-primary">Biblioteca de Reglas de Validaci√≥n</h4>
                    <div class="styled-table">
                        <table>
                            <thead>
                                <tr>
                                    <th>Regla</th>
                                    <th>Expresi√≥n SQL</th>
                                    <th>Ejemplo Error</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Rango edad</strong></td>
                                    <td><code>age BETWEEN 16 AND 70</code></td>
                                    <td>age = 150</td>
                                </tr>
                                <tr>
                                    <td><strong>Formato email</strong></td>
                                    <td><code>email RLIKE '^[^@]+@[^@]+\.[^@]+$'</code></td>
                                    <td>email = "juan@"</td>
                                </tr>
                                <tr>
                                    <td><strong>Escala notas</strong></td>
                                    <td><code>grade BETWEEN 0.0 AND 10.0</code></td>
                                    <td>grade = 15.2</td>
                                </tr>
                                <tr>
                                    <td><strong>Fechas l√≥gicas</strong></td>
                                    <td><code>graduation_date >= enrollment_date</code></td>
                                    <td>graduaci√≥n antes de ingreso</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <!-- M√âTODO 2: CROSS-CHECKING -->
                <div class="highlight-box secondary mb-2">
                    <h3 class="color-secondary">2. Comparaci√≥n entre Fuentes (Cross-Checking)</h3>
                    <p>Contrasta datos entre sistemas independientes (LMS, SIS, ERP) para detectar inconsistencias.</p>

                    <h4 class="color-secondary">Ejemplo: Verificar Cr√©ditos Cursados</h4>
                    <pre><code class="language-sql">-- Sistema LMS (Learning Management System)
SELECT student_id, SUM(credits) AS lms_credits
FROM lms.course_enrollments
WHERE status = 'COMPLETED'
GROUP BY student_id;

-- Sistema SIS (Student Information System)
SELECT student_id, credits_completed AS sis_credits
FROM sis.student_records;

-- Cross-check: Detectar discrepancias
SELECT
    lms.student_id,
    lms.lms_credits,
    sis.sis_credits,
    ABS(lms.lms_credits - sis.sis_credits) AS discrepancy
FROM lms_credits lms
JOIN sis_credits sis ON lms.student_id = sis.student_id
WHERE ABS(lms.lms_credits - sis.sis_credits) > 3  -- Tolerancia 3 cr√©ditos
ORDER BY discrepancy DESC;

-- Resultado: 340 estudiantes con discrepancias > 3 cr√©ditos</code></pre>
                </div>

                <!-- M√âTODO 3: ML -->
                <div class="highlight-box primary mb-2">
                    <h3 class="color-primary">3. Modelos Estad√≠sticos y Machine Learning</h3>
                    <p>Detectan <strong>outliers</strong> y <strong>anomal√≠as</strong> utilizando t√©cnicas estad√≠sticas
                        o algoritmos de clustering.</p>

                    <h4 class="color-primary">Detecci√≥n de Outliers con IQR (Rango Intercuart√≠lico)</h4>
                    <pre><code class="language-python">from pyspark.sql.functions import expr, percentile_approx

# Calcular Q1, Q3 y IQR para tiempo de estudio
quantiles = df.approxQuantile("study_hours_per_week", [0.25, 0.75], 0.01)
Q1, Q3 = quantiles[0], quantiles[1]
IQR = Q3 - Q1

# L√≠mites para outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Detectar outliers
outliers = df.filter(
    (col("study_hours_per_week") < lower_bound) |
    (col("study_hours_per_week") > upper_bound)
)

print(f"Outliers detectados: {outliers.count()} / {df.count()}")
print(f"Rango normal: [{lower_bound:.1f}, {upper_bound:.1f}] horas/semana")

# Ejemplo resultado:
# Outliers detectados: 156 / 12,000
# Rango normal: [5.0, 35.0] horas/semana
# ‚Üí 156 estudiantes reportan <5h o >35h semanales (posibles errores)</code></pre>

                    <h4 class="color-primary mt-1">Detecci√≥n de Anomal√≠as con Isolation Forest</h4>
                    <pre><code class="language-python">from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml.linalg import Vectors
import numpy as np

# Preparar features para detecci√≥n de anomal√≠as en rendimiento acad√©mico
feature_cols = ["attendance_rate", "assignments_submitted", "avg_grade", "study_hours"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
df_features = assembler.transform(df)

# M√©todo simplificado: Distancia al centroide (Isolation Forest requiere librer√≠as externas)
from pyspark.ml.stat import Summarizer

# Calcular centroide (media) de cada feature
summary = df_features.select(
    Summarizer.mean(col("features")).alias("mean_vector")
).collect()[0]

mean_vector = summary["mean_vector"]

# Calcular distancia euclidiana al centroide
from pyspark.sql.types import DoubleType
from pyspark.ml.linalg import DenseVector

def euclidean_distance(features, centroid):
    return float(np.linalg.norm(np.array(features) - np.array(centroid)))

distance_udf = udf(lambda f: euclidean_distance(f.toArray(), mean_vector.toArray()), DoubleType())

df_with_distance = df_features.withColumn("anomaly_score", distance_udf(col("features")))

# Detectar anomal√≠as (top 5% distancias)
threshold = df_with_distance.approxQuantile("anomaly_score", [0.95], 0.01)[0]
anomalies = df_with_distance.filter(col("anomaly_score") > threshold)

print(f"Anomal√≠as detectadas: {anomalies.count()} estudiantes con patrones inusuales")

# Analizar caracter√≠sticas de anomal√≠as
anomalies.select("student_id", *feature_cols, "anomaly_score").show()
# Ejemplo: Estudiantes con 95% asistencia pero 0 tareas entregadas (fraude acad√©mico?)</code></pre>
                </div>
            </section>

            <!-- SECCI√ìN 4: ESTRATEGIAS DE CORRECCI√ìN -->
            <section class="section" id="estrategias-correccion">
                <h2 class="section-title">üõ†Ô∏è Estrategias de Correcci√≥n de Errores</h2>

                <!-- ESTRATEGIA 1: IMPUTACI√ìN -->
                <div class="highlight-box secondary mb-2">
                    <h3 class="color-secondary">1. Imputaci√≥n de Valores Faltantes</h3>
                    <p>Sustituci√≥n de valores nulos o faltantes con estimaciones basadas en estad√≠sticas o modelos
                        predictivos.</p>

                    <div class="comparison-grid">
                        <div class="highlight-box primary">
                            <h4 class="color-primary text-center">M√©todos Estad√≠sticos</h4>
                            <ul>
                                <li><strong>Media/Mediana:</strong> Para variables num√©ricas</li>
                                <li><strong>Moda:</strong> Para variables categ√≥ricas</li>
                                <li><strong>Interpolaci√≥n:</strong> Para series temporales</li>
                            </ul>
                        </div>
                        <div class="highlight-box secondary">
                            <h4 class="color-secondary text-center">M√©todos Predictivos</h4>
                            <ul>
                                <li><strong>KNN:</strong> Imputar con vecinos similares</li>
                                <li><strong>Regresi√≥n:</strong> Predecir valor faltante</li>
                                <li><strong>Random Forest:</strong> Modelo ML avanzado</li>
                            </ul>
                        </div>
                    </div>

                    <h4 class="color-secondary mt-1">Imputaci√≥n con PySpark</h4>
                    <pre><code class="language-python">from pyspark.ml.feature import Imputer

# M√©todo 1: Imputar con mediana (robusto a outliers)
imputer = Imputer(
    inputCols=["study_hours_per_week", "attendance_rate"],
    outputCols=["study_hours_imputed", "attendance_imputed"],
    strategy="median"  # Opciones: mean, median, mode
)

model = imputer.fit(df)
df_imputed = model.transform(df)

# M√©todo 2: Imputaci√≥n por grupo (m√°s preciso)
# Ejemplo: Imputar notas faltantes con la media del mismo curso
from pyspark.sql import Window
from pyspark.sql.functions import avg, when, col

window_spec = Window.partitionBy("course_id")

df_imputed_group = df.withColumn(
    "grade_imputed",
    when(
        col("grade").isNull(),
        avg("grade").over(window_spec)  # Media del curso
    ).otherwise(col("grade"))
)

# Comparar antes/despu√©s
print("Valores nulos antes:", df.filter(col("grade").isNull()).count())
print("Valores nulos despu√©s:", df_imputed_group.filter(col("grade_imputed").isNull()).count())</code></pre>
                </div>

                <!-- ESTRATEGIA 2: DEDUPLICACI√ìN -->
                <div class="highlight-box primary mb-2">
                    <h3 class="color-primary">2. Eliminaci√≥n de Duplicados</h3>
                    <p>Uso de algoritmos de similitud para identificar y fusionar registros duplicados.</p>

                    <h4 class="color-primary">Estrategia de Fusi√≥n de Duplicados</h4>
                    <pre><code class="language-python">from pyspark.sql.functions import first, collect_list, concat_ws
from pyspark.sql import Window

# Paso 1: Agrupar duplicados por DNI
window = Window.partitionBy("dni").orderBy(col("created_at").desc())

df_with_rank = df.withColumn("rank", row_number().over(window))

# Paso 2: Seleccionar registro "master" (m√°s reciente)
df_master = df_with_rank.filter(col("rank") == 1).drop("rank")

# Paso 3: Fusionar informaci√≥n de duplicados
# Estrategia: Priorizar campos no nulos del registro m√°s reciente
df_dedup = df.groupBy("dni").agg(
    first("student_id", ignorenulls=True).alias("student_id"),
    first("name", ignorenulls=True).alias("name"),
    first("email", ignorenulls=True).alias("email"),
    first("phone", ignorenulls=True).alias("phone"),
    first("address", ignorenulls=True).alias("address")
)

# Resultado: Reducci√≥n de 50,000 a 48,500 registros (1,500 duplicados eliminados)
print(f"Registros originales: {df.count()}")
print(f"Registros deduplicados: {df_dedup.count()}")
print(f"Duplicados eliminados: {df.count() - df_dedup.count()}")</code></pre>
                </div>

                <!-- ESTRATEGIA 3: RECONCILIACI√ìN -->
                <div class="highlight-box secondary mb-2">
                    <h3 class="color-secondary">3. Reconciliaci√≥n de Fuentes</h3>
                    <p>Seleccionar la fuente m√°s confiable seg√∫n criterios jer√°rquicos cuando existen conflictos.</p>

                    <h4 class="color-secondary">Jerarqu√≠a de Confiabilidad</h4>
                    <div class="styled-table">
                        <table>
                            <thead>
                                <tr>
                                    <th>Prioridad</th>
                                    <th>Sistema</th>
                                    <th>Justificaci√≥n</th>
                                    <th>Datos</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td class="success-cell"><strong>1</strong></td>
                                    <td><strong>SIS (Registro Oficial)</strong></td>
                                    <td>Sistema master, auditable, hist√≥rico completo</td>
                                    <td>Matr√≠culas, t√≠tulos, expediente</td>
                                </tr>
                                <tr>
                                    <td class="success-cell"><strong>2</strong></td>
                                    <td><strong>LMS (Moodle/Canvas)</strong></td>
                                    <td>Datos pedag√≥gicos en tiempo real</td>
                                    <td>Notas, asistencia, entregas</td>
                                </tr>
                                <tr>
                                    <td class="warning-cell"><strong>3</strong></td>
                                    <td><strong>Portal Estudiantil</strong></td>
                                    <td>Auto-reportado por estudiantes</td>
                                    <td>Contacto, preferencias</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h4 class="color-secondary mt-1">Implementaci√≥n de Reconciliaci√≥n</h4>
                    <pre><code class="language-python">from pyspark.sql.functions import coalesce, when

# Unir tres fuentes de datos
df_sis = spark.read.parquet("/data/sis/students/")
df_lms = spark.read.parquet("/data/lms/students/")
df_portal = spark.read.parquet("/data/portal/students/")

# Join por student_id
df_joined = df_sis.alias("sis") \
    .join(df_lms.alias("lms"), "student_id", "left") \
    .join(df_portal.alias("portal"), "student_id", "left")

# Reconciliaci√≥n con prioridad: SIS > LMS > Portal
df_reconciled = df_joined.select(
    "student_id",
    # Email: Priorizar SIS, luego Portal (LMS no confiable para contacto)
    coalesce("sis.email", "portal.email").alias("email"),

    # Calificaci√≥n final: Priorizar SIS (oficial), fallback a LMS
    coalesce("sis.final_grade", "lms.final_grade").alias("final_grade"),

    # Tel√©fono: Portal m√°s actualizado para datos de contacto
    coalesce("portal.phone", "sis.phone").alias("phone"),

    # Direcci√≥n: SIS es fuente can√≥nica
    col("sis.address").alias("address")
)

# M√©tricas de reconciliaci√≥n
print("Conflictos resueltos:")
print(f"  - Email: {df_joined.filter(col('sis.email') != col('portal.email')).count()}")
print(f"  - Calificaci√≥n: {df_joined.filter(col('sis.final_grade') != col('lms.final_grade')).count()}")</code></pre>
                </div>

                <!-- ESTRATEGIA 4: REPROCESAMIENTO -->
                <div class="warning-box mb-2">
                    <h3 class="color-warning">4. Reprocesamiento de Datos</h3>
                    <p>Regenerar registros corruptos a partir de logs, backups o sistemas maestros.</p>

                    <h4 class="color-warning">Recovery desde S3 Versioning</h4>
                    <pre><code class="language-python">import boto3
from datetime import datetime, timedelta

s3 = boto3.client('s3')
bucket = 'university-data-lake'

# Detectar archivos corruptos
corrupted_files = [
    'grades/2025/january/week1.parquet',
    'grades/2025/january/week2.parquet'
]

# Restaurar desde versi√≥n anterior (24h antes)
target_date = datetime.now() - timedelta(days=1)

for file_key in corrupted_files:
    # Listar versiones del archivo
    versions = s3.list_object_versions(Bucket=bucket, Prefix=file_key)

    # Encontrar versi√≥n m√°s cercana a target_date
    for version in versions['Versions']:
        if version['LastModified'] < target_date:
            version_id = version['VersionId']

            # Restaurar versi√≥n
            s3.copy_object(
                Bucket=bucket,
                CopySource={'Bucket': bucket, 'Key': file_key, 'VersionId': version_id},
                Key=file_key
            )
            print(f"‚úì Restaurado {file_key} desde versi√≥n {version_id}")
            break</code></pre>
                </div>
            </section>

            <!-- SECCI√ìN 5: CASO DE ESTUDIO -->
            <section class="section" id="caso-estudio">
                <h2 class="section-title">üéì Caso de Estudio: Universidad de Barcelona</h2>

                <div class="highlight-box primary">
                    <h3 class="color-primary">Contexto del Proyecto</h3>
                    <p>La <strong>Universidad de Barcelona (UB)</strong> gestiona un Data Lake de 85 TB con datos de
                        50,000 estudiantes, 5,000 profesores y 80 programas acad√©micos. Tras migrar a un sistema
                        unificado, detectaron m√∫ltiples problemas de calidad de datos que afectaban decisiones cr√≠ticas.
                    </p>
                </div>

                <h3 class="color-secondary mt-2">Problemas Identificados</h3>
                <div class="styled-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Tipo de Error</th>
                                <th>Magnitud</th>
                                <th>Impacto</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Calificaciones inv√°lidas</strong></td>
                                <td>8% fuera de rango 0-10</td>
                                <td>Promedios acad√©micos incorrectos</td>
                            </tr>
                            <tr>
                                <td><strong>Fechas imposibles</strong></td>
                                <td>12% graduaciones antes de ingreso</td>
                                <td>Reportes institucionales inv√°lidos</td>
                            </tr>
                            <tr>
                                <td><strong>Duplicados</strong></td>
                                <td>3.2% registros duplicados</td>
                                <td>Inflaci√≥n de m√©tricas de matr√≠cula</td>
                            </tr>
                            <tr>
                                <td><strong>Inconsistencias cross-system</strong></td>
                                <td>15% discrepancias cr√©ditos LMS vs SIS</td>
                                <td>Errores en elegibilidad de t√≠tulos</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3 class="color-primary mt-2">Soluci√≥n Implementada</h3>
                <div class="highlight-box secondary">
                    <h4 class="color-secondary">Pipeline de Detecci√≥n y Correcci√≥n</h4>
                    <ol>
                        <li><strong>Ingesta:</strong> Sqoop importa datos de 12 sistemas legacy diariamente (03:00)</li>
                        <li><strong>Validaci√≥n:</strong> 35 reglas de negocio con Great Expectations (03:30)</li>
                        <li><strong>Detecci√≥n ML:</strong> Isolation Forest detecta anomal√≠as en rendimiento (04:00)</li>
                        <li><strong>Correcci√≥n:</strong> Pipeline PySpark aplica imputaci√≥n y deduplicaci√≥n (04:30)</li>
                        <li><strong>Reconciliaci√≥n:</strong> Jerarqu√≠a SIS > LMS > Portal resuelve conflictos (05:00)
                        </li>
                        <li><strong>Alertas:</strong> Dashboard Power BI + emails si errores cr√≠ticos (05:30)</li>
                    </ol>
                </div>

                <h3 class="color-secondary mt-2">Resultados (12 meses despu√©s)</h3>
                <div class="styled-table">
                    <table>
                        <thead>
                            <tr>
                                <th>M√©trica</th>
                                <th>Antes</th>
                                <th>Despu√©s</th>
                                <th>Mejora</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Errores sint√°cticos</strong></td>
                                <td class="error-cell">8%</td>
                                <td class="success-cell">0.2%</td>
                                <td>-97.5%</td>
                            </tr>
                            <tr>
                                <td><strong>Errores sem√°nticos</strong></td>
                                <td class="error-cell">12%</td>
                                <td class="success-cell">0.8%</td>
                                <td>-93.3%</td>
                            </tr>
                            <tr>
                                <td><strong>Duplicados</strong></td>
                                <td class="error-cell">3.2%</td>
                                <td class="success-cell">0.1%</td>
                                <td>-96.9%</td>
                            </tr>
                            <tr>
                                <td><strong>Inconsistencias</strong></td>
                                <td class="error-cell">15%</td>
                                <td class="success-cell">1.5%</td>
                                <td>-90%</td>
                            </tr>
                            <tr>
                                <td><strong>Tiempo correcci√≥n manual</strong></td>
                                <td class="warning-cell">600 h/a√±o</td>
                                <td class="success-cell">30 h/a√±o</td>
                                <td>-95%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3 class="color-primary mt-2">Impacto en Modelos Predictivos</h3>
                <div class="comparison-grid">
                    <div class="highlight-box primary">
                        <h4 class="color-primary text-center">Predicci√≥n de Abandono</h4>
                        <ul>
                            <li><strong>Precisi√≥n antes:</strong> 72% (datos sucios)</li>
                            <li><strong>Precisi√≥n despu√©s:</strong> 89% (+17%)</li>
                            <li><strong>Resultado:</strong> 420 estudiantes identificados correctamente, 340
                                intervenciones exitosas</li>
                        </ul>
                    </div>
                    <div class="highlight-box secondary">
                        <h4 class="color-secondary text-center">Asignaci√≥n de Recursos</h4>
                        <ul>
                            <li><strong>Errores antes:</strong> 180 plazas mal asignadas/semestre</li>
                            <li><strong>Errores despu√©s:</strong> 12 plazas mal asignadas/semestre</li>
                            <li><strong>Resultado:</strong> Optimizaci√≥n de ocupaci√≥n +8%, ahorro ‚Ç¨150K/a√±o</li>
                        </ul>
                    </div>
                </div>

                <div class="highlight-box secondary mt-2">
                    <p class="title">‚úÖ Conclusi√≥n del Caso</p>
                    <p class="content">La Universidad de Barcelona logr√≥ <strong>reducir errores en un 95%</strong>
                        mediante un pipeline automatizado de detecci√≥n y correcci√≥n. El proyecto elimin√≥ 570 horas
                        anuales de trabajo manual, mejor√≥ la precisi√≥n de modelos predictivos en +17-25%, y gener√≥ un
                        ROI del <strong>380%</strong> en el primer a√±o (inversi√≥n en infraestructura vs ahorros
                        operativos + mejora de decisiones).</p>
                </div>
            </section>
        </main>

        <footer>
            <h3>iLERNA</h3>
            <p class="footer-course">Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</p>
            <a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a>
            <p class="footer-info">Centro oficial de FP online y presencial. Ciclos formativos de Grado Medio y Grado
                Superior.</p>
            <p class="footer-info">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>

            <div class="penguin">
                <span>üêß</span>
            </div>
        </footer>
    </div>

    <script src="../js/lecciones.js"></script>
</body>

</html>
