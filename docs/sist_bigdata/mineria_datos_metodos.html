<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="M√©todos principales en la miner√≠a de datos - Clasificaci√≥n, Regresi√≥n, Clustering y m√°s">
    <meta name="keywords"
        content="Miner√≠a de Datos, Clasificaci√≥n, Regresi√≥n, Clustering, Asociaci√≥n, Anomal√≠as, Machine Learning">
    <meta name="author" content="Bjlanza">
    <meta name="organization" content="ILERNA">
    <title>M√©todos Principales en Miner√≠a de Datos | iLERNA</title>
    <link rel="stylesheet" href="../css/lecciones.css">
</head>

<body>
    <header>
        <div class="header-container">
            <div class="logo-container">
                <a href="../index.html">
                    <img src="../img/logo-ilerna.svg" alt="Logo iLERNA">
                </a>
                <div class="logo-text">
                    <h1>iLERNA</h1>
                    <p>Centro de Formaci√≥n Profesional Online</p>
                </div>
            </div>
            <nav class="breadcrumb">
                <a href="../index.html">Inicio</a>
                <span>‚Ä∫</span>
                <a href="index.html">Sistemas Big Data</a>
                <span>‚Ä∫</span>
                <span>M√©todos de Miner√≠a</span>
            </nav>
        </div>
    </header>

    <main class="container">
        <!-- Hero Section -->
        <div class="hero">
            <h1 class="color-primary">‚öôÔ∏è M√©todos de la Miner√≠a de Datos</h1>
            <p class="subtitle">Herramientas y algoritmos para transformar datos en conocimiento</p>
        </div>

        <!-- TOC -->
        <nav class="toc-container">
            <h3>üìë Contenido de la Lecci√≥n</h3>
            <ul class="toc-list">
                <li><a href="#clasificacion">1. Clasificaci√≥n: Asignaci√≥n de Etiquetas</a></li>
                <li><a href="#regresion">2. Regresi√≥n: Estimaci√≥n de Valores Continuos</a></li>
                <li><a href="#clustering">3. Agrupamiento (Clustering): Grupos Naturales</a></li>
                <li><a href="#asociacion">4. Asociaci√≥n y Detecci√≥n de Anomal√≠as</a></li>
                <li><a href="#dimension">5. Reducci√≥n de Dimensionalidad</a></li>
                <li><a href="#paradigma">6. Supervisado vs No Supervisado</a></li>
            </ul>
        </nav>

        <!-- SECCI√ìN 1: CLASIFICACI√ìN -->
        <section id="clasificacion" class="section">
            <h2>1. Clasificaci√≥n: Asignaci√≥n de Etiquetas</h2>
            <p>
                La clasificaci√≥n consiste en asignar una <strong>etiqueta o categor√≠a discreta</strong> a cada elemento
                de un conjunto de datos bas√°ndose en sus caracter√≠sticas previas. Es uno de los m√©todos m√°s utilizados
                en la anal√≠tica de negocio.
            </p>

            <div class="grid-2-cols" style="margin-top: 2rem;">
                <div class="card primary">
                    <h4 class="color-primary">üéØ Aplicaciones T√≠picas</h4>
                    <ul class="custom-list">
                        <li><strong>Detecci√≥n de fraude:</strong> ¬øEs una transacci√≥n leg√≠tima o sospechosa?</li>
                        <li><strong>Diagn√≥stico m√©dico:</strong> ¬øPresenta el paciente una patolog√≠a espec√≠fica?</li>
                        <li><strong>Segmentaci√≥n:</strong> ¬øEste cliente pertenece al grupo de "Alta fidelidad"?</li>
                    </ul>
                </div>
                <div class="card secondary">
                    <h4 class="color-secondary">üß± Algoritmos Comunes</h4>
                    <div style="display: flex; flex-wrap: wrap; gap: 0.5rem; margin-top: 0.5rem;">
                        <span class="code-badge">√Årboles de Decisi√≥n</span>
                        <span class="code-badge">SVM</span>
                        <span class="code-badge">KNN</span>
                        <span class="code-badge">Redes Neuronales (ANN)</span>
                    </div>
                    <p style="font-size: 0.95rem; margin-top: 1rem; color: var(--text-light);">
                        <em>Ejemplo: Un modelo analiza la edad e ingresos para predecir si un cliente tiene "Riesgo de
                            abandono".</em>
                    </p>
                </div>
            </div>
        </section>

        <!-- SECCI√ìN 2: REGRESI√ìN -->
        <section id="regresion" class="section">
            <h2>2. Regresi√≥n: Estimaci√≥n de Valores Continuos</h2>
            <p>
                A diferencia de la clasificaci√≥n, en la regresi√≥n el objetivo es predecir un <strong>valor num√©rico
                    continuo</strong>. Se utiliza cuando necesitamos saber "cu√°nto" de algo va a suceder.
            </p>

            <div class="highlight-box secondary">
                <p><strong>Uso principal:</strong> Estimaci√≥n de precios, predicci√≥n de ingresos trimestrales o c√°lculo
                    de niveles de demanda futura.</p>
            </div>

            <div class="grid-features">
                <div class="feature-card primary">
                    <h4>üìâ Regresi√≥n Lineal</h4>
                    <p>Relaciona variables para predecir un valor num√©rico simple.</p>
                </div>
                <div class="feature-card secondary">
                    <h4>üå≥ Random Forest</h4>
                    <p>Combina m√∫ltiples √°rboles para mejorar la precisi√≥n y evitar el sobreajuste.</p>
                </div>
                <div class="feature-card primary">
                    <h4>üöÄ Gradient Boosting</h4>
                    <p>T√©cnica iterativa que corrige errores de modelos anteriores para m√°xima precisi√≥n.</p>
                </div>
            </div>
        </section>

        <!-- SECCI√ìN 3: CLUSTERING -->
        <section id="clustering" class="section">
            <h2>3. Agrupamiento (Clustering): Grupos Naturales</h2>
            <p>
                El agrupamiento busca identificar grupos de elementos similares <strong>sin que exista una etiqueta
                    previa</strong>. Es el sistema el que "descubre" la estructura oculta de los datos.
            </p>

            <div class="scenario-box primary" style="margin-top: 2rem;">
                <h4 class="mb-1">T√©cnicas de Agrupamiento</h4>
                <p style="font-size: 1.125rem;">
                    <strong>K-Means</strong> es el algoritmo m√°s popular, pero existen variantes como
                    <strong>DBSCAN</strong> (ideal para grupos con formas irregulares) o <strong>Hierarchical
                        Clustering</strong> (para estructuras de √°rbol).
                </p>
            </div>

            <div class="expert-quote" style="margin-top: 2rem;">
                <p class="quote-text">
                    "El clustering es fundamental para la segmentaci√≥n de clientes donde no sabemos de antemano cu√°ntos
                    perfiles diferentes existen en nuestra base de datos."
                </p>
                <span class="quote-author">‚Äî Data Architect</span>
            </div>
        </section>

        <!-- SECCI√ìN 4: ASOCIACI√ìN Y ANOMAL√çAS -->
        <section id="asociacion" class="section">
            <h2>4. Asociaci√≥n y Detecci√≥n de Anomal√≠as</h2>
            <div class="grid-2-cols" style="margin-top: 1rem;">
                <div class="card primary-bg">
                    <h4 class="color-primary">üõí Reglas de Asociaci√≥n</h4>
                    <p style="font-size: 1.125rem;">
                        Identifica relaciones frecuentes entre variables. Es el famoso <strong>an√°lisis de la cesta de
                            la compra</strong>.
                    </p>
                    <div class="highlight-box"
                        style="background: white; border: 1px dashed var(--color-primary); margin-top: 1rem;">
                        <p style="font-style: italic; font-size: 1.125rem; margin-bottom: 0.5rem;"><strong>Ejemplos de
                                Reglas:</strong></p>
                        <ul style="font-size: 1.125rem; margin-bottom: 0;">
                            <li>"Si compra <strong>Pan</strong> y <strong>Mantequilla</strong> ‚Üí 70% de probabilidad de
                                comprar <strong>Leche</strong>."</li>
                            <li>"Si reserva <strong>Vuelo</strong> y <strong>Hotel</strong> ‚Üí 45% de probabilidad de
                                contratar <strong>Seguro de Viaje</strong>."</li>
                            <li>"Si adquiere <strong>Smartphone</strong> ‚Üí 80% de probabilidad de comprar <strong>Funda
                                    Protectora</strong> en la misma transacci√≥n."</li>
                            <li>"En streaming: Si ve <strong>Documental de Naturaleza</strong> ‚Üí 60% de probabilidad de
                                ver <strong>Serie de Ciencia Ficci√≥n</strong>."</li>
                        </ul>
                    </div>
                </div>
                <div class="card secondary-bg">
                    <h4 class="color-secondary">üö® Detecci√≥n de Anomal√≠as</h4>
                    <p style="font-size: 1.125rem;">
                        Identifica observaciones que se desv√≠an dr√°sticamente de la norma. Es cr√≠tico para la
                        <strong>ciberseguridad</strong>, la detecci√≥n de intrusiones y el fraude bancario.
                    </p>

                    <div style="display: flex; flex-direction: column; gap: 0.75rem; margin-top: 1.5rem;">
                        <div class="card white" style="padding: 1.25rem; margin-bottom: 0;">
                            <h4 class="color-secondary">üå≤ Isolation Forest</h4>
                            <p style="font-size: 1.125rem; margin-bottom: 0;">En lugar de modelar lo "normal", este
                                algoritmo <strong>a√≠sla</strong> las anomal√≠as. Dado que los valores at√≠picos son pocos
                                y diferentes, se "separan" de la masa de datos mucho m√°s r√°pido en un √°rbol de decisi√≥n,
                                requiriendo menos particiones aleatorias para ser correctamente identificados por el
                                modelo.</p>
                        </div>
                        <div class="card white" style="padding: 1.25rem; margin-bottom: 0;">
                            <h4 class="color-secondary">üß† Autoencoders</h4>
                            <p style="font-size: 1.125rem; margin-bottom: 0;">Son redes neuronales que aprenden a
                                comprimir y reconstruir los datos. Cuando se les presenta una anomal√≠a, la red no sabe
                                c√≥mo reconstruirla correctamente, generando un <strong>error de reconstrucci√≥n</strong>
                                significativamente alto que delata de inmediato la irregularidad del patr√≥n.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- SECCI√ìN 5: DIMENSIONALIDAD -->
        <section id="dimension" class="section">
            <h2>5. Reducci√≥n de Dimensionalidad</h2>
            <p>
                En Big Data es com√∫n trabajar con cientos de variables. Las t√©cnicas de reducci√≥n permiten simplificar
                esta complejidad manteniendo la informaci√≥n esencial.
            </p>

            <div style="display: flex; flex-direction: column; gap: 2rem; margin-top: 2rem;">
                <div class="feature-card primary">
                    <h4 class="color-primary" style="font-size: 1.5rem;">üìê PCA (An√°lisis de Componentes Principales)
                    </h4>
                    <p>Es el m√©todo est√°ndar de reducci√≥n lineal de la dimensionalidad. Su funcionamiento se basa en
                        encontrar nuevas variables, llamadas componentes principales, que son combinaciones lineales de
                        las originales y que capturan la m√°xima varianza de los datos.</p>
                    <p>Al proyectar los datos en estas nuevas componentes, podemos permitirnos descartar aquellas que
                        contienen poca informaci√≥n o ruido. Esto simplifica dr√°sticamente la complejidad computacional
                        de los modelos posteriores sin sacrificar la esencia predictiva del conjunto de datos original.
                    </p>
                    <p>Desde una perspectiva pr√°ctica, PCA es ideal para eliminar la <strong>multicolinealidad</strong>
                        (variables altamente correlacionadas que confunden al modelo) y para acelerar el entrenamiento
                        de algoritmos en datasets con miles de columnas, como ocurre en el an√°lisis gen√≥mico o
                        financiero.</p>
                    <p>Adem√°s, al reducir la cantidad de variables, PCA ayuda a mitigar el fen√≥meno conocido como "la
                        maldici√≥n de la dimensionalidad", donde los modelos pierden capacidad de generalizaci√≥n debido a
                        un exceso de ruido innecesario en los datos de entrada.</p>

                    <!-- SVG PCA (Al final, m√°s ancho) -->
                    <div
                        style="background: white; border-radius: 0.5rem; padding: 1.5rem; margin-top: 1.5rem; border: 1px solid var(--bg-primary-light); text-align: center;">
                        <svg viewBox="0 0 500 200" xmlns="http://www.w3.org/2000/svg"
                            style="max-width: 500px; margin: 0 auto; display: block; width: 100%;">
                            <line x1="50" y1="180" x2="450" y2="180" stroke="#ccc" stroke-width="1" />
                            <line x1="50" y1="20" x2="50" y2="180" stroke="#ccc" stroke-width="1" />
                            <circle cx="100" cy="150" r="3" fill="#49B9CE" opacity="0.6" />
                            <circle cx="120" cy="130" r="3" fill="#49B9CE" opacity="0.6" />
                            <circle cx="150" cy="120" r="3" fill="#49B9CE" opacity="0.6" />
                            <circle cx="180" cy="100" r="3" fill="#49B9CE" opacity="0.6" />
                            <circle cx="210" cy="80" r="3" fill="#49B9CE" opacity="0.6" />
                            <circle cx="240" cy="70" r="3" fill="#49B9CE" opacity="0.6" />
                            <circle cx="270" cy="50" r="3" fill="#49B9CE" opacity="0.6" />
                            <line x1="70" y1="170" x2="350" y2="25" stroke="var(--color-primary)" stroke-width="4"
                                stroke-linecap="round" />
                            <text x="360" y="25" fill="var(--color-primary)" font-size="14" font-weight="800">PC1
                                (M√°xima Varianza)</text>
                            <line x1="160" y1="40" x2="220" y2="140" stroke="var(--color-secondary)" stroke-width="2"
                                stroke-dasharray="4,2" />
                            <text x="210" y="155" fill="var(--color-secondary)" font-size="12">PC2</text>
                        </svg>
                        <p style="font-size: 1.125rem; color: var(--text-medium); margin-top: 0.75rem;"><strong>Concepto
                                PCA:</strong> Transformaci√≥n de ejes para alinear la mayor dispersi√≥n de datos con la
                            primera componente.</p>
                    </div>
                </div>

                <div class="feature-card secondary">
                    <h4 class="color-secondary" style="font-size: 1.5rem;">üé® t-SNE (t-Distributed Stochastic Neighbor
                        Embedding)</h4>
                    <p>A diferencia de PCA, t-SNE es un algoritmo <strong>no lineal</strong> dise√±ado espec√≠ficamente
                        para la visualizaci√≥n de datos extremadamente complejos. Su objetivo principal es preservar la
                        estructura local, asegurando que los puntos que son vecinos cercanos en el espacio de alta
                        dimensi√≥n sigan estando juntos en una representaci√≥n de 2D o 3D.</p>
                    <p>Funciona convirtiendo las distancias en probabilidades y minimizando la divergencia entre estas
                        probabilidades en el espacio original y el de baja dimensi√≥n. Esto lo convierte en una
                        herramienta sumamente potente para detectar cl√∫steres visuales que otros m√©todos lineales
                        pasar√≠an por alto por completo.</p>
                    <p>Sin embargo, t-SNE es computacionalmente costoso y sus resultados pueden variar seg√∫n los
                        par√°metros (como la perplejidad). Se usa masivamente en la exploraci√≥n inicial de datos para
                        entender si existen agrupaciones naturales antes de proceder a aplicar modelos de aprendizaje
                        supervisado m√°s complejos.</p>

                    <!-- SVG t-SNE (Al final, m√°s ancho) -->
                    <div
                        style="background: white; border-radius: 0.5rem; padding: 1.5rem; margin-top: 1.5rem; border: 1px solid var(--bg-secondary-light); text-align: center;">
                        <svg viewBox="0 0 400 200" xmlns="http://www.w3.org/2000/svg"
                            style="max-width: 500px; margin: 0 auto; display: block; width: 100%;">
                            <circle cx="80" cy="60" r="35" fill="var(--color-primary)" opacity="0.1" />
                            <circle cx="70" cy="50" r="4" fill="var(--color-primary)" />
                            <circle cx="90" cy="70" r="4" fill="var(--color-primary)" />
                            <circle cx="85" cy="45" r="4" fill="var(--color-primary)" />
                            <circle cx="65" cy="75" r="4" fill="var(--color-primary)" />

                            <circle cx="320" cy="140" r="35" fill="var(--color-secondary)" opacity="0.1" />
                            <circle cx="310" cy="130" r="4" fill="var(--color-secondary)" />
                            <circle cx="335" cy="150" r="4" fill="var(--color-secondary)" />
                            <circle cx="325" cy="120" r="4" fill="var(--color-secondary)" />
                            <circle cx="305" cy="155" r="4" fill="var(--color-secondary)" />

                            <path d="M 120 70 C 180 120, 220 50, 280 120" stroke="#ddd" fill="transparent"
                                stroke-dasharray="6,4" />
                            <text x="200" y="180" text-anchor="middle" fill="var(--color-secondary)" font-size="13"
                                font-weight="600">Preservaci√≥n de topolog√≠a local (vecindad)</text>
                        </svg>
                        <p style="font-size: 1.125rem; color: var(--text-medium); margin-top: 0.75rem;"><strong>Concepto
                                t-SNE:</strong> Agrupaci√≥n de elementos similares en "islas" visuales ignorando
                            distancias globales.</p>
                    </div>
                </div>

                <div class="feature-card neutral" style="border: 2px solid var(--text-dark);">
                    <h4 class="text-dark" style="font-size: 1.5rem;">üöÄ UMAP (Uniform Manifold Approximation and
                        Projection)</h4>
                    <p>UMAP es el est√°ndar moderno que ha superado a t-SNE en muchos escenarios de Big Data debido a su
                        eficiencia superior. Se basa en fundamentos matem√°ticos de la topolog√≠a algebraica para
                        proyectar los datos manteniendo un equilibrio ideal entre la estructura local (vecinos cercanos)
                        y la <strong>estructura global</strong> del dataset original.</p>
                    <p>Una de sus grandes ventajas competitivas es su velocidad y escalabilidad. Mientras que t-SNE
                        puede tardar horas en procesar millones de registros, UMAP lo hace en una fracci√≥n de ese
                        tiempo, lo que permite una iteraci√≥n mucho m√°s √°gil durante todo el ciclo de vida de la miner√≠a
                        de datos.</p>
                    <p>Es la herramienta preferida en campos como el procesamiento de lenguaje natural (NLP) para
                        visualizar embeddings de palabras o en bioinform√°tica para el an√°lisis de secuenciaci√≥n de
                        c√©lulas √∫nicas, debido a su capacidad para preservar las relaciones jer√°rquicas complejas entre
                        los diferentes grupos de datos.</p>

                    <!-- SVG UMAP (Al final, m√°s ancho) -->
                    <div
                        style="background: white; border-radius: 0.5rem; padding: 1.5rem; margin-top: 1.5rem; border: 1px solid var(--text-dark); text-align: center;">
                        <svg viewBox="0 0 400 200" xmlns="http://www.w3.org/2000/svg"
                            style="max-width: 500px; margin: 0 auto; display: block; width: 100%;">
                            <!-- Representaci√≥n de Manifold -->
                            <path d="M 50 150 Q 150 50 350 150" stroke="#eee" fill="none" stroke-width="20"
                                opacity="0.5" stroke-linecap="round" />
                            <!-- Puntos sobre el manifold -->
                            <circle cx="80" cy="135" r="5" fill="var(--color-primary)" />
                            <circle cx="120" cy="100" r="5" fill="var(--color-primary)" />
                            <circle cx="200" cy="85" r="5" fill="var(--color-secondary)" />
                            <circle cx="280" cy="110" r="5" fill="var(--color-secondary)" />
                            <circle cx="330" cy="140" r="5" fill="#48bb78" />
                            <!-- Conexiones topol√≥gicas -->
                            <path d="M 80 135 L 120 100" stroke="var(--color-primary)" stroke-width="1.5"
                                stroke-linecap="round" />
                            <path d="M 120 100 L 200 85" stroke="#ccc" stroke-width="1" stroke-dasharray="2,2" />
                            <path d="M 200 85 L 280 110" stroke="var(--color-secondary)" stroke-width="1.5"
                                stroke-linecap="round" />
                            <path d="M 280 110 L 330 140" stroke="#ccc" stroke-width="1" stroke-dasharray="2,2" />
                            <text x="200" y="40" text-anchor="middle" fill="var(--text-dark)" font-size="14"
                                font-weight="800">Aproximaci√≥n Topol√≥gica Global (Manifold)</text>
                        </svg>
                        <p style="font-size: 1.125rem; color: var(--text-medium); margin-top: 0.75rem;"><strong>Concepto
                                UMAP:</strong> Modelado de la estructura de los datos como una superficie continua
                            compleja, preservando vecindades globales.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- SECCI√ìN 6: PARADIGMAS -->
        <section id="paradigma" class="section">
            <h2>6. Supervisado vs No Supervisado</h2>
            <p>
                La gran divisi√≥n en el Machine Learning se define por la presencia o ausencia de un "maestro" o
                etiquetas que gu√≠en el aprendizaje durante el proceso de entrenamiento.
            </p>

            <div class="table-container" style="margin-top: 2rem;">
                <table>
                    <thead>
                        <tr>
                            <th>Caracter√≠stica</th>
                            <th>‚úÖ Supervisado</th>
                            <th>‚ùì No Supervisado</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Datos de Entrada</strong></td>
                            <td>Etiquetados (Entrada + Respuesta)</td>
                            <td>No etiquetados (Solo entrada)</td>
                        </tr>
                        <tr>
                            <td><strong>Objetivo</strong></td>
                            <td>Predecir resultados o clasificar</td>
                            <td>Descubrir estructuras ocultas</td>
                        </tr>
                        <tr>
                            <td><strong>Retroalimentaci√≥n</strong></td>
                            <td>Directa (Error vs Etiqueta real)</td>
                            <td>Ninguna (Se basa en similitudes)</td>
                        </tr>
                        <tr>
                            <td><strong>M√©todos principales</strong></td>
                            <td>Clasificaci√≥n y Regresi√≥n</td>
                            <td>Clustering y Asociaci√≥n</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="grid-2-cols" style="margin-top: 2rem;">
                <div class="card primary-bg">
                    <h4 class="color-primary">üéØ Cu√°ndo usar Supervisado</h4>
                    <p>Cuando tienes datos hist√≥ricos donde ya conoces el resultado y quieres automatizar esa decisi√≥n
                        para el futuro (ej: aprobar un cr√©dito basado en impagos anteriores o clasificar correos
                        electr√≥nicos).</p>
                </div>
                <div class="card secondary-bg">
                    <h4 class="color-secondary">üî≠ Cu√°ndo usar No Supervisado</h4>
                    <p>Cuando te enfrentas a un dataset totalmente nuevo y quieres explorar qu√© grupos de comportamiento
                        existen sin imponer categor√≠as previas (ej: descubrir nuevos segmentos de mercado o detectar
                        patrones an√≥malos de red).</p>
                </div>
            </div>
        </section>

    </main>

    <footer>
        <h3>iLERNA</h3>
        <p class="footer-course">Curso de Especializaci√≥n en Inteligencia Artificial y Big Data</p>
        <a href="https://www.ilerna.es/" target="_blank">www.ilerna.es</a>
        <p class="footer-info">Centro oficial de FP online y presencial. Ciclos formativos de Grado Medio y Grado
            Superior.</p>
        <p class="footer-info">Titulaciones 100% oficiales. ¬°Sin pruebas libres!</p>
        <div class="penguin">
            <span>üêß</span>
        </div>
    </footer>

    <script src="../js/lecciones.js"></script>
</body>

</html>